2025-04-29T11:43:40,010 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Started daemon with process name: 60@c27de723f0d4
2025-04-29T11:43:40,011 [main] INFO  org.apache.spark.deploy.master.Master [] - Started daemon with process name: 62@bb9210fe839d
2025-04-29T11:43:40,016 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Started daemon with process name: 60@066f03cf7e53
2025-04-29T11:43:40,028 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-29T11:43:40,028 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Started daemon with process name: 60@6cca10b5c892
2025-04-29T11:43:40,030 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-29T11:43:40,032 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-29T11:43:40,037 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-29T11:43:40,039 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-29T11:43:40,040 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-29T11:43:40,040 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-29T11:43:40,042 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-29T11:43:40,043 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-29T11:43:40,046 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-29T11:43:40,049 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-29T11:43:40,050 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-29T11:43:40,527 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-29T11:43:40,527 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-29T11:43:40,527 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-29T11:43:40,529 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-29T11:43:40,529 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-29T11:43:40,531 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-29T11:43:40,535 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T11:43:40,535 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T11:43:40,538 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T11:43:40,538 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T11:43:40,539 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T11:43:40,540 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-29T11:43:40,541 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T11:43:40,540 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-29T11:43:40,542 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-29T11:43:40,543 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-29T11:43:40,545 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-29T11:43:40,549 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T11:43:40,552 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T11:43:40,554 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-29T11:43:41,042 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-29T11:43:41,042 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-29T11:43:41,051 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-29T11:43:41,082 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-29T11:43:42,088 [main] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkWorker' on port 34443.
2025-04-29T11:43:42,090 [main] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkWorker' on port 38821.
2025-04-29T11:43:42,091 [main] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkWorker' on port 36391.
2025-04-29T11:43:42,093 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Worker decommissioning not enabled.
2025-04-29T11:43:42,094 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Worker decommissioning not enabled.
2025-04-29T11:43:42,098 [main] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkMaster' on port 7077.
2025-04-29T11:43:42,098 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Worker decommissioning not enabled.
2025-04-29T11:43:42,138 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Starting Spark master at spark://bb9210fe839d:7077
2025-04-29T11:43:42,149 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Running Spark version 3.5.5
2025-04-29T11:43:42,225 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @5034ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-29T11:43:42,396 [dispatcher-event-loop-1] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:8080 for MasterUI
2025-04-29T11:43:42,441 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-29T11:43:42,508 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - Started @5319ms
2025-04-29T11:43:42,554 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Starting Spark worker 172.18.0.4:36391 with 2 cores, 3.0 GiB RAM
2025-04-29T11:43:42,558 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Starting Spark worker 172.18.0.5:38821 with 2 cores, 3.0 GiB RAM
2025-04-29T11:43:42,558 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Starting Spark worker 172.18.0.3:34443 with 2 cores, 3.0 GiB RAM
2025-04-29T11:43:42,566 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Running Spark version 3.5.5
2025-04-29T11:43:42,568 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Spark home: /opt/bitnami/spark
2025-04-29T11:43:42,569 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Running Spark version 3.5.5
2025-04-29T11:43:42,569 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Running Spark version 3.5.5
2025-04-29T11:43:42,570 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Spark home: /opt/bitnami/spark
2025-04-29T11:43:42,570 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Spark home: /opt/bitnami/spark
2025-04-29T11:43:42,586 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@61eb529f{HTTP/1.1, (http/1.1)}{0.0.0.0:8080}
2025-04-29T11:43:42,587 [dispatcher-event-loop-1] INFO  org.apache.spark.util.Utils [] - Successfully started service 'MasterUI' on port 8080.
2025-04-29T11:43:42,603 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T11:43:42,605 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.worker.
2025-04-29T11:43:42,605 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T11:43:42,605 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T11:43:42,607 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.worker.
2025-04-29T11:43:42,606 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T11:43:42,607 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.worker.
2025-04-29T11:43:42,609 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T11:43:42,611 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T11:43:42,638 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6f742a8b{/app,null,AVAILABLE,@Spark}
2025-04-29T11:43:42,646 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6c9cf10{/app/json,null,AVAILABLE,@Spark}
2025-04-29T11:43:42,649 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@264bd6a{/,null,AVAILABLE,@Spark}
2025-04-29T11:43:42,652 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2dd6ac90{/json,null,AVAILABLE,@Spark}
2025-04-29T11:43:42,666 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2a47bcd5{/static,null,AVAILABLE,@Spark}
2025-04-29T11:43:42,666 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @5258ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-29T11:43:42,668 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5b4d4288{/app/kill,null,AVAILABLE,@Spark}
2025-04-29T11:43:42,671 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @5481ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-29T11:43:42,671 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5288b29d{/driver/kill,null,AVAILABLE,@Spark}
2025-04-29T11:43:42,673 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @5280ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-29T11:43:42,675 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@67995065{/workers/kill,null,AVAILABLE,@Spark}
2025-04-29T11:43:42,679 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.ui.MasterWebUI [] - Bound MasterWebUI to 0.0.0.0, and started at http://bb9210fe839d:8080
2025-04-29T11:43:42,734 [dispatcher-event-loop-1] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:8081 for WorkerUI
2025-04-29T11:43:42,741 [dispatcher-event-loop-1] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:8081 for WorkerUI
2025-04-29T11:43:42,745 [dispatcher-event-loop-1] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:8081 for WorkerUI
2025-04-29T11:43:42,761 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-29T11:43:42,763 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-29T11:43:42,766 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-29T11:43:42,793 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - Started @5402ms
2025-04-29T11:43:42,801 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - Started @5394ms
2025-04-29T11:43:42,802 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - Started @5614ms
2025-04-29T11:43:42,871 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@33f885d7{HTTP/1.1, (http/1.1)}{0.0.0.0:8081}
2025-04-29T11:43:42,880 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@33f885d7{HTTP/1.1, (http/1.1)}{0.0.0.0:8081}
2025-04-29T11:43:42,881 [dispatcher-event-loop-1] INFO  org.apache.spark.util.Utils [] - Successfully started service 'WorkerUI' on port 8081.
2025-04-29T11:43:42,881 [dispatcher-event-loop-1] INFO  org.apache.spark.util.Utils [] - Successfully started service 'WorkerUI' on port 8081.
2025-04-29T11:43:42,893 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@7e44958f{HTTP/1.1, (http/1.1)}{0.0.0.0:8081}
2025-04-29T11:43:42,894 [dispatcher-event-loop-1] INFO  org.apache.spark.util.Utils [] - Successfully started service 'WorkerUI' on port 8081.
2025-04-29T11:43:42,942 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3b663a8f{/logPage,null,AVAILABLE,@Spark}
2025-04-29T11:43:42,943 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3b663a8f{/logPage,null,AVAILABLE,@Spark}
2025-04-29T11:43:42,949 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@73d89683{/logPage/json,null,AVAILABLE,@Spark}
2025-04-29T11:43:42,952 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@73d89683{/logPage/json,null,AVAILABLE,@Spark}
2025-04-29T11:43:42,955 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40f8f985{/,null,AVAILABLE,@Spark}
2025-04-29T11:43:42,959 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@598c8536{/json,null,AVAILABLE,@Spark}
2025-04-29T11:43:42,962 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40f8f985{/,null,AVAILABLE,@Spark}
2025-04-29T11:43:42,965 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@598c8536{/json,null,AVAILABLE,@Spark}
2025-04-29T11:43:42,978 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@31dbd36b{/static,null,AVAILABLE,@Spark}
2025-04-29T11:43:42,990 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@184c8302{/logPage,null,AVAILABLE,@Spark}
2025-04-29T11:43:42,995 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@31dbd36b{/static,null,AVAILABLE,@Spark}
2025-04-29T11:43:43,002 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7b1da60{/log,null,AVAILABLE,@Spark}
2025-04-29T11:43:43,005 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1c437a4b{/logPage/json,null,AVAILABLE,@Spark}
2025-04-29T11:43:43,007 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7b1da60{/log,null,AVAILABLE,@Spark}
2025-04-29T11:43:43,013 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@713d193b{/,null,AVAILABLE,@Spark}
2025-04-29T11:43:43,020 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.ui.WorkerWebUI [] - Bound WorkerWebUI to 0.0.0.0, and started at http://066f03cf7e53:8081
2025-04-29T11:43:43,021 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.ui.WorkerWebUI [] - Bound WorkerWebUI to 0.0.0.0, and started at http://6cca10b5c892:8081
2025-04-29T11:43:43,022 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4c46a8a8{/json,null,AVAILABLE,@Spark}
2025-04-29T11:43:43,027 [worker-register-master-threadpool-0] INFO  org.apache.spark.deploy.worker.Worker [] - Connecting to master spark-master:7077...
2025-04-29T11:43:43,032 [worker-register-master-threadpool-0] INFO  org.apache.spark.deploy.worker.Worker [] - Connecting to master spark-master:7077...
2025-04-29T11:43:43,045 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@511d0513{/metrics/master/json,null,AVAILABLE,@Spark}
2025-04-29T11:43:43,058 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4480211c{/metrics/applications/json,null,AVAILABLE,@Spark}
2025-04-29T11:43:43,065 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7eacafc9{/static,null,AVAILABLE,@Spark}
2025-04-29T11:43:43,071 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@62cd5b4{/log,null,AVAILABLE,@Spark}
2025-04-29T11:43:43,082 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.ui.WorkerWebUI [] - Bound WorkerWebUI to 0.0.0.0, and started at http://c27de723f0d4:8081
2025-04-29T11:43:43,090 [worker-register-master-threadpool-0] INFO  org.apache.spark.deploy.worker.Worker [] - Connecting to master spark-master:7077...
2025-04-29T11:43:43,099 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@d6f93f5{/metrics/json,null,AVAILABLE,@Spark}
2025-04-29T11:43:43,112 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@d6f93f5{/metrics/json,null,AVAILABLE,@Spark}
2025-04-29T11:43:43,123 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@146d3fff{/metrics/json,null,AVAILABLE,@Spark}
2025-04-29T11:43:43,131 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - I have been elected leader! New state: ALIVE
2025-04-29T11:43:43,262 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 151 ms (0 ms spent in bootstraps)
2025-04-29T11:43:43,289 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 130 ms (0 ms spent in bootstraps)
2025-04-29T11:43:43,351 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 173 ms (0 ms spent in bootstraps)
2025-04-29T11:43:43,596 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.master.Master [] - Registering worker 172.18.0.4:36391 with 2 cores, 3.0 GiB RAM
2025-04-29T11:43:43,612 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.master.Master [] - Registering worker 172.18.0.5:38821 with 2 cores, 3.0 GiB RAM
2025-04-29T11:43:43,616 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.master.Master [] - Registering worker 172.18.0.3:34443 with 2 cores, 3.0 GiB RAM
2025-04-29T11:43:43,624 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.worker.Worker [] - Successfully registered with master spark://bb9210fe839d:7077
2025-04-29T11:43:43,636 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.worker.Worker [] - Successfully registered with master spark://bb9210fe839d:7077
2025-04-29T11:43:43,639 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Successfully registered with master spark://bb9210fe839d:7077
2025-04-29T11:56:50,285 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-29T11:56:50,290 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-29T11:56:50,292 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-29T11:56:50,324 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T11:56:50,325 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-29T11:56:50,326 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T11:56:50,327 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: mod-2-pr-2.py
2025-04-29T11:56:50,350 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-29T11:56:50,359 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-29T11:56:50,361 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-29T11:56:50,424 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-29T11:56:50,425 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-29T11:56:50,426 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T11:56:50,427 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T11:56:50,429 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-29T11:56:50,495 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-29T11:56:50,787 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 40223.
2025-04-29T11:56:50,838 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-29T11:56:50,884 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-29T11:56:50,910 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-29T11:56:50,911 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-29T11:56:50,920 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-29T11:56:50,953 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-1dde5278-0c34-4c67-92d3-dce2a528332f
2025-04-29T11:56:50,973 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-29T11:56:50,993 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-29T11:56:51,048 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3518ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-29T11:56:51,209 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-29T11:56:51,228 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-29T11:56:51,265 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @3738ms
2025-04-29T11:56:51,321 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@35a35547{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-29T11:56:51,323 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-29T11:56:51,371 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@72626681{/,null,AVAILABLE,@Spark}
2025-04-29T11:56:51,528 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-29T11:56:51,600 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 45 ms (0 ms spent in bootstraps)
2025-04-29T11:56:51,730 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - Registering app mod-2-pr-2.py
2025-04-29T11:56:51,759 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - Registered app mod-2-pr-2.py with ID app-20250429115651-0000
2025-04-29T11:56:51,765 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250429115651-0000 with rpId: 0
2025-04-29T11:56:51,771 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250429115651-0000
2025-04-29T11:56:51,788 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32911.
2025-04-29T11:56:51,789 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on bb9210fe839d:32911
2025-04-29T11:56:51,792 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-29T11:56:51,796 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250429115651-0000/0 on worker worker-20250429114342-172.18.0.5-38821
2025-04-29T11:56:51,811 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, bb9210fe839d, 32911, None)
2025-04-29T11:56:51,815 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250429115651-0000/1 on worker worker-20250429114342-172.18.0.4-36391
2025-04-29T11:56:51,818 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager bb9210fe839d:32911 with 434.4 MiB RAM, BlockManagerId(driver, bb9210fe839d, 32911, None)
2025-04-29T11:56:51,822 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250429115651-0000/0 on worker-20250429114342-172.18.0.5-38821 (172.18.0.5:38821) with 2 core(s)
2025-04-29T11:56:51,825 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, bb9210fe839d, 32911, None)
2025-04-29T11:56:51,827 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250429115651-0000/2 on worker worker-20250429114342-172.18.0.3-34443
2025-04-29T11:56:51,832 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, bb9210fe839d, 32911, None)
2025-04-29T11:56:51,834 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250429115651-0000/0 on hostPort 172.18.0.5:38821 with 2 core(s), 1024.0 MiB RAM
2025-04-29T11:56:51,837 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250429115651-0000/1 on worker-20250429114342-172.18.0.4-36391 (172.18.0.4:36391) with 2 core(s)
2025-04-29T11:56:51,839 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250429115651-0000/1 on hostPort 172.18.0.4:36391 with 2 core(s), 1024.0 MiB RAM
2025-04-29T11:56:51,840 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250429115651-0000/2 on worker-20250429114342-172.18.0.3-34443 (172.18.0.3:34443) with 2 core(s)
2025-04-29T11:56:51,841 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250429115651-0000/2 on hostPort 172.18.0.3:34443 with 2 core(s), 1024.0 MiB RAM
2025-04-29T11:56:51,915 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250429115651-0000/0 for mod-2-pr-2.py
2025-04-29T11:56:51,933 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250429115651-0000/1 for mod-2-pr-2.py
2025-04-29T11:56:51,940 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250429115651-0000/2 for mod-2-pr-2.py
2025-04-29T11:56:51,987 [ExecutorRunner for app-20250429115651-0000/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-29T11:56:51,988 [ExecutorRunner for app-20250429115651-0000/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-29T11:56:51,989 [ExecutorRunner for app-20250429115651-0000/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T11:56:51,989 [ExecutorRunner for app-20250429115651-0000/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T11:56:51,990 [ExecutorRunner for app-20250429115651-0000/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-29T11:56:51,999 [ExecutorRunner for app-20250429115651-0000/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-29T11:56:52,000 [ExecutorRunner for app-20250429115651-0000/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-29T11:56:52,001 [ExecutorRunner for app-20250429115651-0000/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T11:56:52,002 [ExecutorRunner for app-20250429115651-0000/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T11:56:52,003 [ExecutorRunner for app-20250429115651-0000/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-29T11:56:52,003 [ExecutorRunner for app-20250429115651-0000/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-29T11:56:52,004 [ExecutorRunner for app-20250429115651-0000/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-29T11:56:52,005 [ExecutorRunner for app-20250429115651-0000/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T11:56:52,006 [ExecutorRunner for app-20250429115651-0000/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T11:56:52,007 [ExecutorRunner for app-20250429115651-0000/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-29T11:56:52,030 [ExecutorRunner for app-20250429115651-0000/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=40223" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:40223" "--executor-id" "0" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250429115651-0000" "--worker-url" "spark://Worker@172.18.0.5:38821" "--resourceProfileId" "0"
2025-04-29T11:56:52,052 [ExecutorRunner for app-20250429115651-0000/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=40223" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:40223" "--executor-id" "2" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250429115651-0000" "--worker-url" "spark://Worker@172.18.0.3:34443" "--resourceProfileId" "0"
2025-04-29T11:56:52,057 [ExecutorRunner for app-20250429115651-0000/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=40223" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:40223" "--executor-id" "1" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250429115651-0000" "--worker-url" "spark://Worker@172.18.0.4:36391" "--resourceProfileId" "0"
2025-04-29T11:56:52,131 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250429115651-0000 with rpId: 0
2025-04-29T11:56:52,135 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250429115651-0000 with rpId: 0
2025-04-29T11:56:52,139 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250429115651-0000 with rpId: 0
2025-04-29T11:56:52,171 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250429115651-0000/0 is now RUNNING
2025-04-29T11:56:52,185 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250429115651-0000/2 is now RUNNING
2025-04-29T11:56:52,189 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250429115651-0000/1 is now RUNNING
2025-04-29T11:56:52,479 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250429115651-0000.inprogress
2025-04-29T11:56:52,842 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@72626681{/,null,STOPPED,@Spark}
2025-04-29T11:56:52,850 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@216286d9{/jobs,null,AVAILABLE,@Spark}
2025-04-29T11:56:52,852 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@568d6d9e{/jobs/json,null,AVAILABLE,@Spark}
2025-04-29T11:56:52,855 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@511da534{/jobs/job,null,AVAILABLE,@Spark}
2025-04-29T11:56:52,872 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6c6c8b41{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-29T11:56:52,876 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@32e27e33{/stages,null,AVAILABLE,@Spark}
2025-04-29T11:56:52,889 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2ee3ea7e{/stages/json,null,AVAILABLE,@Spark}
2025-04-29T11:56:52,893 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@180278cd{/stages/stage,null,AVAILABLE,@Spark}
2025-04-29T11:56:52,897 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@24f3fca1{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-29T11:56:52,899 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@16f744e2{/stages/pool,null,AVAILABLE,@Spark}
2025-04-29T11:56:52,908 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7878574f{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-29T11:56:52,911 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@df9bf0d{/storage,null,AVAILABLE,@Spark}
2025-04-29T11:56:52,914 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@59514c7a{/storage/json,null,AVAILABLE,@Spark}
2025-04-29T11:56:52,917 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@116185c2{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-29T11:56:52,921 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6c6c39c3{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-29T11:56:52,930 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@220439f6{/environment,null,AVAILABLE,@Spark}
2025-04-29T11:56:52,940 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7c738b1b{/environment/json,null,AVAILABLE,@Spark}
2025-04-29T11:56:52,945 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@77c7909c{/executors,null,AVAILABLE,@Spark}
2025-04-29T11:56:52,952 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6f7863f{/executors/json,null,AVAILABLE,@Spark}
2025-04-29T11:56:52,959 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@479914b1{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-29T11:56:52,966 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7c4d6797{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-29T11:56:52,974 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@765cae9f{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-29T11:56:52,986 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@29fbc798{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-29T11:56:53,009 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3ba2a30b{/static,null,AVAILABLE,@Spark}
2025-04-29T11:56:53,013 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4cf4795b{/,null,AVAILABLE,@Spark}
2025-04-29T11:56:53,019 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@65b4c46a{/api,null,AVAILABLE,@Spark}
2025-04-29T11:56:53,027 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@dccb9ee{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-29T11:56:53,031 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6b47c480{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-29T11:56:53,047 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@73ca6bf9{/metrics/json,null,AVAILABLE,@Spark}
2025-04-29T11:56:53,051 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-29T11:56:53,788 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Invoking stop() from shutdown hook
2025-04-29T11:56:53,790 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-29T11:56:53,814 [shutdown-hook-0] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@35a35547{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-29T11:56:53,823 [shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://bb9210fe839d:4040
2025-04-29T11:56:53,832 [shutdown-hook-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-29T11:56:53,848 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-29T11:56:53,872 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250429115651-0000
2025-04-29T11:56:53,880 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250429115651-0000
2025-04-29T11:56:53,940 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250429115651-0000/2
2025-04-29T11:56:53,943 [ExecutorRunner for app-20250429115651-0000/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250429115651-0000/2 interrupted
2025-04-29T11:56:53,947 [ExecutorRunner for app-20250429115651-0000/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-29T11:56:53,948 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250429115651-0000/1
2025-04-29T11:56:53,958 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250429115651-0000/0
2025-04-29T11:56:53,963 [ExecutorRunner for app-20250429115651-0000/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250429115651-0000/1 interrupted
2025-04-29T11:56:53,968 [ExecutorRunner for app-20250429115651-0000/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-29T11:56:53,967 [ExecutorRunner for app-20250429115651-0000/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250429115651-0000/0 interrupted
2025-04-29T11:56:53,975 [dispatcher-event-loop-0] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-29T11:56:53,979 [ExecutorRunner for app-20250429115651-0000/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-29T11:56:54,007 [dispatcher-event-loop-1] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250429115651-0000/2
2025-04-29T11:56:54,009 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250429115651-0000/2 finished with state KILLED exitStatus 143
2025-04-29T11:56:54,015 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-29T11:56:54,017 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250429115651-0000, execId=2)
2025-04-29T11:56:54,022 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250429115651-0000/1 finished with state KILLED exitStatus 143
2025-04-29T11:56:54,023 [dispatcher-event-loop-7] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250429115651-0000/1
2025-04-29T11:56:54,023 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250429115651-0000 removed, cleanupLocalDirs = true
2025-04-29T11:56:54,024 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250429115651-0000
2025-04-29T11:56:54,030 [dispatcher-event-loop-5] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250429115651-0000/0
2025-04-29T11:56:54,031 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250429115651-0000/0 finished with state KILLED exitStatus 143
2025-04-29T11:56:54,029 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-29T11:56:54,036 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250429115651-0000, execId=1)
2025-04-29T11:56:54,036 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-29T11:56:54,037 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250429115651-0000, execId=0)
2025-04-29T11:56:54,040 [shutdown-hook-0] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-29T11:56:54,040 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250429115651-0000 removed, cleanupLocalDirs = true
2025-04-29T11:56:54,040 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250429115651-0000
2025-04-29T11:56:54,041 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-29T11:56:54,042 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250429115651-0000
2025-04-29T11:56:54,042 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250429115651-0000 removed, cleanupLocalDirs = true
2025-04-29T11:56:54,053 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-29T11:56:54,058 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-29T11:56:54,069 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:54826 got disassociated, removing it.
2025-04-29T11:56:54,071 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - bb9210fe839d:40223 got disassociated, removing it.
2025-04-29T11:56:54,073 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-29T11:56:54,074 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-29T11:56:54,076 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-7392d2a1-038c-4290-890f-f283feab4139
2025-04-29T11:56:54,082 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-7392d2a1-038c-4290-890f-f283feab4139/pyspark-1cfd0453-bb4a-4bf4-9ad7-5297ed8cfc31
2025-04-29T11:56:54,087 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-aa9df799-fdaf-4e71-a9e7-2750fff9548a
2025-04-29T12:01:27,428 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-29T12:01:27,434 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-29T12:01:27,435 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-29T12:01:27,467 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T12:01:27,468 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-29T12:01:27,469 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T12:01:27,471 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: mod-2-pr-2.py
2025-04-29T12:01:27,500 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-29T12:01:27,514 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-29T12:01:27,515 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-29T12:01:27,582 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-29T12:01:27,584 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-29T12:01:27,585 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T12:01:27,588 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T12:01:27,589 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-29T12:01:27,654 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-29T12:01:27,986 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 39823.
2025-04-29T12:01:28,023 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-29T12:01:28,064 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-29T12:01:28,086 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-29T12:01:28,088 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-29T12:01:28,095 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-29T12:01:28,115 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-6cff1d1e-5e6d-490d-87f7-13555b07105e
2025-04-29T12:01:28,133 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-29T12:01:28,153 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-29T12:01:28,207 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @4280ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-29T12:01:28,397 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-29T12:01:28,412 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-29T12:01:28,438 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @4513ms
2025-04-29T12:01:28,474 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@413b8567{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-29T12:01:28,475 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-29T12:01:28,500 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1320cce0{/,null,AVAILABLE,@Spark}
2025-04-29T12:01:28,621 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-29T12:01:28,685 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 40 ms (0 ms spent in bootstraps)
2025-04-29T12:01:28,792 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Registering app mod-2-pr-2.py
2025-04-29T12:01:28,795 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Registered app mod-2-pr-2.py with ID app-20250429120128-0001
2025-04-29T12:01:28,796 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250429120128-0001 with rpId: 0
2025-04-29T12:01:28,798 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250429120128-0001/0 on worker worker-20250429114342-172.18.0.5-38821
2025-04-29T12:01:28,800 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250429120128-0001/1 on worker worker-20250429114342-172.18.0.4-36391
2025-04-29T12:01:28,801 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250429120128-0001/2 on worker worker-20250429114342-172.18.0.3-34443
2025-04-29T12:01:28,804 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250429120128-0001
2025-04-29T12:01:28,808 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250429120128-0001/0 on worker-20250429114342-172.18.0.5-38821 (172.18.0.5:38821) with 2 core(s)
2025-04-29T12:01:28,808 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250429120128-0001/0 for mod-2-pr-2.py
2025-04-29T12:01:28,812 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250429120128-0001/1 for mod-2-pr-2.py
2025-04-29T12:01:28,813 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250429120128-0001/0 on hostPort 172.18.0.5:38821 with 2 core(s), 1024.0 MiB RAM
2025-04-29T12:01:28,813 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250429120128-0001/2 for mod-2-pr-2.py
2025-04-29T12:01:28,814 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250429120128-0001/1 on worker-20250429114342-172.18.0.4-36391 (172.18.0.4:36391) with 2 core(s)
2025-04-29T12:01:28,815 [ExecutorRunner for app-20250429120128-0001/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-29T12:01:28,816 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250429120128-0001/1 on hostPort 172.18.0.4:36391 with 2 core(s), 1024.0 MiB RAM
2025-04-29T12:01:28,816 [ExecutorRunner for app-20250429120128-0001/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-29T12:01:28,818 [ExecutorRunner for app-20250429120128-0001/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T12:01:28,818 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250429120128-0001/2 on worker-20250429114342-172.18.0.3-34443 (172.18.0.3:34443) with 2 core(s)
2025-04-29T12:01:28,819 [ExecutorRunner for app-20250429120128-0001/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T12:01:28,820 [ExecutorRunner for app-20250429120128-0001/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-29T12:01:28,820 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250429120128-0001/2 on hostPort 172.18.0.3:34443 with 2 core(s), 1024.0 MiB RAM
2025-04-29T12:01:28,820 [ExecutorRunner for app-20250429120128-0001/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-29T12:01:28,822 [ExecutorRunner for app-20250429120128-0001/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-29T12:01:28,823 [ExecutorRunner for app-20250429120128-0001/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T12:01:28,824 [ExecutorRunner for app-20250429120128-0001/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T12:01:28,824 [ExecutorRunner for app-20250429120128-0001/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-29T12:01:28,824 [ExecutorRunner for app-20250429120128-0001/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-29T12:01:28,825 [ExecutorRunner for app-20250429120128-0001/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-29T12:01:28,825 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33441.
2025-04-29T12:01:28,825 [ExecutorRunner for app-20250429120128-0001/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T12:01:28,826 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on bb9210fe839d:33441
2025-04-29T12:01:28,826 [ExecutorRunner for app-20250429120128-0001/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T12:01:28,827 [ExecutorRunner for app-20250429120128-0001/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-29T12:01:28,829 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-29T12:01:28,842 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, bb9210fe839d, 33441, None)
2025-04-29T12:01:28,847 [ExecutorRunner for app-20250429120128-0001/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=39823" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:39823" "--executor-id" "0" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250429120128-0001" "--worker-url" "spark://Worker@172.18.0.5:38821" "--resourceProfileId" "0"
2025-04-29T12:01:28,850 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager bb9210fe839d:33441 with 434.4 MiB RAM, BlockManagerId(driver, bb9210fe839d, 33441, None)
2025-04-29T12:01:28,855 [ExecutorRunner for app-20250429120128-0001/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=39823" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:39823" "--executor-id" "1" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250429120128-0001" "--worker-url" "spark://Worker@172.18.0.4:36391" "--resourceProfileId" "0"
2025-04-29T12:01:28,857 [ExecutorRunner for app-20250429120128-0001/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=39823" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:39823" "--executor-id" "2" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250429120128-0001" "--worker-url" "spark://Worker@172.18.0.3:34443" "--resourceProfileId" "0"
2025-04-29T12:01:28,859 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, bb9210fe839d, 33441, None)
2025-04-29T12:01:28,863 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, bb9210fe839d, 33441, None)
2025-04-29T12:01:28,867 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250429120128-0001 with rpId: 0
2025-04-29T12:01:28,870 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250429120128-0001 with rpId: 0
2025-04-29T12:01:28,876 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250429120128-0001 with rpId: 0
2025-04-29T12:01:28,905 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250429120128-0001/0 is now RUNNING
2025-04-29T12:01:28,908 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250429120128-0001/1 is now RUNNING
2025-04-29T12:01:28,913 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250429120128-0001/2 is now RUNNING
2025-04-29T12:01:29,408 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250429120128-0001.inprogress
2025-04-29T12:01:29,722 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@1320cce0{/,null,STOPPED,@Spark}
2025-04-29T12:01:29,726 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40857667{/jobs,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,728 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7fa04fb2{/jobs/json,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,730 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@35083451{/jobs/job,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,733 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@354b22b6{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,736 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@484d933e{/stages,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,743 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7274d8f2{/stages/json,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,753 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2dab95f3{/stages/stage,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,791 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4622b9f0{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,810 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@611e343f{/stages/pool,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,832 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@56578f8c{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,843 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@fa43f57{/storage,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,849 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2a9cdf28{/storage/json,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,859 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@41d8187c{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,869 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@251ea385{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,880 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@d678b60{/environment,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,885 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@ef17b66{/environment/json,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,900 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@58ad57bd{/executors,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,904 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1cadc900{/executors/json,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,909 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40aadd59{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,913 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3eeb6954{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,916 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1964fe85{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,918 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@52124300{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,936 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@76c527e1{/static,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,940 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@548a25dc{/,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,946 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@28825700{/api,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,949 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@611d70f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,954 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@21e7c069{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,975 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@64ddd77d{/metrics/json,null,AVAILABLE,@Spark}
2025-04-29T12:01:29,978 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-29T12:01:30,672 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Invoking stop() from shutdown hook
2025-04-29T12:01:30,674 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-29T12:01:30,692 [shutdown-hook-0] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@413b8567{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-29T12:01:30,704 [shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://bb9210fe839d:4040
2025-04-29T12:01:30,727 [shutdown-hook-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-29T12:01:30,736 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-29T12:01:30,751 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250429120128-0001
2025-04-29T12:01:30,754 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250429120128-0001
2025-04-29T12:01:30,762 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250429120128-0001/0
2025-04-29T12:01:30,762 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250429120128-0001/2
2025-04-29T12:01:30,765 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250429120128-0001/1
2025-04-29T12:01:30,773 [ExecutorRunner for app-20250429120128-0001/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250429120128-0001/1 interrupted
2025-04-29T12:01:30,774 [ExecutorRunner for app-20250429120128-0001/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250429120128-0001/0 interrupted
2025-04-29T12:01:30,775 [ExecutorRunner for app-20250429120128-0001/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-29T12:01:30,775 [ExecutorRunner for app-20250429120128-0001/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-29T12:01:30,779 [ExecutorRunner for app-20250429120128-0001/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250429120128-0001/2 interrupted
2025-04-29T12:01:30,785 [ExecutorRunner for app-20250429120128-0001/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-29T12:01:30,813 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250429120128-0001/0 finished with state KILLED exitStatus 143
2025-04-29T12:01:30,819 [dispatcher-event-loop-9] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250429120128-0001/0
2025-04-29T12:01:30,826 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-29T12:01:30,827 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250429120128-0001, execId=0)
2025-04-29T12:01:30,831 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250429120128-0001 removed, cleanupLocalDirs = true
2025-04-29T12:01:30,831 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250429120128-0001
2025-04-29T12:01:30,834 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250429120128-0001/1 finished with state KILLED exitStatus 143
2025-04-29T12:01:30,836 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-29T12:01:30,840 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250429120128-0001/2 finished with state KILLED exitStatus 143
2025-04-29T12:01:30,841 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-29T12:01:30,842 [dispatcher-event-loop-11] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250429120128-0001/2
2025-04-29T12:01:30,842 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250429120128-0001, execId=2)
2025-04-29T12:01:30,841 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250429120128-0001, execId=1)
2025-04-29T12:01:30,844 [dispatcher-event-loop-0] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250429120128-0001/1
2025-04-29T12:01:30,845 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250429120128-0001 removed, cleanupLocalDirs = true
2025-04-29T12:01:30,845 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250429120128-0001
2025-04-29T12:01:30,846 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250429120128-0001 removed, cleanupLocalDirs = true
2025-04-29T12:01:30,845 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250429120128-0001
2025-04-29T12:01:30,872 [dispatcher-event-loop-10] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-29T12:01:30,899 [shutdown-hook-0] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-29T12:01:30,900 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-29T12:01:30,908 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-29T12:01:30,911 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-29T12:01:30,914 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:57912 got disassociated, removing it.
2025-04-29T12:01:30,915 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - bb9210fe839d:39823 got disassociated, removing it.
2025-04-29T12:01:30,921 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-29T12:01:30,921 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-29T12:01:30,922 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-c172367a-95ae-467d-8a10-45fbfb2d3fba
2025-04-29T12:01:30,927 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-7438959b-6ac8-4607-a669-697e5d6ef1f0/pyspark-5b98ff7c-27ad-4ff3-bd42-63a86c3b1b09
2025-04-29T12:01:30,930 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-7438959b-6ac8-4607-a669-697e5d6ef1f0
2025-04-29T12:04:11,837 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-29T12:04:11,844 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-29T12:04:11,845 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-29T12:04:11,886 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T12:04:11,887 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-29T12:04:11,889 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T12:04:11,891 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: mod-2-pr-2.py
2025-04-29T12:04:11,918 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-29T12:04:11,931 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-29T12:04:11,933 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-29T12:04:11,998 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-29T12:04:11,999 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-29T12:04:12,001 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T12:04:12,002 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T12:04:12,003 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-29T12:04:12,129 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-29T12:04:12,667 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 33715.
2025-04-29T12:04:12,729 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-29T12:04:12,786 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-29T12:04:12,819 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-29T12:04:12,822 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-29T12:04:12,832 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-29T12:04:12,863 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-bcf9226d-07aa-49f8-9553-b723413860ab
2025-04-29T12:04:12,889 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-29T12:04:12,925 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-29T12:04:13,009 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @4474ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-29T12:04:13,206 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-29T12:04:13,233 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-29T12:04:13,281 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @4749ms
2025-04-29T12:04:13,357 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@2a2301eb{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-29T12:04:13,363 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-29T12:04:13,407 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2f72c3c0{/,null,AVAILABLE,@Spark}
2025-04-29T12:04:13,630 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-29T12:04:13,719 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 53 ms (0 ms spent in bootstraps)
2025-04-29T12:04:13,831 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Registering app mod-2-pr-2.py
2025-04-29T12:04:13,834 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Registered app mod-2-pr-2.py with ID app-20250429120413-0002
2025-04-29T12:04:13,836 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250429120413-0002 with rpId: 0
2025-04-29T12:04:13,838 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250429120413-0002/0 on worker worker-20250429114342-172.18.0.5-38821
2025-04-29T12:04:13,841 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250429120413-0002/1 on worker worker-20250429114342-172.18.0.4-36391
2025-04-29T12:04:13,844 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250429120413-0002/2 on worker worker-20250429114342-172.18.0.3-34443
2025-04-29T12:04:13,845 [dispatcher-event-loop-11] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250429120413-0002
2025-04-29T12:04:13,850 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250429120413-0002/0 for mod-2-pr-2.py
2025-04-29T12:04:13,851 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250429120413-0002/1 for mod-2-pr-2.py
2025-04-29T12:04:13,852 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250429120413-0002/0 on worker-20250429114342-172.18.0.5-38821 (172.18.0.5:38821) with 2 core(s)
2025-04-29T12:04:13,858 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250429120413-0002/0 on hostPort 172.18.0.5:38821 with 2 core(s), 1024.0 MiB RAM
2025-04-29T12:04:13,859 [ExecutorRunner for app-20250429120413-0002/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-29T12:04:13,859 [ExecutorRunner for app-20250429120413-0002/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-29T12:04:13,860 [ExecutorRunner for app-20250429120413-0002/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-29T12:04:13,858 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250429120413-0002/2 for mod-2-pr-2.py
2025-04-29T12:04:13,860 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250429120413-0002/1 on worker-20250429114342-172.18.0.4-36391 (172.18.0.4:36391) with 2 core(s)
2025-04-29T12:04:13,861 [ExecutorRunner for app-20250429120413-0002/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-29T12:04:13,862 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250429120413-0002/1 on hostPort 172.18.0.4:36391 with 2 core(s), 1024.0 MiB RAM
2025-04-29T12:04:13,863 [ExecutorRunner for app-20250429120413-0002/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T12:04:13,864 [ExecutorRunner for app-20250429120413-0002/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T12:04:13,865 [ExecutorRunner for app-20250429120413-0002/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T12:04:13,865 [ExecutorRunner for app-20250429120413-0002/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T12:04:13,865 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250429120413-0002/2 on worker-20250429114342-172.18.0.3-34443 (172.18.0.3:34443) with 2 core(s)
2025-04-29T12:04:13,867 [ExecutorRunner for app-20250429120413-0002/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-29T12:04:13,867 [ExecutorRunner for app-20250429120413-0002/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-29T12:04:13,867 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250429120413-0002/2 on hostPort 172.18.0.3:34443 with 2 core(s), 1024.0 MiB RAM
2025-04-29T12:04:13,870 [ExecutorRunner for app-20250429120413-0002/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-29T12:04:13,870 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36507.
2025-04-29T12:04:13,870 [ExecutorRunner for app-20250429120413-0002/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-29T12:04:13,871 [ExecutorRunner for app-20250429120413-0002/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T12:04:13,871 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on bb9210fe839d:36507
2025-04-29T12:04:13,874 [ExecutorRunner for app-20250429120413-0002/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T12:04:13,876 [ExecutorRunner for app-20250429120413-0002/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-29T12:04:13,878 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-29T12:04:13,893 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, bb9210fe839d, 36507, None)
2025-04-29T12:04:13,897 [ExecutorRunner for app-20250429120413-0002/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=33715" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:33715" "--executor-id" "0" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250429120413-0002" "--worker-url" "spark://Worker@172.18.0.5:38821" "--resourceProfileId" "0"
2025-04-29T12:04:13,898 [ExecutorRunner for app-20250429120413-0002/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=33715" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:33715" "--executor-id" "1" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250429120413-0002" "--worker-url" "spark://Worker@172.18.0.4:36391" "--resourceProfileId" "0"
2025-04-29T12:04:13,904 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager bb9210fe839d:36507 with 434.4 MiB RAM, BlockManagerId(driver, bb9210fe839d, 36507, None)
2025-04-29T12:04:13,906 [ExecutorRunner for app-20250429120413-0002/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=33715" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:33715" "--executor-id" "2" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250429120413-0002" "--worker-url" "spark://Worker@172.18.0.3:34443" "--resourceProfileId" "0"
2025-04-29T12:04:13,916 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, bb9210fe839d, 36507, None)
2025-04-29T12:04:13,920 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, bb9210fe839d, 36507, None)
2025-04-29T12:04:13,930 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250429120413-0002 with rpId: 0
2025-04-29T12:04:13,934 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250429120413-0002 with rpId: 0
2025-04-29T12:04:13,953 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250429120413-0002 with rpId: 0
2025-04-29T12:04:13,988 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250429120413-0002/1 is now RUNNING
2025-04-29T12:04:13,999 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250429120413-0002/0 is now RUNNING
2025-04-29T12:04:14,001 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250429120413-0002/2 is now RUNNING
2025-04-29T12:04:14,713 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250429120413-0002.inprogress
2025-04-29T12:04:15,006 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@2f72c3c0{/,null,STOPPED,@Spark}
2025-04-29T12:04:15,014 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7287b184{/jobs,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,020 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@efb1f4{/jobs/json,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,044 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@43ebfa73{/jobs/job,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,050 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@28e2aeeb{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,053 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3ed8bd3d{/stages,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,066 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3915b297{/stages/json,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,071 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@518a5e73{/stages/stage,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,084 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@69163bf1{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,091 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@315c40fd{/stages/pool,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,102 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7f1ed11d{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,110 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@29fdd570{/storage,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,121 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7a4a5cf0{/storage/json,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,135 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6eb60a32{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,173 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1ad7d5a1{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,188 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@55b9f894{/environment,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,207 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@56041559{/environment/json,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,214 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@43ce77c8{/executors,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,219 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1288e51d{/executors/json,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,223 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@313b917c{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,232 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@42e22e1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,236 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@36e20084{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,241 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4ed622ff{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,274 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4ddb2236{/static,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,284 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3244d86f{/,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,297 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5cbcb4b8{/api,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,309 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3092c8e3{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,321 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6dd65270{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,339 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1db22e23{/metrics/json,null,AVAILABLE,@Spark}
2025-04-29T12:04:15,353 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-29T12:04:16,475 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Invoking stop() from shutdown hook
2025-04-29T12:04:16,478 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-29T12:04:16,520 [shutdown-hook-0] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@2a2301eb{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-29T12:04:16,550 [shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://bb9210fe839d:4040
2025-04-29T12:04:16,593 [shutdown-hook-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-29T12:04:16,751 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-29T12:04:16,807 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250429120413-0002
2025-04-29T12:04:16,818 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250429120413-0002
2025-04-29T12:04:16,944 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250429120413-0002/0
2025-04-29T12:04:16,947 [ExecutorRunner for app-20250429120413-0002/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250429120413-0002/0 interrupted
2025-04-29T12:04:16,948 [ExecutorRunner for app-20250429120413-0002/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-29T12:04:16,952 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250429120413-0002/1
2025-04-29T12:04:16,957 [ExecutorRunner for app-20250429120413-0002/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250429120413-0002/1 interrupted
2025-04-29T12:04:16,961 [ExecutorRunner for app-20250429120413-0002/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-29T12:04:16,966 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250429120413-0002/2
2025-04-29T12:04:16,971 [ExecutorRunner for app-20250429120413-0002/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250429120413-0002/2 interrupted
2025-04-29T12:04:16,974 [ExecutorRunner for app-20250429120413-0002/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-29T12:04:17,011 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250429120413-0002/1 finished with state KILLED exitStatus 143
2025-04-29T12:04:17,015 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-29T12:04:17,018 [dispatcher-event-loop-2] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250429120413-0002/1
2025-04-29T12:04:17,026 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250429120413-0002, execId=1)
2025-04-29T12:04:17,045 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250429120413-0002/0 finished with state KILLED exitStatus 143
2025-04-29T12:04:17,046 [dispatcher-event-loop-1] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-29T12:04:17,069 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-29T12:04:17,069 [dispatcher-event-loop-2] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250429120413-0002/0
2025-04-29T12:04:17,070 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250429120413-0002 removed, cleanupLocalDirs = true
2025-04-29T12:04:17,076 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250429120413-0002, execId=0)
2025-04-29T12:04:17,079 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250429120413-0002 removed, cleanupLocalDirs = true
2025-04-29T12:04:17,079 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250429120413-0002
2025-04-29T12:04:17,074 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250429120413-0002
2025-04-29T12:04:17,085 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250429120413-0002/2 finished with state KILLED exitStatus 143
2025-04-29T12:04:17,086 [dispatcher-event-loop-8] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-29T12:04:17,087 [dispatcher-event-loop-7] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250429120413-0002/2
2025-04-29T12:04:17,087 [dispatcher-event-loop-8] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250429120413-0002, execId=2)
2025-04-29T12:04:17,089 [dispatcher-event-loop-8] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250429120413-0002 removed, cleanupLocalDirs = true
2025-04-29T12:04:17,090 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250429120413-0002
2025-04-29T12:04:17,107 [shutdown-hook-0] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-29T12:04:17,109 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-29T12:04:17,117 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-29T12:04:17,121 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-29T12:04:17,132 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:51066 got disassociated, removing it.
2025-04-29T12:04:17,133 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - bb9210fe839d:33715 got disassociated, removing it.
2025-04-29T12:04:17,140 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-29T12:04:17,142 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-29T12:04:17,145 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-12226ce8-4d48-4b56-b1de-b427d31b325e
2025-04-29T12:04:17,157 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-86491313-7ba3-4f89-90ad-6f7b3531df93
2025-04-29T12:04:17,168 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-86491313-7ba3-4f89-90ad-6f7b3531df93/pyspark-50036f96-c43b-4231-b93a-4379ab62b475
2025-04-29T12:10:17,425 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-29T12:10:17,430 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-29T12:10:17,432 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-29T12:10:17,464 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T12:10:17,466 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-29T12:10:17,467 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T12:10:17,468 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: mod-2-pr-2.py
2025-04-29T12:10:17,490 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 512, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-29T12:10:17,502 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-29T12:10:17,503 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-29T12:10:17,555 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-29T12:10:17,556 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-29T12:10:17,557 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T12:10:17,558 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T12:10:17,559 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-29T12:10:17,621 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-29T12:10:17,898 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 32837.
2025-04-29T12:10:17,925 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-29T12:10:17,957 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-29T12:10:17,974 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-29T12:10:17,976 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-29T12:10:17,982 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-29T12:10:18,004 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-49d43bc5-ec4d-4bcf-a310-299b9690663c
2025-04-29T12:10:18,023 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-29T12:10:18,043 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-29T12:10:18,102 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3265ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-29T12:10:18,191 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-29T12:10:18,203 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-29T12:10:18,223 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @3386ms
2025-04-29T12:10:18,253 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@5f2f1763{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-29T12:10:18,254 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-29T12:10:18,279 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@47227198{/,null,AVAILABLE,@Spark}
2025-04-29T12:10:18,404 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-29T12:10:18,455 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 29 ms (0 ms spent in bootstraps)
2025-04-29T12:10:18,547 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Registering app mod-2-pr-2.py
2025-04-29T12:10:18,549 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Registered app mod-2-pr-2.py with ID app-20250429121018-0003
2025-04-29T12:10:18,551 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250429121018-0003 with rpId: 0
2025-04-29T12:10:18,553 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250429121018-0003/0 on worker worker-20250429114342-172.18.0.5-38821
2025-04-29T12:10:18,554 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250429121018-0003/1 on worker worker-20250429114342-172.18.0.4-36391
2025-04-29T12:10:18,555 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250429121018-0003/2 on worker worker-20250429114342-172.18.0.3-34443
2025-04-29T12:10:18,558 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250429121018-0003
2025-04-29T12:10:18,560 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250429121018-0003/1 for mod-2-pr-2.py
2025-04-29T12:10:18,561 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250429121018-0003/0 for mod-2-pr-2.py
2025-04-29T12:10:18,563 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250429121018-0003/0 on worker-20250429114342-172.18.0.5-38821 (172.18.0.5:38821) with 2 core(s)
2025-04-29T12:10:18,566 [ExecutorRunner for app-20250429121018-0003/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-29T12:10:18,567 [ExecutorRunner for app-20250429121018-0003/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-29T12:10:18,567 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250429121018-0003/2 for mod-2-pr-2.py
2025-04-29T12:10:18,568 [ExecutorRunner for app-20250429121018-0003/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T12:10:18,568 [ExecutorRunner for app-20250429121018-0003/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T12:10:18,568 [ExecutorRunner for app-20250429121018-0003/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-29T12:10:18,569 [ExecutorRunner for app-20250429121018-0003/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-29T12:10:18,569 [ExecutorRunner for app-20250429121018-0003/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-29T12:10:18,570 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250429121018-0003/0 on hostPort 172.18.0.5:38821 with 2 core(s), 512.0 MiB RAM
2025-04-29T12:10:18,571 [ExecutorRunner for app-20250429121018-0003/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T12:10:18,571 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250429121018-0003/1 on worker-20250429114342-172.18.0.4-36391 (172.18.0.4:36391) with 2 core(s)
2025-04-29T12:10:18,571 [ExecutorRunner for app-20250429121018-0003/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T12:10:18,572 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250429121018-0003/1 on hostPort 172.18.0.4:36391 with 2 core(s), 512.0 MiB RAM
2025-04-29T12:10:18,572 [ExecutorRunner for app-20250429121018-0003/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-29T12:10:18,575 [ExecutorRunner for app-20250429121018-0003/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-29T12:10:18,576 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250429121018-0003/2 on worker-20250429114342-172.18.0.3-34443 (172.18.0.3:34443) with 2 core(s)
2025-04-29T12:10:18,577 [ExecutorRunner for app-20250429121018-0003/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-29T12:10:18,577 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34651.
2025-04-29T12:10:18,578 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on bb9210fe839d:34651
2025-04-29T12:10:18,578 [ExecutorRunner for app-20250429121018-0003/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T12:10:18,579 [ExecutorRunner for app-20250429121018-0003/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T12:10:18,582 [ExecutorRunner for app-20250429121018-0003/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-29T12:10:18,582 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250429121018-0003/2 on hostPort 172.18.0.3:34443 with 2 core(s), 512.0 MiB RAM
2025-04-29T12:10:18,582 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-29T12:10:18,592 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, bb9210fe839d, 34651, None)
2025-04-29T12:10:18,595 [ExecutorRunner for app-20250429121018-0003/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx512M" "-Dspark.driver.port=32837" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:32837" "--executor-id" "1" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250429121018-0003" "--worker-url" "spark://Worker@172.18.0.4:36391" "--resourceProfileId" "0"
2025-04-29T12:10:18,599 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager bb9210fe839d:34651 with 434.4 MiB RAM, BlockManagerId(driver, bb9210fe839d, 34651, None)
2025-04-29T12:10:18,602 [ExecutorRunner for app-20250429121018-0003/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx512M" "-Dspark.driver.port=32837" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:32837" "--executor-id" "0" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250429121018-0003" "--worker-url" "spark://Worker@172.18.0.5:38821" "--resourceProfileId" "0"
2025-04-29T12:10:18,605 [ExecutorRunner for app-20250429121018-0003/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx512M" "-Dspark.driver.port=32837" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:32837" "--executor-id" "2" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250429121018-0003" "--worker-url" "spark://Worker@172.18.0.3:34443" "--resourceProfileId" "0"
2025-04-29T12:10:18,608 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, bb9210fe839d, 34651, None)
2025-04-29T12:10:18,608 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250429121018-0003 with rpId: 0
2025-04-29T12:10:18,612 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, bb9210fe839d, 34651, None)
2025-04-29T12:10:18,619 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250429121018-0003 with rpId: 0
2025-04-29T12:10:18,627 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250429121018-0003 with rpId: 0
2025-04-29T12:10:18,640 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250429121018-0003/1 is now RUNNING
2025-04-29T12:10:18,642 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250429121018-0003/0 is now RUNNING
2025-04-29T12:10:18,648 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250429121018-0003/2 is now RUNNING
2025-04-29T12:10:19,043 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250429121018-0003.inprogress
2025-04-29T12:10:19,256 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@47227198{/,null,STOPPED,@Spark}
2025-04-29T12:10:19,259 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@e5e455c{/jobs,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,261 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5f7d9b82{/jobs/json,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,265 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@61a08f9{/jobs/job,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,268 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3c138251{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,271 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1ec6170e{/stages,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,274 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@52d5cde7{/stages/json,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,278 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5e975084{/stages/stage,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,281 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@10eb3fdc{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,286 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5694be8{/stages/pool,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,289 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1f0b4f45{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,292 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@405d0340{/storage,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,295 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6fa8cc59{/storage/json,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,302 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@69b75404{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,310 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@cfd797b{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,314 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5ea3e9bc{/environment,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,320 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@69a0fd62{/environment/json,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,325 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1982ab1d{/executors,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,329 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@72086c1a{/executors/json,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,332 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4c7c4c12{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,336 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@f32ba2f{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,344 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@37d62d67{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,354 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@48eb68af{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,376 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@70fe914b{/static,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,380 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@171c8b74{/,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,398 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7e69178e{/api,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,405 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@78436f07{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,410 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7f5515e3{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,423 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5324a0b1{/metrics/json,null,AVAILABLE,@Spark}
2025-04-29T12:10:19,427 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-29T12:10:19,901 [Thread-4] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-29T12:10:19,934 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@5f2f1763{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-29T12:10:19,943 [Thread-4] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://bb9210fe839d:4040
2025-04-29T12:10:19,956 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-29T12:10:19,961 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-29T12:10:19,980 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250429121018-0003
2025-04-29T12:10:19,981 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250429121018-0003
2025-04-29T12:10:19,987 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250429121018-0003/0
2025-04-29T12:10:19,989 [ExecutorRunner for app-20250429121018-0003/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250429121018-0003/0 interrupted
2025-04-29T12:10:19,989 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250429121018-0003/2
2025-04-29T12:10:19,990 [ExecutorRunner for app-20250429121018-0003/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250429121018-0003/2 interrupted
2025-04-29T12:10:19,991 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250429121018-0003/1
2025-04-29T12:10:19,990 [ExecutorRunner for app-20250429121018-0003/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-29T12:10:19,992 [ExecutorRunner for app-20250429121018-0003/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250429121018-0003/1 interrupted
2025-04-29T12:10:19,992 [ExecutorRunner for app-20250429121018-0003/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-29T12:10:19,992 [ExecutorRunner for app-20250429121018-0003/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-29T12:10:20,017 [dispatcher-event-loop-1] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-29T12:10:20,022 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250429121018-0003/1 finished with state KILLED exitStatus 143
2025-04-29T12:10:20,022 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250429121018-0003/0 finished with state KILLED exitStatus 143
2025-04-29T12:10:20,023 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-29T12:10:20,023 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-29T12:10:20,023 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250429121018-0003, execId=1)
2025-04-29T12:10:20,023 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250429121018-0003/2 finished with state KILLED exitStatus 143
2025-04-29T12:10:20,024 [dispatcher-event-loop-4] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250429121018-0003/0
2025-04-29T12:10:20,025 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-29T12:10:20,025 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250429121018-0003
2025-04-29T12:10:20,025 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250429121018-0003, execId=2)
2025-04-29T12:10:20,025 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250429121018-0003 removed, cleanupLocalDirs = true
2025-04-29T12:10:20,027 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250429121018-0003
2025-04-29T12:10:20,027 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250429121018-0003 removed, cleanupLocalDirs = true
2025-04-29T12:10:20,028 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250429121018-0003, execId=0)
2025-04-29T12:10:20,026 [dispatcher-event-loop-4] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250429121018-0003/2
2025-04-29T12:10:20,030 [dispatcher-event-loop-4] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250429121018-0003/1
2025-04-29T12:10:20,032 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250429121018-0003
2025-04-29T12:10:20,032 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250429121018-0003 removed, cleanupLocalDirs = true
2025-04-29T12:10:20,047 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-29T12:10:20,048 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-29T12:10:20,054 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-29T12:10:20,056 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-29T12:10:20,059 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:48970 got disassociated, removing it.
2025-04-29T12:10:20,060 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - bb9210fe839d:32837 got disassociated, removing it.
2025-04-29T12:10:20,065 [Thread-4] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-29T12:10:20,501 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-29T12:10:20,502 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-801b82df-f301-4604-b8ed-2bad40a1599f
2025-04-29T12:10:20,507 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-c6137125-a4e4-436c-9412-ee1803c6bab5
2025-04-29T12:10:20,511 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-801b82df-f301-4604-b8ed-2bad40a1599f/pyspark-6a9cafc5-639d-485e-a92f-1e1b2b60096c
2025-04-29T17:03:09,511 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-29T17:03:09,519 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-26f54f08-6750-48ea-879c-1d41e102893b
2025-04-29T17:04:27,841 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-29T17:04:27,846 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-815abeef-74df-42fc-902b-93078824b9c8
2025-04-29T17:05:39,833 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-29T17:05:39,841 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-29T17:05:39,842 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-29T17:05:39,881 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T17:05:39,882 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-29T17:05:39,884 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T17:05:39,886 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: Local Development
2025-04-29T17:05:39,916 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-29T17:05:39,931 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-29T17:05:39,932 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-29T17:05:40,000 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-29T17:05:40,002 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-29T17:05:40,004 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T17:05:40,005 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T17:05:40,007 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-29T17:05:40,076 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-29T17:05:40,538 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 44573.
2025-04-29T17:05:40,567 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-29T17:05:40,602 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-29T17:05:40,626 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-29T17:05:40,627 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-29T17:05:40,632 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-29T17:05:40,660 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-49689efb-7775-48e7-bb9b-cb39fa7ccb3e
2025-04-29T17:05:40,677 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-29T17:05:40,695 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-29T17:05:40,741 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3951ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-29T17:05:40,823 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-29T17:05:40,832 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-29T17:05:40,851 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @4062ms
2025-04-29T17:05:40,887 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@3dd648f9{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-29T17:05:40,888 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-29T17:05:40,917 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6c652bdc{/,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,003 [Thread-4] INFO  org.apache.spark.executor.Executor [] - Starting executor ID driver on host bb9210fe839d
2025-04-29T17:05:41,004 [Thread-4] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-29T17:05:41,005 [Thread-4] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-29T17:05:41,014 [Thread-4] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-29T17:05:41,015 [Thread-4] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@438d65a6 for default.
2025-04-29T17:05:41,039 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45169.
2025-04-29T17:05:41,040 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on bb9210fe839d:45169
2025-04-29T17:05:41,057 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-29T17:05:41,069 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, bb9210fe839d, 45169, None)
2025-04-29T17:05:41,073 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager bb9210fe839d:45169 with 434.4 MiB RAM, BlockManagerId(driver, bb9210fe839d, 45169, None)
2025-04-29T17:05:41,077 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, bb9210fe839d, 45169, None)
2025-04-29T17:05:41,078 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, bb9210fe839d, 45169, None)
2025-04-29T17:05:41,291 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/local-1745946340965.inprogress
2025-04-29T17:05:41,407 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@6c652bdc{/,null,STOPPED,@Spark}
2025-04-29T17:05:41,410 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@23897eae{/jobs,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,412 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@785c0d06{/jobs/json,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,414 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@73bfe39{/jobs/job,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,415 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4c9fa5d5{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,418 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@349d09cc{/stages,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,420 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@668e62bf{/stages/json,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,423 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3cd4c991{/stages/stage,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,426 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@59be2bfd{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,428 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@37de6416{/stages/pool,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,429 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@429277d7{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,431 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4e7f8b32{/storage,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,434 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2faca845{/storage/json,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,437 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3a5f3148{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,440 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@47a48e6{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,442 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@171c8b74{/environment,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,445 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7e69178e{/environment/json,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,447 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@87c1a7c{/executors,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,449 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@41fc282{/executors/json,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,451 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5462bbe0{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,454 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7f0fe3e1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,456 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2d0a74ce{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,458 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5940f59b{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,467 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@78436f07{/static,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,470 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1c680795{/,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,473 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@48a997af{/api,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,475 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@555be51d{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,478 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7fd899b7{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,482 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@15a8c7b8{/metrics/json,null,AVAILABLE,@Spark}
2025-04-29T17:05:41,634 [Thread-4] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-29T17:05:41,642 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@3dd648f9{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-29T17:05:41,646 [Thread-4] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://bb9210fe839d:4040
2025-04-29T17:05:41,680 [dispatcher-event-loop-1] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-29T17:05:41,691 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-29T17:05:41,692 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-29T17:05:41,698 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-29T17:05:41,701 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-29T17:05:41,710 [Thread-4] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-29T17:05:42,050 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-29T17:05:42,052 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-10db19cd-3785-46d9-9e42-f5c9d28cdc0c
2025-04-29T17:05:42,058 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-10db19cd-3785-46d9-9e42-f5c9d28cdc0c/pyspark-f266eeaa-8b6b-4c00-aa8d-4e35c27dcc28
2025-04-29T17:05:42,065 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-059e23ce-8a40-46f9-8096-38fe842515bd
2025-04-29T17:07:39,804 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-29T17:07:39,811 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-29T17:07:39,812 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-29T17:07:39,848 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T17:07:39,849 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-29T17:07:39,850 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T17:07:39,852 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: Local Development
2025-04-29T17:07:39,879 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-29T17:07:39,892 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-29T17:07:39,894 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-29T17:07:39,957 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-29T17:07:39,958 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-29T17:07:39,959 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T17:07:39,960 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T17:07:39,962 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-29T17:07:40,034 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-29T17:07:40,314 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 46465.
2025-04-29T17:07:40,341 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-29T17:07:40,375 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-29T17:07:40,392 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-29T17:07:40,393 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-29T17:07:40,399 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-29T17:07:40,421 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-7adcd9c0-d052-43a8-a218-ed0a120a7cdd
2025-04-29T17:07:40,440 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-29T17:07:40,459 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-29T17:07:40,502 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3433ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-29T17:07:40,591 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-29T17:07:40,601 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-29T17:07:40,620 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @3552ms
2025-04-29T17:07:40,652 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@3dd648f9{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-29T17:07:40,653 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-29T17:07:40,681 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6c652bdc{/,null,AVAILABLE,@Spark}
2025-04-29T17:07:40,766 [Thread-4] INFO  org.apache.spark.executor.Executor [] - Starting executor ID driver on host bb9210fe839d
2025-04-29T17:07:40,767 [Thread-4] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-29T17:07:40,768 [Thread-4] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-29T17:07:40,777 [Thread-4] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-29T17:07:40,778 [Thread-4] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@438d65a6 for default.
2025-04-29T17:07:40,800 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34765.
2025-04-29T17:07:40,801 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on bb9210fe839d:34765
2025-04-29T17:07:40,821 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-29T17:07:40,835 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, bb9210fe839d, 34765, None)
2025-04-29T17:07:40,840 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager bb9210fe839d:34765 with 434.4 MiB RAM, BlockManagerId(driver, bb9210fe839d, 34765, None)
2025-04-29T17:07:40,843 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, bb9210fe839d, 34765, None)
2025-04-29T17:07:40,845 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, bb9210fe839d, 34765, None)
2025-04-29T17:07:41,056 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/local-1745946460727.inprogress
2025-04-29T17:07:41,170 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@6c652bdc{/,null,STOPPED,@Spark}
2025-04-29T17:07:41,173 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@23897eae{/jobs,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,175 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@785c0d06{/jobs/json,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,177 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@73bfe39{/jobs/job,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,180 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4c9fa5d5{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,182 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@349d09cc{/stages,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,185 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@668e62bf{/stages/json,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,188 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3cd4c991{/stages/stage,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,190 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@59be2bfd{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,192 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@37de6416{/stages/pool,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,194 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@429277d7{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,196 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4e7f8b32{/storage,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,199 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2faca845{/storage/json,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,202 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3a5f3148{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,205 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@47a48e6{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,207 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@171c8b74{/environment,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,210 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7e69178e{/environment/json,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,212 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@87c1a7c{/executors,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,214 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@41fc282{/executors/json,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,216 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5462bbe0{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,218 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7f0fe3e1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,221 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2d0a74ce{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,223 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5940f59b{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,232 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@78436f07{/static,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,234 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1c680795{/,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,238 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@48a997af{/api,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,240 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@555be51d{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,243 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7fd899b7{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,249 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@15a8c7b8{/metrics/json,null,AVAILABLE,@Spark}
2025-04-29T17:07:41,400 [Thread-4] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-29T17:07:41,409 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@3dd648f9{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-29T17:07:41,414 [Thread-4] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://bb9210fe839d:4040
2025-04-29T17:07:41,448 [dispatcher-event-loop-1] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-29T17:07:41,461 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-29T17:07:41,462 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-29T17:07:41,468 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-29T17:07:41,472 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-29T17:07:41,481 [Thread-4] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-29T17:07:41,813 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-29T17:07:41,815 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-b8b09ccf-2a37-4cb4-8343-66f882e5edc0/pyspark-6b83df1d-abd0-4851-a5ce-4ff22b8a2601
2025-04-29T17:07:41,821 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-b8b09ccf-2a37-4cb4-8343-66f882e5edc0
2025-04-29T17:07:41,827 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-f954448c-7474-49cb-b733-f686fabf9f25
2025-04-29T17:10:25,037 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-29T17:10:25,041 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-29T17:10:25,042 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-29T17:10:25,067 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T17:10:25,067 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-29T17:10:25,069 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T17:10:25,070 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: Local Development
2025-04-29T17:10:25,089 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-29T17:10:25,100 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-29T17:10:25,101 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-29T17:10:25,151 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-29T17:10:25,152 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-29T17:10:25,153 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T17:10:25,155 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T17:10:25,156 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-29T17:10:25,216 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-29T17:10:25,474 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 36321.
2025-04-29T17:10:25,502 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-29T17:10:25,536 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-29T17:10:25,553 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-29T17:10:25,554 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-29T17:10:25,559 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-29T17:10:25,581 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-c4d08559-7ed6-4e12-a693-9d66284fcdeb
2025-04-29T17:10:25,600 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-29T17:10:25,617 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-29T17:10:25,665 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3106ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-29T17:10:25,749 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-29T17:10:25,761 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-29T17:10:25,781 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @3224ms
2025-04-29T17:10:25,819 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@3dd648f9{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-29T17:10:25,820 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-29T17:10:25,843 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6c652bdc{/,null,AVAILABLE,@Spark}
2025-04-29T17:10:25,925 [Thread-4] INFO  org.apache.spark.executor.Executor [] - Starting executor ID driver on host bb9210fe839d
2025-04-29T17:10:25,926 [Thread-4] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-29T17:10:25,926 [Thread-4] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-29T17:10:25,934 [Thread-4] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-29T17:10:25,935 [Thread-4] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@438d65a6 for default.
2025-04-29T17:10:25,955 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36919.
2025-04-29T17:10:25,956 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on bb9210fe839d:36919
2025-04-29T17:10:25,969 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-29T17:10:25,981 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, bb9210fe839d, 36919, None)
2025-04-29T17:10:25,986 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager bb9210fe839d:36919 with 434.4 MiB RAM, BlockManagerId(driver, bb9210fe839d, 36919, None)
2025-04-29T17:10:25,988 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, bb9210fe839d, 36919, None)
2025-04-29T17:10:25,990 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, bb9210fe839d, 36919, None)
2025-04-29T17:10:26,186 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/local-1745946625888.inprogress
2025-04-29T17:10:26,293 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@6c652bdc{/,null,STOPPED,@Spark}
2025-04-29T17:10:26,295 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@23897eae{/jobs,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,297 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@785c0d06{/jobs/json,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,300 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@73bfe39{/jobs/job,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,302 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4c9fa5d5{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,304 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@349d09cc{/stages,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,305 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@668e62bf{/stages/json,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,308 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3cd4c991{/stages/stage,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,310 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@59be2bfd{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,312 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@37de6416{/stages/pool,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,314 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@429277d7{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,316 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4e7f8b32{/storage,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,319 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2faca845{/storage/json,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,321 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3a5f3148{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,324 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@47a48e6{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,326 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@171c8b74{/environment,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,329 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7e69178e{/environment/json,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,331 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@87c1a7c{/executors,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,333 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@41fc282{/executors/json,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,335 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5462bbe0{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,337 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7f0fe3e1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,339 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2d0a74ce{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,342 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5940f59b{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,352 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@78436f07{/static,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,353 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1c680795{/,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,356 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@48a997af{/api,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,358 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@555be51d{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,360 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7fd899b7{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,366 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@15a8c7b8{/metrics/json,null,AVAILABLE,@Spark}
2025-04-29T17:10:26,510 [Thread-4] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-29T17:10:26,518 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@3dd648f9{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-29T17:10:26,523 [Thread-4] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://bb9210fe839d:4040
2025-04-29T17:10:26,561 [dispatcher-event-loop-1] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-29T17:10:26,571 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-29T17:10:26,572 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-29T17:10:26,579 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-29T17:10:26,583 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-29T17:10:26,590 [Thread-4] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-29T17:10:26,920 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-29T17:10:26,921 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-ff5b2890-6b05-49fb-baac-6e663348aa4e
2025-04-29T17:10:26,927 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-ff5b2890-6b05-49fb-baac-6e663348aa4e/pyspark-f582be4e-0cfc-499c-b6ae-21d4da7999ff
2025-04-29T17:10:26,932 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-54c3303d-fa90-4d26-987e-c3edb22baa88
2025-04-29T17:14:01,159 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-29T17:14:01,165 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-29T17:14:01,166 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-29T17:14:01,196 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T17:14:01,198 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-29T17:14:01,200 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T17:14:01,202 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: Local Development
2025-04-29T17:14:01,227 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-29T17:14:01,243 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-29T17:14:01,246 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-29T17:14:01,311 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-29T17:14:01,313 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-29T17:14:01,315 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T17:14:01,316 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T17:14:01,318 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-29T17:14:01,415 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-29T17:14:01,777 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 32795.
2025-04-29T17:14:01,821 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-29T17:14:01,859 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-29T17:14:01,887 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-29T17:14:01,889 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-29T17:14:01,896 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-29T17:14:01,932 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-f7cfc52e-739a-42d1-8b10-c4d6cdb1661e
2025-04-29T17:14:01,958 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-29T17:14:01,987 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-29T17:14:02,058 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @4370ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-29T17:14:02,164 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-29T17:14:02,177 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-29T17:14:02,204 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @4517ms
2025-04-29T17:14:02,243 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@3dd648f9{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-29T17:14:02,245 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-29T17:14:02,274 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6c652bdc{/,null,AVAILABLE,@Spark}
2025-04-29T17:14:02,370 [Thread-4] INFO  org.apache.spark.executor.Executor [] - Starting executor ID driver on host bb9210fe839d
2025-04-29T17:14:02,371 [Thread-4] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-29T17:14:02,371 [Thread-4] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-29T17:14:02,381 [Thread-4] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-29T17:14:02,384 [Thread-4] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@438d65a6 for default.
2025-04-29T17:14:02,434 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34125.
2025-04-29T17:14:02,435 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on bb9210fe839d:34125
2025-04-29T17:14:02,438 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-29T17:14:02,446 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, bb9210fe839d, 34125, None)
2025-04-29T17:14:02,452 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager bb9210fe839d:34125 with 434.4 MiB RAM, BlockManagerId(driver, bb9210fe839d, 34125, None)
2025-04-29T17:14:02,455 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, bb9210fe839d, 34125, None)
2025-04-29T17:14:02,458 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, bb9210fe839d, 34125, None)
2025-04-29T17:14:02,772 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/local-1745946842329.inprogress
2025-04-29T17:14:02,930 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@6c652bdc{/,null,STOPPED,@Spark}
2025-04-29T17:14:02,933 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@23897eae{/jobs,null,AVAILABLE,@Spark}
2025-04-29T17:14:02,936 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@785c0d06{/jobs/json,null,AVAILABLE,@Spark}
2025-04-29T17:14:02,939 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@73bfe39{/jobs/job,null,AVAILABLE,@Spark}
2025-04-29T17:14:02,941 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4c9fa5d5{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-29T17:14:02,943 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@349d09cc{/stages,null,AVAILABLE,@Spark}
2025-04-29T17:14:02,946 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@668e62bf{/stages/json,null,AVAILABLE,@Spark}
2025-04-29T17:14:02,951 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3cd4c991{/stages/stage,null,AVAILABLE,@Spark}
2025-04-29T17:14:02,954 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@59be2bfd{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-29T17:14:02,957 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@37de6416{/stages/pool,null,AVAILABLE,@Spark}
2025-04-29T17:14:02,959 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@429277d7{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-29T17:14:02,961 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4e7f8b32{/storage,null,AVAILABLE,@Spark}
2025-04-29T17:14:02,965 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2faca845{/storage/json,null,AVAILABLE,@Spark}
2025-04-29T17:14:02,968 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3a5f3148{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-29T17:14:02,971 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@47a48e6{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-29T17:14:02,974 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@171c8b74{/environment,null,AVAILABLE,@Spark}
2025-04-29T17:14:02,976 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7e69178e{/environment/json,null,AVAILABLE,@Spark}
2025-04-29T17:14:02,980 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@87c1a7c{/executors,null,AVAILABLE,@Spark}
2025-04-29T17:14:02,984 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@41fc282{/executors/json,null,AVAILABLE,@Spark}
2025-04-29T17:14:02,987 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5462bbe0{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-29T17:14:02,990 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7f0fe3e1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-29T17:14:02,992 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2d0a74ce{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-29T17:14:02,995 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5940f59b{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-29T17:14:03,007 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@78436f07{/static,null,AVAILABLE,@Spark}
2025-04-29T17:14:03,009 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1c680795{/,null,AVAILABLE,@Spark}
2025-04-29T17:14:03,016 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@48a997af{/api,null,AVAILABLE,@Spark}
2025-04-29T17:14:03,018 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@555be51d{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-29T17:14:03,020 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7fd899b7{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-29T17:14:03,026 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@15a8c7b8{/metrics/json,null,AVAILABLE,@Spark}
2025-04-29T17:14:03,220 [Thread-4] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-29T17:14:03,231 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@3dd648f9{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-29T17:14:03,239 [Thread-4] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://bb9210fe839d:4040
2025-04-29T17:14:03,286 [dispatcher-event-loop-1] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-29T17:14:03,303 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-29T17:14:03,305 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-29T17:14:03,312 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-29T17:14:03,318 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-29T17:14:03,331 [Thread-4] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-29T17:14:03,592 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-29T17:14:03,594 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-635e549c-4008-4380-b782-27b00b6c7d06/pyspark-6ad79c66-7fcc-42d4-926e-95cac694f7ad
2025-04-29T17:14:03,601 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-bbe183ea-fe34-4756-be35-0d6b4fa7f2ba
2025-04-29T17:14:03,608 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-635e549c-4008-4380-b782-27b00b6c7d06
2025-04-29T17:15:13,828 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-29T17:15:13,835 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-29T17:15:13,836 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-29T17:15:13,866 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T17:15:13,867 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-29T17:15:13,868 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T17:15:13,870 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: mod-2-pr-3.py
2025-04-29T17:15:13,892 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-29T17:15:13,906 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpus at 2 tasks per executor
2025-04-29T17:15:13,910 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-29T17:15:13,968 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-29T17:15:13,969 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-29T17:15:13,970 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T17:15:13,972 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T17:15:13,973 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-29T17:15:14,048 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-29T17:15:14,335 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 35435.
2025-04-29T17:15:14,365 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-29T17:15:14,405 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-29T17:15:14,425 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-29T17:15:14,426 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-29T17:15:14,432 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-29T17:15:14,457 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-dcc5c379-bd9e-4340-b1e4-0dcc70859621
2025-04-29T17:15:14,475 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-29T17:15:14,495 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-29T17:15:14,544 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3488ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-29T17:15:14,645 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-29T17:15:14,657 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-29T17:15:14,682 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @3626ms
2025-04-29T17:15:14,718 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@52b92de6{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-29T17:15:14,718 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-29T17:15:14,748 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7a9495c1{/,null,AVAILABLE,@Spark}
2025-04-29T17:15:14,883 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-29T17:15:14,945 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 34 ms (0 ms spent in bootstraps)
2025-04-29T17:15:15,103 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - Registering app mod-2-pr-3.py
2025-04-29T17:15:15,154 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - Registered app mod-2-pr-3.py with ID app-20250429171515-0004
2025-04-29T17:15:15,160 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250429171515-0004 with rpId: 0
2025-04-29T17:15:15,163 [dispatcher-event-loop-11] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250429171515-0004
2025-04-29T17:15:15,168 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250429171515-0004/0 on worker worker-20250429114342-172.18.0.5-38821
2025-04-29T17:15:15,172 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250429171515-0004/1 on worker worker-20250429114342-172.18.0.4-36391
2025-04-29T17:15:15,176 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36161.
2025-04-29T17:15:15,177 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on bb9210fe839d:36161
2025-04-29T17:15:15,181 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-29T17:15:15,186 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250429171515-0004/0 on worker-20250429114342-172.18.0.5-38821 (172.18.0.5:38821) with 2 core(s)
2025-04-29T17:15:15,189 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250429171515-0004/0 on hostPort 172.18.0.5:38821 with 2 core(s), 2.0 GiB RAM
2025-04-29T17:15:15,189 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250429171515-0004/1 on worker-20250429114342-172.18.0.4-36391 (172.18.0.4:36391) with 2 core(s)
2025-04-29T17:15:15,190 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250429171515-0004/1 on hostPort 172.18.0.4:36391 with 2 core(s), 2.0 GiB RAM
2025-04-29T17:15:15,193 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, bb9210fe839d, 36161, None)
2025-04-29T17:15:15,199 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager bb9210fe839d:36161 with 434.4 MiB RAM, BlockManagerId(driver, bb9210fe839d, 36161, None)
2025-04-29T17:15:15,204 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, bb9210fe839d, 36161, None)
2025-04-29T17:15:15,207 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, bb9210fe839d, 36161, None)
2025-04-29T17:15:15,311 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250429171515-0004/0 for mod-2-pr-3.py
2025-04-29T17:15:15,324 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250429171515-0004/1 for mod-2-pr-3.py
2025-04-29T17:15:15,361 [ExecutorRunner for app-20250429171515-0004/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-29T17:15:15,362 [ExecutorRunner for app-20250429171515-0004/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-29T17:15:15,364 [ExecutorRunner for app-20250429171515-0004/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T17:15:15,364 [ExecutorRunner for app-20250429171515-0004/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T17:15:15,365 [ExecutorRunner for app-20250429171515-0004/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-29T17:15:15,375 [ExecutorRunner for app-20250429171515-0004/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-29T17:15:15,376 [ExecutorRunner for app-20250429171515-0004/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-29T17:15:15,376 [ExecutorRunner for app-20250429171515-0004/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T17:15:15,377 [ExecutorRunner for app-20250429171515-0004/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T17:15:15,377 [ExecutorRunner for app-20250429171515-0004/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-29T17:15:15,467 [ExecutorRunner for app-20250429171515-0004/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx2048M" "-Dspark.driver.port=35435" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:35435" "--executor-id" "0" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250429171515-0004" "--worker-url" "spark://Worker@172.18.0.5:38821" "--resourceProfileId" "0"
2025-04-29T17:15:15,471 [ExecutorRunner for app-20250429171515-0004/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx2048M" "-Dspark.driver.port=35435" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:35435" "--executor-id" "1" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250429171515-0004" "--worker-url" "spark://Worker@172.18.0.4:36391" "--resourceProfileId" "0"
2025-04-29T17:15:15,503 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250429171515-0004 with rpId: 0
2025-04-29T17:15:15,509 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250429171515-0004 with rpId: 0
2025-04-29T17:15:15,522 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250429171515-0004/0 is now RUNNING
2025-04-29T17:15:15,529 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250429171515-0004/1 is now RUNNING
2025-04-29T17:15:15,557 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250429171515-0004.inprogress
2025-04-29T17:15:15,748 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@7a9495c1{/,null,STOPPED,@Spark}
2025-04-29T17:15:15,752 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5e6c5b58{/jobs,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,755 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@30d68d30{/jobs/json,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,758 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@35491a24{/jobs/job,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,761 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@58f07823{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,764 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@132db08{/stages,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,768 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@75a719ac{/stages/json,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,778 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5086eec8{/stages/stage,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,785 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@252b8bf8{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,788 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7e3a7fbd{/stages/pool,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,793 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3294e384{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,799 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6fb4d59b{/storage,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,805 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7f1f48db{/storage/json,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,808 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@c9a7aa{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,814 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7f1b7b82{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,822 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3e6eea0a{/environment,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,825 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5de150aa{/environment/json,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,828 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@58b4acbc{/executors,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,840 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@61f03750{/executors/json,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,844 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1446b38d{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,852 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@58aa6405{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,855 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@41193139{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,859 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@76530915{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,878 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@f3bcbac{/static,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,884 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@666f2835{/,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,890 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@42ef928{/api,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,894 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@17a5b9e6{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,897 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@271ff960{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,908 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3ccb35c8{/metrics/json,null,AVAILABLE,@Spark}
2025-04-29T17:15:15,915 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-29T17:15:16,237 [Thread-4] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-29T17:15:16,256 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@52b92de6{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-29T17:15:16,268 [Thread-4] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://bb9210fe839d:4040
2025-04-29T17:15:16,279 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-29T17:15:16,288 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-29T17:15:16,309 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250429171515-0004
2025-04-29T17:15:16,310 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250429171515-0004
2025-04-29T17:15:16,327 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250429171515-0004/1
2025-04-29T17:15:16,329 [ExecutorRunner for app-20250429171515-0004/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250429171515-0004/1 interrupted
2025-04-29T17:15:16,334 [ExecutorRunner for app-20250429171515-0004/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-29T17:15:16,340 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250429171515-0004/0
2025-04-29T17:15:16,342 [ExecutorRunner for app-20250429171515-0004/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250429171515-0004/0 interrupted
2025-04-29T17:15:16,343 [ExecutorRunner for app-20250429171515-0004/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-29T17:15:16,375 [dispatcher-event-loop-0] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-29T17:15:16,421 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-29T17:15:16,423 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-29T17:15:16,440 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-29T17:15:16,443 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-29T17:15:16,455 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250429171515-0004/0 finished with state KILLED exitStatus 143
2025-04-29T17:15:16,457 [dispatcher-event-loop-11] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250429171515-0004/0
2025-04-29T17:15:16,459 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-29T17:15:16,466 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250429171515-0004, execId=0)
2025-04-29T17:15:16,445 [dispatcher-event-loop-11] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250429171515-0004 removed, cleanupLocalDirs = true
2025-04-29T17:15:16,469 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250429171515-0004 removed, cleanupLocalDirs = true
2025-04-29T17:15:16,470 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250429171515-0004
2025-04-29T17:15:16,482 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:38228 got disassociated, removing it.
2025-04-29T17:15:16,483 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - bb9210fe839d:35435 got disassociated, removing it.
2025-04-29T17:15:16,498 [Thread-4] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-29T17:15:16,514 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250429171515-0004/1 finished with state KILLED exitStatus 143
2025-04-29T17:15:16,515 [dispatcher-event-loop-4] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250429171515-0004/1
2025-04-29T17:15:16,518 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-29T17:15:16,519 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250429171515-0004, execId=1)
2025-04-29T17:15:16,520 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250429171515-0004 removed, cleanupLocalDirs = true
2025-04-29T17:15:16,521 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250429171515-0004
2025-04-29T17:15:17,018 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-29T17:15:17,020 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-1d2f07ff-26f9-4fd1-b4ca-24c12ed45ebf/pyspark-d9cd10a0-10bb-412c-8bcf-df457fac87d9
2025-04-29T17:15:17,025 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-c97065c3-bad2-4bc6-b3c6-081c212299b6
2025-04-29T17:15:17,033 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-1d2f07ff-26f9-4fd1-b4ca-24c12ed45ebf
2025-04-29T17:16:15,237 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-29T17:16:15,243 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-29T17:16:15,244 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-29T17:16:15,286 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T17:16:15,288 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-29T17:16:15,289 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T17:16:15,290 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: mod-2-pr-3.py
2025-04-29T17:16:15,321 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-29T17:16:15,339 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpus at 2 tasks per executor
2025-04-29T17:16:15,343 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-29T17:16:15,408 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-29T17:16:15,410 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-29T17:16:15,411 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T17:16:15,412 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T17:16:15,413 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-29T17:16:15,489 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-29T17:16:15,796 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 45517.
2025-04-29T17:16:15,837 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-29T17:16:15,883 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-29T17:16:15,910 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-29T17:16:15,911 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-29T17:16:15,921 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-29T17:16:15,952 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-4ae0a2f8-86f5-4ec4-af49-f8a784ecd3d5
2025-04-29T17:16:15,972 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-29T17:16:15,999 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-29T17:16:16,068 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3831ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-29T17:16:16,194 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-29T17:16:16,210 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-29T17:16:16,235 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @3999ms
2025-04-29T17:16:16,273 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@770d3fdd{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-29T17:16:16,274 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-29T17:16:16,301 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@78aff462{/,null,AVAILABLE,@Spark}
2025-04-29T17:16:16,464 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-29T17:16:16,535 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 37 ms (0 ms spent in bootstraps)
2025-04-29T17:16:16,627 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - Registering app mod-2-pr-3.py
2025-04-29T17:16:16,630 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - Registered app mod-2-pr-3.py with ID app-20250429171616-0005
2025-04-29T17:16:16,631 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250429171616-0005 with rpId: 0
2025-04-29T17:16:16,634 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250429171616-0005/0 on worker worker-20250429114342-172.18.0.5-38821
2025-04-29T17:16:16,636 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250429171616-0005/1 on worker worker-20250429114342-172.18.0.4-36391
2025-04-29T17:16:16,639 [dispatcher-event-loop-10] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250429171616-0005
2025-04-29T17:16:16,642 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250429171616-0005/0 on worker-20250429114342-172.18.0.5-38821 (172.18.0.5:38821) with 2 core(s)
2025-04-29T17:16:16,643 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250429171616-0005/0 for mod-2-pr-3.py
2025-04-29T17:16:16,645 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250429171616-0005/1 for mod-2-pr-3.py
2025-04-29T17:16:16,649 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250429171616-0005/0 on hostPort 172.18.0.5:38821 with 2 core(s), 2.0 GiB RAM
2025-04-29T17:16:16,650 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250429171616-0005/1 on worker-20250429114342-172.18.0.4-36391 (172.18.0.4:36391) with 2 core(s)
2025-04-29T17:16:16,651 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250429171616-0005/1 on hostPort 172.18.0.4:36391 with 2 core(s), 2.0 GiB RAM
2025-04-29T17:16:16,654 [ExecutorRunner for app-20250429171616-0005/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-29T17:16:16,655 [ExecutorRunner for app-20250429171616-0005/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-29T17:16:16,655 [ExecutorRunner for app-20250429171616-0005/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-29T17:16:16,656 [ExecutorRunner for app-20250429171616-0005/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T17:16:16,656 [ExecutorRunner for app-20250429171616-0005/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-29T17:16:16,656 [ExecutorRunner for app-20250429171616-0005/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T17:16:16,657 [ExecutorRunner for app-20250429171616-0005/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T17:16:16,657 [ExecutorRunner for app-20250429171616-0005/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-29T17:16:16,658 [ExecutorRunner for app-20250429171616-0005/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T17:16:16,658 [ExecutorRunner for app-20250429171616-0005/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-29T17:16:16,661 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44129.
2025-04-29T17:16:16,661 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on bb9210fe839d:44129
2025-04-29T17:16:16,665 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-29T17:16:16,676 [ExecutorRunner for app-20250429171616-0005/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx2048M" "-Dspark.driver.port=45517" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:45517" "--executor-id" "1" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250429171616-0005" "--worker-url" "spark://Worker@172.18.0.4:36391" "--resourceProfileId" "0"
2025-04-29T17:16:16,676 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, bb9210fe839d, 44129, None)
2025-04-29T17:16:16,679 [ExecutorRunner for app-20250429171616-0005/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx2048M" "-Dspark.driver.port=45517" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:45517" "--executor-id" "0" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250429171616-0005" "--worker-url" "spark://Worker@172.18.0.5:38821" "--resourceProfileId" "0"
2025-04-29T17:16:16,682 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager bb9210fe839d:44129 with 434.4 MiB RAM, BlockManagerId(driver, bb9210fe839d, 44129, None)
2025-04-29T17:16:16,688 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, bb9210fe839d, 44129, None)
2025-04-29T17:16:16,690 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250429171616-0005 with rpId: 0
2025-04-29T17:16:16,691 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, bb9210fe839d, 44129, None)
2025-04-29T17:16:16,696 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250429171616-0005 with rpId: 0
2025-04-29T17:16:16,722 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250429171616-0005/1 is now RUNNING
2025-04-29T17:16:16,724 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250429171616-0005/0 is now RUNNING
2025-04-29T17:16:17,158 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250429171616-0005.inprogress
2025-04-29T17:16:17,429 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@78aff462{/,null,STOPPED,@Spark}
2025-04-29T17:16:17,436 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1fcf6fc7{/jobs,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,439 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@713dccbe{/jobs/json,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,443 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@46b7202{/jobs/job,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,446 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4ffbaab3{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,452 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6b2702e4{/stages,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,459 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2121ea13{/stages/json,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,468 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@672b6d2a{/stages/stage,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,472 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@313be3c1{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,480 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2c307a1f{/stages/pool,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,489 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@35a7d85f{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,493 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7a7e4fde{/storage,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,499 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4a2f5413{/storage/json,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,503 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1ef5f182{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,514 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@67880998{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,521 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2a20be67{/environment,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,532 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@609c54f4{/environment/json,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,538 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@15f9a442{/executors,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,544 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2a8808b0{/executors/json,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,555 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@672c397f{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,563 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2b09bec3{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,573 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6809850c{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,577 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3816e974{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,605 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3723e5c2{/static,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,611 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@67b6b8a3{/,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,617 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1f118e76{/api,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,622 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@cf173f9{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,626 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1d2d9d71{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,646 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5ae60dc6{/metrics/json,null,AVAILABLE,@Spark}
2025-04-29T17:16:17,649 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-29T17:16:18,203 [Thread-4] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-29T17:16:18,217 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@770d3fdd{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-29T17:16:18,224 [Thread-4] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://bb9210fe839d:4040
2025-04-29T17:16:18,237 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-29T17:16:18,242 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-29T17:16:18,253 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250429171616-0005
2025-04-29T17:16:18,258 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250429171616-0005
2025-04-29T17:16:18,269 [dispatcher-event-loop-3] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250429171616-0005 removed, cleanupLocalDirs = true
2025-04-29T17:16:18,280 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250429171616-0005/1
2025-04-29T17:16:18,282 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250429171616-0005/0
2025-04-29T17:16:18,284 [ExecutorRunner for app-20250429171616-0005/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250429171616-0005/1 interrupted
2025-04-29T17:16:18,285 [ExecutorRunner for app-20250429171616-0005/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-29T17:16:18,286 [ExecutorRunner for app-20250429171616-0005/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250429171616-0005/0 interrupted
2025-04-29T17:16:18,287 [ExecutorRunner for app-20250429171616-0005/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-29T17:16:18,306 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250429171616-0005/0 finished with state KILLED exitStatus 143
2025-04-29T17:16:18,311 [dispatcher-event-loop-0] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250429171616-0005/0
2025-04-29T17:16:18,312 [dispatcher-event-loop-0] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-29T17:16:18,315 [dispatcher-event-loop-0] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250429171616-0005, execId=0)
2025-04-29T17:16:18,316 [dispatcher-event-loop-0] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250429171616-0005 removed, cleanupLocalDirs = true
2025-04-29T17:16:18,316 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250429171616-0005
2025-04-29T17:16:18,319 [dispatcher-event-loop-11] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-29T17:16:18,319 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250429171616-0005/1 finished with state KILLED exitStatus 143
2025-04-29T17:16:18,320 [dispatcher-event-loop-4] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250429171616-0005/1
2025-04-29T17:16:18,321 [dispatcher-event-loop-8] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-29T17:16:18,322 [dispatcher-event-loop-8] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250429171616-0005, execId=1)
2025-04-29T17:16:18,323 [dispatcher-event-loop-8] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250429171616-0005 removed, cleanupLocalDirs = true
2025-04-29T17:16:18,323 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250429171616-0005
2025-04-29T17:16:18,339 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-29T17:16:18,340 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-29T17:16:18,347 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-29T17:16:18,351 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-29T17:16:18,360 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:54190 got disassociated, removing it.
2025-04-29T17:16:18,362 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - bb9210fe839d:45517 got disassociated, removing it.
2025-04-29T17:16:18,375 [Thread-4] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-29T17:16:18,780 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-29T17:16:18,782 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-2a42de75-ae1b-4c5b-b162-96709a9d3ec4
2025-04-29T17:16:18,789 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-4c8d4d9c-470a-463c-8cf5-ee5b81d83afb
2025-04-29T17:16:18,795 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-4c8d4d9c-470a-463c-8cf5-ee5b81d83afb/pyspark-5aced8bd-ce04-4151-b389-3bf6411ea51a
2025-04-29T17:17:09,080 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-29T17:17:09,090 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-29T17:17:09,091 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-29T17:17:09,135 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T17:17:09,140 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-29T17:17:09,141 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-29T17:17:09,142 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: mod-2-pr-3.py
2025-04-29T17:17:09,176 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-29T17:17:09,193 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-29T17:17:09,195 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-29T17:17:09,259 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-29T17:17:09,260 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-29T17:17:09,260 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T17:17:09,262 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T17:17:09,264 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-29T17:17:09,346 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-29T17:17:09,667 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 45913.
2025-04-29T17:17:09,702 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-29T17:17:09,747 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-29T17:17:09,772 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-29T17:17:09,774 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-29T17:17:09,782 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-29T17:17:09,805 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-18584186-7fdf-4307-8257-3bdb2aeea8a9
2025-04-29T17:17:09,824 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-29T17:17:09,846 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-29T17:17:09,904 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @4113ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-29T17:17:10,019 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-29T17:17:10,034 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-29T17:17:10,059 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @4269ms
2025-04-29T17:17:10,099 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@2a2301eb{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-29T17:17:10,100 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-29T17:17:10,129 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2f72c3c0{/,null,AVAILABLE,@Spark}
2025-04-29T17:17:10,274 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-29T17:17:10,337 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 37 ms (0 ms spent in bootstraps)
2025-04-29T17:17:10,432 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Registering app mod-2-pr-3.py
2025-04-29T17:17:10,441 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Registered app mod-2-pr-3.py with ID app-20250429171710-0006
2025-04-29T17:17:10,443 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250429171710-0006 with rpId: 0
2025-04-29T17:17:10,447 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250429171710-0006/0 on worker worker-20250429114342-172.18.0.5-38821
2025-04-29T17:17:10,449 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250429171710-0006/1 on worker worker-20250429114342-172.18.0.4-36391
2025-04-29T17:17:10,450 [dispatcher-event-loop-11] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250429171710-0006
2025-04-29T17:17:10,450 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250429171710-0006/2 on worker worker-20250429114342-172.18.0.3-34443
2025-04-29T17:17:10,456 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250429171710-0006/0 on worker-20250429114342-172.18.0.5-38821 (172.18.0.5:38821) with 2 core(s)
2025-04-29T17:17:10,459 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250429171710-0006/0 on hostPort 172.18.0.5:38821 with 2 core(s), 1024.0 MiB RAM
2025-04-29T17:17:10,458 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250429171710-0006/0 for mod-2-pr-3.py
2025-04-29T17:17:10,461 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250429171710-0006/1 on worker-20250429114342-172.18.0.4-36391 (172.18.0.4:36391) with 2 core(s)
2025-04-29T17:17:10,463 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250429171710-0006/1 on hostPort 172.18.0.4:36391 with 2 core(s), 1024.0 MiB RAM
2025-04-29T17:17:10,466 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250429171710-0006/2 on worker-20250429114342-172.18.0.3-34443 (172.18.0.3:34443) with 2 core(s)
2025-04-29T17:17:10,466 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250429171710-0006/1 for mod-2-pr-3.py
2025-04-29T17:17:10,468 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250429171710-0006/2 on hostPort 172.18.0.3:34443 with 2 core(s), 1024.0 MiB RAM
2025-04-29T17:17:10,468 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34941.
2025-04-29T17:17:10,470 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on bb9210fe839d:34941
2025-04-29T17:17:10,476 [ExecutorRunner for app-20250429171710-0006/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-29T17:17:10,478 [ExecutorRunner for app-20250429171710-0006/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-29T17:17:10,478 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-29T17:17:10,479 [ExecutorRunner for app-20250429171710-0006/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T17:17:10,480 [ExecutorRunner for app-20250429171710-0006/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T17:17:10,480 [ExecutorRunner for app-20250429171710-0006/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-29T17:17:10,484 [ExecutorRunner for app-20250429171710-0006/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-29T17:17:10,485 [ExecutorRunner for app-20250429171710-0006/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-29T17:17:10,487 [ExecutorRunner for app-20250429171710-0006/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T17:17:10,487 [ExecutorRunner for app-20250429171710-0006/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T17:17:10,488 [ExecutorRunner for app-20250429171710-0006/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-29T17:17:10,496 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, bb9210fe839d, 34941, None)
2025-04-29T17:17:10,508 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager bb9210fe839d:34941 with 434.4 MiB RAM, BlockManagerId(driver, bb9210fe839d, 34941, None)
2025-04-29T17:17:10,510 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250429171710-0006/2 for mod-2-pr-3.py
2025-04-29T17:17:10,513 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, bb9210fe839d, 34941, None)
2025-04-29T17:17:10,517 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, bb9210fe839d, 34941, None)
2025-04-29T17:17:10,523 [ExecutorRunner for app-20250429171710-0006/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=45913" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:45913" "--executor-id" "0" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250429171710-0006" "--worker-url" "spark://Worker@172.18.0.5:38821" "--resourceProfileId" "0"
2025-04-29T17:17:10,528 [ExecutorRunner for app-20250429171710-0006/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=45913" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:45913" "--executor-id" "1" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250429171710-0006" "--worker-url" "spark://Worker@172.18.0.4:36391" "--resourceProfileId" "0"
2025-04-29T17:17:10,547 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250429171710-0006 with rpId: 0
2025-04-29T17:17:10,551 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250429171710-0006 with rpId: 0
2025-04-29T17:17:10,584 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250429171710-0006/0 is now RUNNING
2025-04-29T17:17:10,585 [ExecutorRunner for app-20250429171710-0006/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-29T17:17:10,586 [ExecutorRunner for app-20250429171710-0006/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-29T17:17:10,588 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250429171710-0006/1 is now RUNNING
2025-04-29T17:17:10,588 [ExecutorRunner for app-20250429171710-0006/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-29T17:17:10,589 [ExecutorRunner for app-20250429171710-0006/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-29T17:17:10,589 [ExecutorRunner for app-20250429171710-0006/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-29T17:17:10,713 [ExecutorRunner for app-20250429171710-0006/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=45913" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:45913" "--executor-id" "2" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250429171710-0006" "--worker-url" "spark://Worker@172.18.0.3:34443" "--resourceProfileId" "0"
2025-04-29T17:17:10,744 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250429171710-0006 with rpId: 0
2025-04-29T17:17:10,750 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250429171710-0006/2 is now RUNNING
2025-04-29T17:17:11,016 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250429171710-0006.inprogress
2025-04-29T17:17:11,313 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@2f72c3c0{/,null,STOPPED,@Spark}
2025-04-29T17:17:11,321 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7287b184{/jobs,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,324 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@efb1f4{/jobs/json,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,329 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@43ebfa73{/jobs/job,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,335 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@28e2aeeb{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,341 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3ed8bd3d{/stages,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,351 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3915b297{/stages/json,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,356 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@518a5e73{/stages/stage,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,360 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@69163bf1{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,371 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@315c40fd{/stages/pool,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,374 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7f1ed11d{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,377 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@29fdd570{/storage,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,379 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7a4a5cf0{/storage/json,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,382 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6eb60a32{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,388 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1ad7d5a1{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,392 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@55b9f894{/environment,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,395 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@56041559{/environment/json,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,400 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@43ce77c8{/executors,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,406 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1288e51d{/executors/json,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,410 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@313b917c{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,414 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@42e22e1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,421 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@36e20084{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,430 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4ed622ff{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,468 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4ddb2236{/static,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,474 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3244d86f{/,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,479 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5cbcb4b8{/api,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,489 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3092c8e3{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,492 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6dd65270{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,508 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1db22e23{/metrics/json,null,AVAILABLE,@Spark}
2025-04-29T17:17:11,520 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-29T17:17:12,018 [Thread-4] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-29T17:17:12,054 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@2a2301eb{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-29T17:17:12,062 [Thread-4] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://bb9210fe839d:4040
2025-04-29T17:17:12,074 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-29T17:17:12,080 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-29T17:17:12,090 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250429171710-0006
2025-04-29T17:17:12,091 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250429171710-0006
2025-04-29T17:17:12,094 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250429171710-0006/0
2025-04-29T17:17:12,099 [ExecutorRunner for app-20250429171710-0006/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250429171710-0006/0 interrupted
2025-04-29T17:17:12,099 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250429171710-0006/2
2025-04-29T17:17:12,102 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250429171710-0006/1
2025-04-29T17:17:12,103 [ExecutorRunner for app-20250429171710-0006/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-29T17:17:12,104 [ExecutorRunner for app-20250429171710-0006/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250429171710-0006/2 interrupted
2025-04-29T17:17:12,105 [ExecutorRunner for app-20250429171710-0006/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-29T17:17:12,105 [ExecutorRunner for app-20250429171710-0006/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250429171710-0006/1 interrupted
2025-04-29T17:17:12,106 [ExecutorRunner for app-20250429171710-0006/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-29T17:17:12,125 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250429171710-0006/1 finished with state KILLED exitStatus 143
2025-04-29T17:17:12,126 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-29T17:17:12,126 [dispatcher-event-loop-10] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250429171710-0006/1
2025-04-29T17:17:12,127 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250429171710-0006, execId=1)
2025-04-29T17:17:12,128 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250429171710-0006 removed, cleanupLocalDirs = true
2025-04-29T17:17:12,128 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250429171710-0006
2025-04-29T17:17:12,129 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250429171710-0006/0 finished with state KILLED exitStatus 143
2025-04-29T17:17:12,129 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250429171710-0006/2 finished with state KILLED exitStatus 143
2025-04-29T17:17:12,130 [dispatcher-event-loop-5] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250429171710-0006/2
2025-04-29T17:17:12,130 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-29T17:17:12,131 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250429171710-0006, execId=0)
2025-04-29T17:17:12,131 [dispatcher-event-loop-5] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250429171710-0006/0
2025-04-29T17:17:12,131 [dispatcher-event-loop-0] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-29T17:17:12,132 [dispatcher-event-loop-0] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250429171710-0006, execId=2)
2025-04-29T17:17:12,132 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250429171710-0006
2025-04-29T17:17:12,132 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250429171710-0006 removed, cleanupLocalDirs = true
2025-04-29T17:17:12,135 [dispatcher-event-loop-0] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250429171710-0006 removed, cleanupLocalDirs = true
2025-04-29T17:17:12,136 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250429171710-0006
2025-04-29T17:17:12,143 [dispatcher-event-loop-1] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-29T17:17:12,159 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-29T17:17:12,159 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-29T17:17:12,165 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-29T17:17:12,168 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-29T17:17:12,173 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:38006 got disassociated, removing it.
2025-04-29T17:17:12,174 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - bb9210fe839d:45913 got disassociated, removing it.
2025-04-29T17:17:12,182 [Thread-4] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-29T17:17:12,625 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-29T17:17:12,628 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-004ed303-13dd-40a6-8626-647751c66fa6/pyspark-16941783-1e9b-44b8-ade4-67506273c5bf
2025-04-29T17:17:12,636 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-004ed303-13dd-40a6-8626-647751c66fa6
2025-04-29T17:17:12,643 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-b4684d9a-5f0a-4c10-9ae5-c148bda83e03
2025-04-30T11:37:24,640 [dispatcher-event-loop-9] WARN  org.apache.spark.deploy.master.Master [] - Removing worker-20250429114342-172.18.0.3-34443 because we got no heartbeat in 60 seconds
2025-04-30T11:37:24,688 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Removing worker worker-20250429114342-172.18.0.3-34443 on 172.18.0.3:34443
2025-04-30T11:37:25,184 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Telling app of lost worker: worker-20250429114342-172.18.0.3-34443
2025-04-30T11:37:25,308 [dispatcher-event-loop-9] WARN  org.apache.spark.deploy.master.Master [] - Removing worker-20250429114342-172.18.0.4-36391 because we got no heartbeat in 60 seconds
2025-04-30T11:37:25,408 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Removing worker worker-20250429114342-172.18.0.4-36391 on 172.18.0.4:36391
2025-04-30T11:37:25,492 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Telling app of lost worker: worker-20250429114342-172.18.0.4-36391
2025-04-30T11:37:25,556 [dispatcher-event-loop-9] WARN  org.apache.spark.deploy.master.Master [] - Removing worker-20250429114342-172.18.0.5-38821 because we got no heartbeat in 60 seconds
2025-04-30T11:37:25,603 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Removing worker worker-20250429114342-172.18.0.5-38821 on 172.18.0.5:38821
2025-04-30T11:37:25,637 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Telling app of lost worker: worker-20250429114342-172.18.0.5-38821
2025-04-30T11:37:25,693 [dispatcher-event-loop-9] WARN  org.apache.spark.deploy.master.Master [] - Got heartbeat from unregistered worker worker-20250429114342-172.18.0.4-36391. Asking it to re-register.
2025-04-30T11:37:26,002 [dispatcher-event-loop-9] WARN  org.apache.spark.deploy.master.Master [] - Got heartbeat from unregistered worker worker-20250429114342-172.18.0.5-38821. Asking it to re-register.
2025-04-30T11:37:26,019 [dispatcher-event-loop-9] WARN  org.apache.spark.deploy.master.Master [] - Got heartbeat from unregistered worker worker-20250429114342-172.18.0.3-34443. Asking it to re-register.
2025-04-30T11:37:26,553 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.worker.Worker [] - Master with url spark://bb9210fe839d:7077 requested this worker to reconnect.
2025-04-30T11:37:26,587 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.worker.Worker [] - Master with url spark://bb9210fe839d:7077 requested this worker to reconnect.
2025-04-30T11:37:26,644 [worker-register-master-threadpool-1] INFO  org.apache.spark.deploy.worker.Worker [] - Connecting to master spark-master:7077...
2025-04-30T11:37:26,670 [worker-register-master-threadpool-1] INFO  org.apache.spark.deploy.worker.Worker [] - Connecting to master spark-master:7077...
2025-04-30T11:37:26,716 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - Registering worker 172.18.0.5:38821 with 2 cores, 3.0 GiB RAM
2025-04-30T11:37:26,770 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - Registering worker 172.18.0.3:34443 with 2 cores, 3.0 GiB RAM
2025-04-30T11:37:26,789 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Successfully registered with master spark://bb9210fe839d:7077
2025-04-30T11:37:26,805 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Successfully registered with master spark://bb9210fe839d:7077
2025-04-30T11:37:26,989 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.worker.Worker [] - Master with url spark://bb9210fe839d:7077 requested this worker to reconnect.
2025-04-30T11:37:27,073 [worker-register-master-threadpool-1] INFO  org.apache.spark.deploy.worker.Worker [] - Connecting to master spark-master:7077...
2025-04-30T11:37:27,118 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Registering worker 172.18.0.4:36391 with 2 cores, 3.0 GiB RAM
2025-04-30T11:37:27,168 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Successfully registered with master spark://bb9210fe839d:7077
2025-04-30T12:00:14,774 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-30T12:00:14,779 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:00:14,780 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-30T12:00:14,826 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:00:14,828 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-30T12:00:14,829 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:00:14,831 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: mod-2-pr-4-data-ingestion.py
2025-04-30T12:00:14,863 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-30T12:00:14,876 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-30T12:00:14,879 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-30T12:00:14,959 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-30T12:00:14,968 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-30T12:00:14,970 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:00:14,972 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:00:14,973 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-30T12:00:15,047 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:00:15,463 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 38121.
2025-04-30T12:00:15,517 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-30T12:00:15,572 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-30T12:00:15,605 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-30T12:00:15,607 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-30T12:00:15,614 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-30T12:00:15,645 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-b2bd6a53-da15-4b9b-b384-b96a35f03173
2025-04-30T12:00:15,663 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:00:15,687 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-30T12:00:15,747 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @4440ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-30T12:00:15,855 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-30T12:00:15,870 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-30T12:00:15,896 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @4590ms
2025-04-30T12:00:15,943 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@2a2301eb{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T12:00:15,944 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-30T12:00:15,981 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2f72c3c0{/,null,AVAILABLE,@Spark}
2025-04-30T12:00:16,129 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-30T12:00:16,193 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 36 ms (0 ms spent in bootstraps)
2025-04-30T12:00:16,330 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Registering app mod-2-pr-4-data-ingestion.py
2025-04-30T12:00:16,346 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Registered app mod-2-pr-4-data-ingestion.py with ID app-20250430120016-0007
2025-04-30T12:00:16,351 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430120016-0007 with rpId: 0
2025-04-30T12:00:16,357 [dispatcher-event-loop-11] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250430120016-0007
2025-04-30T12:00:16,361 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430120016-0007/0 on worker worker-20250429114342-172.18.0.5-38821
2025-04-30T12:00:16,364 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430120016-0007/1 on worker worker-20250429114342-172.18.0.3-34443
2025-04-30T12:00:16,366 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430120016-0007/2 on worker worker-20250429114342-172.18.0.4-36391
2025-04-30T12:00:16,371 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430120016-0007/0 on worker-20250429114342-172.18.0.5-38821 (172.18.0.5:38821) with 2 core(s)
2025-04-30T12:00:16,377 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430120016-0007/0 on hostPort 172.18.0.5:38821 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:00:16,381 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430120016-0007/1 on worker-20250429114342-172.18.0.3-34443 (172.18.0.3:34443) with 2 core(s)
2025-04-30T12:00:16,384 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33875.
2025-04-30T12:00:16,384 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430120016-0007/1 on hostPort 172.18.0.3:34443 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:00:16,388 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on bb9210fe839d:33875
2025-04-30T12:00:16,395 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:00:16,394 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430120016-0007/2 on worker-20250429114342-172.18.0.4-36391 (172.18.0.4:36391) with 2 core(s)
2025-04-30T12:00:16,399 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430120016-0007/2 on hostPort 172.18.0.4:36391 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:00:16,414 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430120016-0007/1 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:00:16,417 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, bb9210fe839d, 33875, None)
2025-04-30T12:00:16,415 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430120016-0007/0 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:00:16,430 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager bb9210fe839d:33875 with 434.4 MiB RAM, BlockManagerId(driver, bb9210fe839d, 33875, None)
2025-04-30T12:00:16,431 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430120016-0007/2 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:00:16,438 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, bb9210fe839d, 33875, None)
2025-04-30T12:00:16,445 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, bb9210fe839d, 33875, None)
2025-04-30T12:00:16,463 [ExecutorRunner for app-20250430120016-0007/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:00:16,464 [ExecutorRunner for app-20250430120016-0007/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:00:16,465 [ExecutorRunner for app-20250430120016-0007/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:00:16,465 [ExecutorRunner for app-20250430120016-0007/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:00:16,465 [ExecutorRunner for app-20250430120016-0007/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:00:16,466 [ExecutorRunner for app-20250430120016-0007/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:00:16,466 [ExecutorRunner for app-20250430120016-0007/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:00:16,468 [ExecutorRunner for app-20250430120016-0007/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:00:16,469 [ExecutorRunner for app-20250430120016-0007/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:00:16,471 [ExecutorRunner for app-20250430120016-0007/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:00:16,483 [ExecutorRunner for app-20250430120016-0007/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:00:16,484 [ExecutorRunner for app-20250430120016-0007/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:00:16,486 [ExecutorRunner for app-20250430120016-0007/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:00:16,487 [ExecutorRunner for app-20250430120016-0007/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:00:16,488 [ExecutorRunner for app-20250430120016-0007/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:00:16,572 [ExecutorRunner for app-20250430120016-0007/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=38121" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:38121" "--executor-id" "1" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250430120016-0007" "--worker-url" "spark://Worker@172.18.0.3:34443" "--resourceProfileId" "0"
2025-04-30T12:00:16,589 [ExecutorRunner for app-20250430120016-0007/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=38121" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:38121" "--executor-id" "0" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250430120016-0007" "--worker-url" "spark://Worker@172.18.0.5:38821" "--resourceProfileId" "0"
2025-04-30T12:00:16,609 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430120016-0007 with rpId: 0
2025-04-30T12:00:16,623 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430120016-0007 with rpId: 0
2025-04-30T12:00:16,637 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430120016-0007/1 is now RUNNING
2025-04-30T12:00:16,643 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430120016-0007/0 is now RUNNING
2025-04-30T12:00:16,738 [ExecutorRunner for app-20250430120016-0007/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=38121" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:38121" "--executor-id" "2" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250430120016-0007" "--worker-url" "spark://Worker@172.18.0.4:36391" "--resourceProfileId" "0"
2025-04-30T12:00:16,764 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430120016-0007 with rpId: 0
2025-04-30T12:00:16,769 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430120016-0007/2 is now RUNNING
2025-04-30T12:00:16,998 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250430120016-0007.inprogress
2025-04-30T12:00:17,339 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@2f72c3c0{/,null,STOPPED,@Spark}
2025-04-30T12:00:17,348 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7287b184{/jobs,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,351 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@efb1f4{/jobs/json,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,368 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@43ebfa73{/jobs/job,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,373 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@28e2aeeb{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,378 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3ed8bd3d{/stages,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,387 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3915b297{/stages/json,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,396 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@518a5e73{/stages/stage,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,406 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@69163bf1{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,414 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@315c40fd{/stages/pool,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,423 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7f1ed11d{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,440 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@29fdd570{/storage,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,452 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7a4a5cf0{/storage/json,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,462 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6eb60a32{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,467 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1ad7d5a1{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,470 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@55b9f894{/environment,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,474 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@56041559{/environment/json,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,478 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@43ce77c8{/executors,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,481 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1288e51d{/executors/json,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,485 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@313b917c{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,493 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@42e22e1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,497 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@36e20084{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,500 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4ed622ff{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,523 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4ddb2236{/static,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,527 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3244d86f{/,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,532 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5cbcb4b8{/api,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,534 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3092c8e3{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,541 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6dd65270{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,553 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1db22e23{/metrics/json,null,AVAILABLE,@Spark}
2025-04-30T12:00:17,556 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-30T12:00:17,986 [Thread-4] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-30T12:00:18,026 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@2a2301eb{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T12:00:18,034 [Thread-4] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://bb9210fe839d:4040
2025-04-30T12:00:18,060 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-30T12:00:18,067 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-30T12:00:18,084 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250430120016-0007
2025-04-30T12:00:18,086 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250430120016-0007
2025-04-30T12:00:18,100 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430120016-0007/0
2025-04-30T12:00:18,096 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430120016-0007/1
2025-04-30T12:00:18,107 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430120016-0007/2
2025-04-30T12:00:18,108 [ExecutorRunner for app-20250430120016-0007/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430120016-0007/2 interrupted
2025-04-30T12:00:18,109 [ExecutorRunner for app-20250430120016-0007/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430120016-0007/0 interrupted
2025-04-30T12:00:18,109 [ExecutorRunner for app-20250430120016-0007/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430120016-0007/1 interrupted
2025-04-30T12:00:18,110 [ExecutorRunner for app-20250430120016-0007/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:00:18,109 [ExecutorRunner for app-20250430120016-0007/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:00:18,111 [ExecutorRunner for app-20250430120016-0007/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:00:18,136 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430120016-0007/2 finished with state KILLED exitStatus 143
2025-04-30T12:00:18,138 [dispatcher-event-loop-10] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430120016-0007/2
2025-04-30T12:00:18,139 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-30T12:00:18,140 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430120016-0007, execId=2)
2025-04-30T12:00:18,141 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430120016-0007 removed, cleanupLocalDirs = true
2025-04-30T12:00:18,142 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430120016-0007
2025-04-30T12:00:18,144 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430120016-0007/0 finished with state KILLED exitStatus 143
2025-04-30T12:00:18,144 [dispatcher-event-loop-0] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430120016-0007/0
2025-04-30T12:00:18,147 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-30T12:00:18,148 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430120016-0007, execId=0)
2025-04-30T12:00:18,149 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430120016-0007 removed, cleanupLocalDirs = true
2025-04-30T12:00:18,149 [dispatcher-event-loop-1] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-30T12:00:18,149 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430120016-0007
2025-04-30T12:00:18,157 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430120016-0007/1 finished with state KILLED exitStatus 143
2025-04-30T12:00:18,158 [dispatcher-event-loop-5] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430120016-0007/1
2025-04-30T12:00:18,161 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-30T12:00:18,163 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430120016-0007, execId=1)
2025-04-30T12:00:18,165 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430120016-0007 removed, cleanupLocalDirs = true
2025-04-30T12:00:18,166 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430120016-0007
2025-04-30T12:00:18,173 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:00:18,174 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:00:18,181 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-30T12:00:18,184 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-30T12:00:18,205 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:55826 got disassociated, removing it.
2025-04-30T12:00:18,207 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - bb9210fe839d:38121 got disassociated, removing it.
2025-04-30T12:00:18,215 [Thread-4] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-30T12:00:18,653 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:00:18,655 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-f783e511-14a6-46da-87e0-9842e55caa4e
2025-04-30T12:00:18,663 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-f783e511-14a6-46da-87e0-9842e55caa4e/pyspark-faf6b092-7f92-4c3c-87b5-58519feb42cb
2025-04-30T12:00:18,670 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-aee3147a-3ed9-4844-893d-c1da028bfbb1
2025-04-30T12:08:54,406 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-30T12:08:54,412 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:08:54,414 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-30T12:08:54,448 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:08:54,449 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-30T12:08:54,452 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:08:54,453 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: mod-2-pr-4-data-ingestion.py
2025-04-30T12:08:54,476 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-30T12:08:54,486 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-30T12:08:54,487 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-30T12:08:54,551 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-30T12:08:54,553 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-30T12:08:54,555 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:08:54,557 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:08:54,558 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-30T12:08:54,647 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:08:54,965 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 34749.
2025-04-30T12:08:55,009 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-30T12:08:55,049 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-30T12:08:55,075 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-30T12:08:55,077 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-30T12:08:55,084 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-30T12:08:55,110 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-00e8d326-1506-45ed-b333-e70d4aba8c99
2025-04-30T12:08:55,128 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:08:55,147 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-30T12:08:55,201 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3589ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-30T12:08:55,308 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-30T12:08:55,325 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-30T12:08:55,346 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @3736ms
2025-04-30T12:08:55,385 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@413b8567{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T12:08:55,386 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-30T12:08:55,417 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1320cce0{/,null,AVAILABLE,@Spark}
2025-04-30T12:08:55,536 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-30T12:08:55,594 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 33 ms (0 ms spent in bootstraps)
2025-04-30T12:08:55,690 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Registering app mod-2-pr-4-data-ingestion.py
2025-04-30T12:08:55,693 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Registered app mod-2-pr-4-data-ingestion.py with ID app-20250430120855-0008
2025-04-30T12:08:55,694 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430120855-0008 with rpId: 0
2025-04-30T12:08:55,696 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430120855-0008/0 on worker worker-20250429114342-172.18.0.5-38821
2025-04-30T12:08:55,699 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430120855-0008/1 on worker worker-20250429114342-172.18.0.3-34443
2025-04-30T12:08:55,703 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430120855-0008/2 on worker worker-20250429114342-172.18.0.4-36391
2025-04-30T12:08:55,704 [dispatcher-event-loop-11] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250430120855-0008
2025-04-30T12:08:55,707 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430120855-0008/0 on worker-20250429114342-172.18.0.5-38821 (172.18.0.5:38821) with 2 core(s)
2025-04-30T12:08:55,707 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430120855-0008/0 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:08:55,710 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430120855-0008/1 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:08:55,711 [dispatcher-event-loop-11] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430120855-0008/0 on hostPort 172.18.0.5:38821 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:08:55,713 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430120855-0008/2 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:08:55,712 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430120855-0008/1 on worker-20250429114342-172.18.0.3-34443 (172.18.0.3:34443) with 2 core(s)
2025-04-30T12:08:55,716 [dispatcher-event-loop-11] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430120855-0008/1 on hostPort 172.18.0.3:34443 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:08:55,718 [ExecutorRunner for app-20250430120855-0008/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:08:55,719 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430120855-0008/2 on worker-20250429114342-172.18.0.4-36391 (172.18.0.4:36391) with 2 core(s)
2025-04-30T12:08:55,720 [ExecutorRunner for app-20250430120855-0008/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:08:55,721 [dispatcher-event-loop-11] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430120855-0008/2 on hostPort 172.18.0.4:36391 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:08:55,723 [ExecutorRunner for app-20250430120855-0008/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:08:55,724 [ExecutorRunner for app-20250430120855-0008/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:08:55,725 [ExecutorRunner for app-20250430120855-0008/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:08:55,725 [ExecutorRunner for app-20250430120855-0008/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:08:55,725 [ExecutorRunner for app-20250430120855-0008/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:08:55,726 [ExecutorRunner for app-20250430120855-0008/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:08:55,726 [ExecutorRunner for app-20250430120855-0008/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:08:55,726 [ExecutorRunner for app-20250430120855-0008/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:08:55,727 [ExecutorRunner for app-20250430120855-0008/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:08:55,727 [ExecutorRunner for app-20250430120855-0008/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:08:55,728 [ExecutorRunner for app-20250430120855-0008/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:08:55,728 [ExecutorRunner for app-20250430120855-0008/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:08:55,729 [ExecutorRunner for app-20250430120855-0008/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:08:55,735 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40905.
2025-04-30T12:08:55,736 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on bb9210fe839d:40905
2025-04-30T12:08:55,739 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:08:55,755 [ExecutorRunner for app-20250430120855-0008/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=34749" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:34749" "--executor-id" "1" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250430120855-0008" "--worker-url" "spark://Worker@172.18.0.3:34443" "--resourceProfileId" "0"
2025-04-30T12:08:55,756 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, bb9210fe839d, 40905, None)
2025-04-30T12:08:55,765 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager bb9210fe839d:40905 with 434.4 MiB RAM, BlockManagerId(driver, bb9210fe839d, 40905, None)
2025-04-30T12:08:55,771 [ExecutorRunner for app-20250430120855-0008/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=34749" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:34749" "--executor-id" "0" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250430120855-0008" "--worker-url" "spark://Worker@172.18.0.5:38821" "--resourceProfileId" "0"
2025-04-30T12:08:55,772 [ExecutorRunner for app-20250430120855-0008/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=34749" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:34749" "--executor-id" "2" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250430120855-0008" "--worker-url" "spark://Worker@172.18.0.4:36391" "--resourceProfileId" "0"
2025-04-30T12:08:55,773 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, bb9210fe839d, 40905, None)
2025-04-30T12:08:55,775 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, bb9210fe839d, 40905, None)
2025-04-30T12:08:55,793 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430120855-0008 with rpId: 0
2025-04-30T12:08:55,799 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430120855-0008 with rpId: 0
2025-04-30T12:08:55,852 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430120855-0008/2 is now RUNNING
2025-04-30T12:08:55,855 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430120855-0008/0 is now RUNNING
2025-04-30T12:08:55,948 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430120855-0008 with rpId: 0
2025-04-30T12:08:55,956 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430120855-0008/1 is now RUNNING
2025-04-30T12:08:56,334 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250430120855-0008.inprogress
2025-04-30T12:08:56,562 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@1320cce0{/,null,STOPPED,@Spark}
2025-04-30T12:08:56,569 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40857667{/jobs,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,572 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7fa04fb2{/jobs/json,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,574 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@35083451{/jobs/job,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,577 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@354b22b6{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,580 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@484d933e{/stages,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,583 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7274d8f2{/stages/json,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,589 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2dab95f3{/stages/stage,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,592 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4622b9f0{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,595 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@611e343f{/stages/pool,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,599 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@56578f8c{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,603 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@fa43f57{/storage,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,606 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2a9cdf28{/storage/json,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,610 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@41d8187c{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,614 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@251ea385{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,618 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@d678b60{/environment,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,622 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@ef17b66{/environment/json,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,626 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@58ad57bd{/executors,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,632 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1cadc900{/executors/json,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,635 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40aadd59{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,641 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3eeb6954{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,645 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1964fe85{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,651 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@52124300{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,671 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@76c527e1{/static,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,673 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@548a25dc{/,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,678 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@28825700{/api,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,685 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@611d70f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,688 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@21e7c069{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,702 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@64ddd77d{/metrics/json,null,AVAILABLE,@Spark}
2025-04-30T12:08:56,706 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-30T12:08:57,396 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-04-30T12:08:57,410 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
2025-04-30T12:08:57,530 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@739d96e{/SQL,null,AVAILABLE,@Spark}
2025-04-30T12:08:57,562 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@113354b6{/SQL/json,null,AVAILABLE,@Spark}
2025-04-30T12:08:57,580 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7bb072e9{/SQL/execution,null,AVAILABLE,@Spark}
2025-04-30T12:08:57,593 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2dd2a8f2{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-04-30T12:08:57,611 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@9867130{/static/sql,null,AVAILABLE,@Spark}
2025-04-30T12:08:58,662 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 424@066f03cf7e53
2025-04-30T12:08:58,676 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:08:58,678 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:08:58,678 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:08:58,690 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 354@c27de723f0d4
2025-04-30T12:08:58,707 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:08:58,708 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:08:58,709 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:08:58,757 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 414@6cca10b5c892
2025-04-30T12:08:58,775 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:08:58,777 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:08:58,777 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:08:59,479 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:08:59,575 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:08:59,704 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:08:59,747 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:08:59,755 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:08:59,758 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:08:59,775 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:08:59,785 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:08:59,963 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:08:59,966 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:08:59,967 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:08:59,970 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:08:59,975 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:09:00,013 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:09:00,034 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:09:00,040 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:09:00,045 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:09:00,050 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:09:00,529 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 126 ms to list leaf files for 1 paths.
2025-04-30T12:09:00,623 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:34749 after 150 ms (0 ms spent in bootstraps)
2025-04-30T12:09:00,691 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:34749 after 144 ms (0 ms spent in bootstraps)
2025-04-30T12:09:00,753 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:34749 after 141 ms (0 ms spent in bootstraps)
2025-04-30T12:09:00,763 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 47 ms to list leaf files for 1 paths.
2025-04-30T12:09:00,878 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:09:00,879 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:09:00,881 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:09:00,883 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:09:00,885 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:09:00,930 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:09:00,935 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:09:00,936 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:09:00,937 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:09:00,938 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:09:00,967 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:09:00,969 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:09:00,972 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:09:00,974 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:09:00,976 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:09:00,977 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:34749 after 6 ms (0 ms spent in bootstraps)
2025-04-30T12:09:01,056 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:34749 after 8 ms (0 ms spent in bootstraps)
2025-04-30T12:09:01,103 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:34749 after 7 ms (0 ms spent in bootstraps)
2025-04-30T12:09:01,141 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-c5ae99e1-4d9a-42b7-9555-210f29b2c4bb/executor-be5d80fc-09f1-493e-a1c4-6fe2874e7d76/blockmgr-6a0f268a-7342-4f4b-b2a3-c526d1a7b35b
2025-04-30T12:09:01,200 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:09:01,226 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-00d4d2fa-4667-4dcc-a4a7-1a25b94c3d06/executor-366fe657-351d-461d-bd64-9ca71e0bed4d/blockmgr-5a92fd96-d6e9-41d9-b3f4-797767cab558
2025-04-30T12:09:01,235 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-8bb716ea-3eee-4da7-a70b-3a99059b4da9/executor-47afe87b-5aa9-40cc-b0bb-703a67116204/blockmgr-2f7153a8-26b0-4db6-8fe5-65fdc202aaae
2025-04-30T12:09:01,282 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:09:01,288 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:09:01,570 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:34749
2025-04-30T12:09:01,572 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.5:38821
2025-04-30T12:09:01,581 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.5:38821 after 5 ms (0 ms spent in bootstraps)
2025-04-30T12:09:01,587 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.5:38821
2025-04-30T12:09:01,599 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:09:01,602 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:09:01,605 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:09:01,675 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:34749
2025-04-30T12:09:01,678 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.4:36391
2025-04-30T12:09:01,686 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:56590) with ID 0,  ResourceProfileId 0
2025-04-30T12:09:01,700 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.3:34443
2025-04-30T12:09:01,703 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:34749
2025-04-30T12:09:01,715 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:09:01,726 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 0 on host 172.18.0.5
2025-04-30T12:09:01,730 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:09:01,733 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.3:34443 after 11 ms (0 ms spent in bootstraps)
2025-04-30T12:09:01,737 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:09:01,740 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.3:34443
2025-04-30T12:09:01,762 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:09:01,763 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:09:01,763 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:09:01,766 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:09:01,767 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:09:01,768 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.4:36391
2025-04-30T12:09:01,768 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:09:01,772 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.4:36391 after 62 ms (0 ms spent in bootstraps)
2025-04-30T12:09:01,822 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:55486) with ID 2,  ResourceProfileId 0
2025-04-30T12:09:01,830 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:33684) with ID 1,  ResourceProfileId 0
2025-04-30T12:09:01,843 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:09:01,845 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:09:01,849 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 2 on host 172.18.0.4
2025-04-30T12:09:01,852 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 1 on host 172.18.0.3
2025-04-30T12:09:01,857 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:09:01,857 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:09:01,859 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:09:01,861 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:09:01,859 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43417.
2025-04-30T12:09:01,865 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.5:43417
2025-04-30T12:09:01,884 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:09:01,906 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(0, 172.18.0.5, 43417, None)
2025-04-30T12:09:01,946 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35197.
2025-04-30T12:09:01,947 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.5:43417 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.5, 43417, None)
2025-04-30T12:09:01,949 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.3:35197
2025-04-30T12:09:01,957 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(0, 172.18.0.5, 43417, None)
2025-04-30T12:09:01,958 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:09:01,961 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38567.
2025-04-30T12:09:01,961 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(0, 172.18.0.5, 43417, None)
2025-04-30T12:09:01,962 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.4:38567
2025-04-30T12:09:01,967 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:09:01,971 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(1, 172.18.0.3, 35197, None)
2025-04-30T12:09:01,975 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:09:01,983 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@146dab70 for default.
2025-04-30T12:09:01,985 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(2, 172.18.0.4, 38567, None)
2025-04-30T12:09:01,995 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.3:35197 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.3, 35197, None)
2025-04-30T12:09:02,006 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.4:38567 with 434.4 MiB RAM, BlockManagerId(2, 172.18.0.4, 38567, None)
2025-04-30T12:09:02,014 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(1, 172.18.0.3, 35197, None)
2025-04-30T12:09:02,019 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(2, 172.18.0.4, 38567, None)
2025-04-30T12:09:02,020 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(1, 172.18.0.3, 35197, None)
2025-04-30T12:09:02,022 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(2, 172.18.0.4, 38567, None)
2025-04-30T12:09:02,038 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:09:02,040 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:09:02,042 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@617f904f for default.
2025-04-30T12:09:02,043 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@25c7034b for default.
2025-04-30T12:09:03,881 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T12:09:03,883 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T12:09:04,085 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 200.2 KiB, free 434.2 MiB)
2025-04-30T12:09:04,159 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
2025-04-30T12:09:04,163 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on bb9210fe839d:40905 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T12:09:04,170 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 0 from json at <unknown>:0
2025-04-30T12:09:04,180 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T12:09:04,342 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: json at <unknown>:0
2025-04-30T12:09:04,368 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 0 (json at <unknown>:0) with 1 output partitions
2025-04-30T12:09:04,370 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 0 (json at <unknown>:0)
2025-04-30T12:09:04,371 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T12:09:04,373 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T12:09:04,378 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0), which has no missing parents
2025-04-30T12:09:04,481 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.5 KiB, free 434.2 MiB)
2025-04-30T12:09:04,488 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
2025-04-30T12:09:04,490 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on bb9210fe839d:40905 (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T12:09:04,492 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 1 from broadcast at DAGScheduler.scala:1585
2025-04-30T12:09:04,512 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T12:09:04,515 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 0.0 with 1 tasks resource profile 0
2025-04-30T12:09:04,558 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 9650 bytes) 
2025-04-30T12:09:04,588 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 0
2025-04-30T12:09:04,598 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 0.0 (TID 0)
2025-04-30T12:09:04,700 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:09:04,751 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:40905 after 2 ms (0 ms spent in bootstraps)
2025-04-30T12:09:04,792 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.4 MiB)
2025-04-30T12:09:04,799 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on 172.18.0.5:43417 (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T12:09:04,805 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 1 took 103 ms
2025-04-30T12:09:04,885 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.5 KiB, free 434.4 MiB)
2025-04-30T12:09:05,260 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl, range: 0-35354, partition values: [empty row]
2025-04-30T12:09:05,647 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 213.198358 ms
2025-04-30T12:09:05,650 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:09:05,660 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.3 MiB)
2025-04-30T12:09:05,664 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on 172.18.0.5:43417 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T12:09:05,667 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 0 took 16 ms
2025-04-30T12:09:05,755 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T12:09:05,936 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 0.0 (TID 0). 2488 bytes result sent to driver
2025-04-30T12:09:05,955 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 0.0 (TID 0) in 1413 ms on 172.18.0.5 (executor 0) (1/1)
2025-04-30T12:09:05,957 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-04-30T12:09:05,963 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 0 (json at <unknown>:0) finished in 1.564 s
2025-04-30T12:09:05,975 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T12:09:05,977 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 0: Stage finished
2025-04-30T12:09:05,982 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 finished: json at <unknown>:0, took 1.638956 s
2025-04-30T12:09:06,019 [Thread-4] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-30T12:09:06,030 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@413b8567{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T12:09:06,036 [Thread-4] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://bb9210fe839d:4040
2025-04-30T12:09:06,040 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-30T12:09:06,041 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-30T12:09:06,049 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:09:06,051 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:09:06,051 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:09:06,054 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250430120855-0008
2025-04-30T12:09:06,055 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250430120855-0008
2025-04-30T12:09:06,060 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430120855-0008/1
2025-04-30T12:09:06,061 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430120855-0008/0
2025-04-30T12:09:06,063 [ExecutorRunner for app-20250430120855-0008/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430120855-0008/0 interrupted
2025-04-30T12:09:06,064 [ExecutorRunner for app-20250430120855-0008/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430120855-0008/1 interrupted
2025-04-30T12:09:06,067 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430120855-0008/2
2025-04-30T12:09:06,069 [ExecutorRunner for app-20250430120855-0008/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:09:06,069 [ExecutorRunner for app-20250430120855-0008/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:09:06,071 [ExecutorRunner for app-20250430120855-0008/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430120855-0008/2 interrupted
2025-04-30T12:09:06,073 [ExecutorRunner for app-20250430120855-0008/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:09:06,074 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:09:06,078 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:09:06,079 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:09:06,080 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:09:06,088 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:09:06,093 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:09:06,094 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:09:06,095 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:09:06,095 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:09:06,096 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:09:06,099 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:09:06,103 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:09:06,113 [dispatcher-event-loop-0] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-30T12:09:06,134 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:09:06,136 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:09:06,139 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430120855-0008/1 finished with state KILLED exitStatus 143
2025-04-30T12:09:06,139 [dispatcher-event-loop-6] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430120855-0008/1
2025-04-30T12:09:06,140 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-30T12:09:06,140 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430120855-0008, execId=1)
2025-04-30T12:09:06,141 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430120855-0008 removed, cleanupLocalDirs = true
2025-04-30T12:09:06,141 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430120855-0008
2025-04-30T12:09:06,145 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-30T12:09:06,145 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430120855-0008/0 finished with state KILLED exitStatus 143
2025-04-30T12:09:06,145 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-30T12:09:06,146 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430120855-0008, execId=0)
2025-04-30T12:09:06,146 [dispatcher-event-loop-9] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430120855-0008/0
2025-04-30T12:09:06,147 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430120855-0008 removed, cleanupLocalDirs = true
2025-04-30T12:09:06,148 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430120855-0008
2025-04-30T12:09:06,149 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-30T12:09:06,154 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:55512 got disassociated, removing it.
2025-04-30T12:09:06,155 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.master.Master [] - bb9210fe839d:34749 got disassociated, removing it.
2025-04-30T12:09:06,164 [Thread-4] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-30T12:09:06,394 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:09:06,395 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-806c7161-ef72-4e7c-bdb1-7eea071a339b/pyspark-1b2c1cbf-936c-4801-a82a-f1155e8820cc
2025-04-30T12:09:06,402 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-806c7161-ef72-4e7c-bdb1-7eea071a339b
2025-04-30T12:09:06,408 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-841019e7-a57c-44fe-9459-af959b7242ca
2025-04-30T12:09:06,439 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430120855-0008/2 finished with state KILLED exitStatus 143
2025-04-30T12:09:06,440 [dispatcher-event-loop-5] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430120855-0008/2
2025-04-30T12:09:06,440 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-30T12:09:06,441 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430120855-0008, execId=2)
2025-04-30T12:09:06,442 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430120855-0008 removed, cleanupLocalDirs = true
2025-04-30T12:09:06,442 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430120855-0008
2025-04-30T12:09:34,675 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-30T12:09:34,679 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:09:34,680 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-30T12:09:34,707 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:09:34,708 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-30T12:09:34,710 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:09:34,711 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: mod-2-pr-4-data-ingestion.py
2025-04-30T12:09:34,737 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-30T12:09:34,750 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-30T12:09:34,752 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-30T12:09:34,820 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-30T12:09:34,822 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-30T12:09:34,825 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:09:34,826 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:09:34,828 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-30T12:09:34,903 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:09:35,212 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 41477.
2025-04-30T12:09:35,245 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-30T12:09:35,284 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-30T12:09:35,307 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-30T12:09:35,308 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-30T12:09:35,313 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-30T12:09:35,336 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-0ba266c1-3b30-4d4c-8ed4-343aa2fcf011
2025-04-30T12:09:35,353 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:09:35,375 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-30T12:09:35,426 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3467ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-30T12:09:35,529 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-30T12:09:35,542 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-30T12:09:35,565 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @3608ms
2025-04-30T12:09:35,602 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@413b8567{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T12:09:35,603 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-30T12:09:35,628 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1320cce0{/,null,AVAILABLE,@Spark}
2025-04-30T12:09:35,751 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-30T12:09:35,805 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 32 ms (0 ms spent in bootstraps)
2025-04-30T12:09:35,911 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Registering app mod-2-pr-4-data-ingestion.py
2025-04-30T12:09:35,913 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Registered app mod-2-pr-4-data-ingestion.py with ID app-20250430120935-0009
2025-04-30T12:09:35,920 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430120935-0009 with rpId: 0
2025-04-30T12:09:35,922 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430120935-0009/0 on worker worker-20250429114342-172.18.0.5-38821
2025-04-30T12:09:35,924 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430120935-0009/1 on worker worker-20250429114342-172.18.0.3-34443
2025-04-30T12:09:35,926 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430120935-0009/2 on worker worker-20250429114342-172.18.0.4-36391
2025-04-30T12:09:35,927 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430120935-0009/0 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:09:35,928 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250430120935-0009
2025-04-30T12:09:35,931 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430120935-0009/1 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:09:35,931 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430120935-0009/2 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:09:35,932 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430120935-0009/0 on worker-20250429114342-172.18.0.5-38821 (172.18.0.5:38821) with 2 core(s)
2025-04-30T12:09:35,935 [ExecutorRunner for app-20250430120935-0009/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:09:35,937 [ExecutorRunner for app-20250430120935-0009/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:09:35,937 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430120935-0009/0 on hostPort 172.18.0.5:38821 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:09:35,937 [ExecutorRunner for app-20250430120935-0009/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:09:35,938 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430120935-0009/1 on worker-20250429114342-172.18.0.3-34443 (172.18.0.3:34443) with 2 core(s)
2025-04-30T12:09:35,938 [ExecutorRunner for app-20250430120935-0009/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:09:35,939 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430120935-0009/1 on hostPort 172.18.0.3:34443 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:09:35,939 [ExecutorRunner for app-20250430120935-0009/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:09:35,940 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430120935-0009/2 on worker-20250429114342-172.18.0.4-36391 (172.18.0.4:36391) with 2 core(s)
2025-04-30T12:09:35,940 [ExecutorRunner for app-20250430120935-0009/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:09:35,940 [ExecutorRunner for app-20250430120935-0009/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:09:35,942 [ExecutorRunner for app-20250430120935-0009/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:09:35,943 [ExecutorRunner for app-20250430120935-0009/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:09:35,943 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430120935-0009/2 on hostPort 172.18.0.4:36391 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:09:35,943 [ExecutorRunner for app-20250430120935-0009/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:09:35,944 [ExecutorRunner for app-20250430120935-0009/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:09:35,944 [ExecutorRunner for app-20250430120935-0009/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:09:35,945 [ExecutorRunner for app-20250430120935-0009/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:09:35,945 [ExecutorRunner for app-20250430120935-0009/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:09:35,946 [ExecutorRunner for app-20250430120935-0009/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:09:35,952 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34187.
2025-04-30T12:09:35,953 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on bb9210fe839d:34187
2025-04-30T12:09:35,958 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:09:35,971 [ExecutorRunner for app-20250430120935-0009/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=41477" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:41477" "--executor-id" "2" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250430120935-0009" "--worker-url" "spark://Worker@172.18.0.4:36391" "--resourceProfileId" "0"
2025-04-30T12:09:35,972 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, bb9210fe839d, 34187, None)
2025-04-30T12:09:35,977 [ExecutorRunner for app-20250430120935-0009/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=41477" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:41477" "--executor-id" "0" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250430120935-0009" "--worker-url" "spark://Worker@172.18.0.5:38821" "--resourceProfileId" "0"
2025-04-30T12:09:35,980 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager bb9210fe839d:34187 with 434.4 MiB RAM, BlockManagerId(driver, bb9210fe839d, 34187, None)
2025-04-30T12:09:35,985 [ExecutorRunner for app-20250430120935-0009/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=41477" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:41477" "--executor-id" "1" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250430120935-0009" "--worker-url" "spark://Worker@172.18.0.3:34443" "--resourceProfileId" "0"
2025-04-30T12:09:35,987 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, bb9210fe839d, 34187, None)
2025-04-30T12:09:35,990 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, bb9210fe839d, 34187, None)
2025-04-30T12:09:35,994 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430120935-0009 with rpId: 0
2025-04-30T12:09:36,008 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430120935-0009 with rpId: 0
2025-04-30T12:09:36,011 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430120935-0009 with rpId: 0
2025-04-30T12:09:36,028 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430120935-0009/2 is now RUNNING
2025-04-30T12:09:36,035 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430120935-0009/1 is now RUNNING
2025-04-30T12:09:36,040 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430120935-0009/0 is now RUNNING
2025-04-30T12:09:36,498 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250430120935-0009.inprogress
2025-04-30T12:09:36,806 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@1320cce0{/,null,STOPPED,@Spark}
2025-04-30T12:09:36,811 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40857667{/jobs,null,AVAILABLE,@Spark}
2025-04-30T12:09:36,815 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7fa04fb2{/jobs/json,null,AVAILABLE,@Spark}
2025-04-30T12:09:36,824 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@35083451{/jobs/job,null,AVAILABLE,@Spark}
2025-04-30T12:09:36,832 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@354b22b6{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-30T12:09:36,839 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@484d933e{/stages,null,AVAILABLE,@Spark}
2025-04-30T12:09:36,844 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7274d8f2{/stages/json,null,AVAILABLE,@Spark}
2025-04-30T12:09:36,857 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2dab95f3{/stages/stage,null,AVAILABLE,@Spark}
2025-04-30T12:09:36,861 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4622b9f0{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-30T12:09:36,870 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@611e343f{/stages/pool,null,AVAILABLE,@Spark}
2025-04-30T12:09:36,875 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@56578f8c{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-30T12:09:36,879 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@fa43f57{/storage,null,AVAILABLE,@Spark}
2025-04-30T12:09:36,883 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2a9cdf28{/storage/json,null,AVAILABLE,@Spark}
2025-04-30T12:09:36,896 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@41d8187c{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-30T12:09:36,899 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@251ea385{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-30T12:09:36,913 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@d678b60{/environment,null,AVAILABLE,@Spark}
2025-04-30T12:09:36,925 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@ef17b66{/environment/json,null,AVAILABLE,@Spark}
2025-04-30T12:09:36,935 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@58ad57bd{/executors,null,AVAILABLE,@Spark}
2025-04-30T12:09:36,943 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1cadc900{/executors/json,null,AVAILABLE,@Spark}
2025-04-30T12:09:36,954 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40aadd59{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-30T12:09:36,962 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3eeb6954{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-30T12:09:36,973 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1964fe85{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-30T12:09:36,982 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@52124300{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-30T12:09:37,019 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@76c527e1{/static,null,AVAILABLE,@Spark}
2025-04-30T12:09:37,023 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@548a25dc{/,null,AVAILABLE,@Spark}
2025-04-30T12:09:37,034 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@28825700{/api,null,AVAILABLE,@Spark}
2025-04-30T12:09:37,041 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@611d70f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-30T12:09:37,045 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@21e7c069{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-30T12:09:37,059 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@64ddd77d{/metrics/json,null,AVAILABLE,@Spark}
2025-04-30T12:09:37,075 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-30T12:09:37,782 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-04-30T12:09:37,806 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
2025-04-30T12:09:37,857 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@739d96e{/SQL,null,AVAILABLE,@Spark}
2025-04-30T12:09:37,868 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@113354b6{/SQL/json,null,AVAILABLE,@Spark}
2025-04-30T12:09:37,874 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7bb072e9{/SQL/execution,null,AVAILABLE,@Spark}
2025-04-30T12:09:37,878 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2dd2a8f2{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-04-30T12:09:37,888 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@9867130{/static/sql,null,AVAILABLE,@Spark}
2025-04-30T12:09:38,938 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 494@6cca10b5c892
2025-04-30T12:09:38,975 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 510@066f03cf7e53
2025-04-30T12:09:38,986 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:09:38,990 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:09:38,992 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:09:39,014 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:09:39,016 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:09:39,017 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:09:39,025 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 433@c27de723f0d4
2025-04-30T12:09:39,058 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:09:39,060 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:09:39,061 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:09:39,996 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:09:40,029 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:09:40,133 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:09:40,347 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:09:40,353 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:09:40,355 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:09:40,357 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:09:40,357 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:09:40,360 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:09:40,361 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:09:40,363 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:09:40,364 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:09:40,385 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:09:40,461 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:09:40,486 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:09:40,496 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:09:40,504 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:09:40,519 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:09:40,647 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 236 ms to list leaf files for 1 paths.
2025-04-30T12:09:40,872 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 27 ms to list leaf files for 1 paths.
2025-04-30T12:09:41,164 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:41477 after 128 ms (0 ms spent in bootstraps)
2025-04-30T12:09:41,169 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:41477 after 136 ms (0 ms spent in bootstraps)
2025-04-30T12:09:41,190 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:41477 after 137 ms (0 ms spent in bootstraps)
2025-04-30T12:09:41,475 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:09:41,481 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:09:41,491 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:09:41,490 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:09:41,494 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:09:41,495 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:09:41,497 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:09:41,497 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:09:41,499 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:09:41,500 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:09:41,501 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:09:41,502 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:09:41,502 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:09:41,504 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:09:41,506 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:09:41,638 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:41477 after 5 ms (0 ms spent in bootstraps)
2025-04-30T12:09:41,657 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:41477 after 14 ms (0 ms spent in bootstraps)
2025-04-30T12:09:41,694 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:41477 after 19 ms (0 ms spent in bootstraps)
2025-04-30T12:09:41,869 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-00d4d2fa-4667-4dcc-a4a7-1a25b94c3d06/executor-b79ae455-e256-4e03-a60e-290bf1d421ef/blockmgr-cd63583a-a73c-4466-b5e6-f34eb660023f
2025-04-30T12:09:41,940 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-8bb716ea-3eee-4da7-a70b-3a99059b4da9/executor-3b403614-67c4-4575-af3f-63723824948a/blockmgr-5de3525a-a7e8-4bdf-9fd3-f6b0bd8cbe02
2025-04-30T12:09:41,949 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:09:42,010 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-c5ae99e1-4d9a-42b7-9555-210f29b2c4bb/executor-ecf46419-5749-423b-8b9b-bafcccb25c77/blockmgr-45540058-a823-441e-9dcf-07b4ede8dbe7
2025-04-30T12:09:42,039 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:09:42,097 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:09:42,348 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:41477
2025-04-30T12:09:42,350 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.3:34443
2025-04-30T12:09:42,363 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.3:34443 after 8 ms (0 ms spent in bootstraps)
2025-04-30T12:09:42,379 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:09:42,382 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:09:42,383 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.3:34443
2025-04-30T12:09:42,384 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:09:42,400 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:41477
2025-04-30T12:09:42,407 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.4:36391
2025-04-30T12:09:42,420 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:09:42,422 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:09:42,424 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:09:42,429 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.4:36391 after 17 ms (0 ms spent in bootstraps)
2025-04-30T12:09:42,446 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:41477
2025-04-30T12:09:42,447 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.5:38821
2025-04-30T12:09:42,454 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:40198) with ID 1,  ResourceProfileId 0
2025-04-30T12:09:42,461 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.4:36391
2025-04-30T12:09:42,470 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:09:42,473 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:09:42,477 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:09:42,477 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.5:38821 after 14 ms (0 ms spent in bootstraps)
2025-04-30T12:09:42,483 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.5:38821
2025-04-30T12:09:42,483 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:09:42,507 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 1 on host 172.18.0.3
2025-04-30T12:09:42,521 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:09:42,523 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:09:42,529 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:39538) with ID 2,  ResourceProfileId 0
2025-04-30T12:09:42,533 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:46900) with ID 0,  ResourceProfileId 0
2025-04-30T12:09:42,548 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:09:42,547 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:09:42,555 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 0 on host 172.18.0.5
2025-04-30T12:09:42,557 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:09:42,559 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 2 on host 172.18.0.4
2025-04-30T12:09:42,562 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:09:42,562 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:09:42,564 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:09:42,621 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46475.
2025-04-30T12:09:42,622 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.3:46475
2025-04-30T12:09:42,626 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:09:42,638 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(1, 172.18.0.3, 46475, None)
2025-04-30T12:09:42,644 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44313.
2025-04-30T12:09:42,645 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33111.
2025-04-30T12:09:42,646 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.4:44313
2025-04-30T12:09:42,647 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.5:33111
2025-04-30T12:09:42,651 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:09:42,651 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:09:42,654 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.3:46475 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.3, 46475, None)
2025-04-30T12:09:42,660 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(1, 172.18.0.3, 46475, None)
2025-04-30T12:09:42,663 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(1, 172.18.0.3, 46475, None)
2025-04-30T12:09:42,663 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(0, 172.18.0.5, 33111, None)
2025-04-30T12:09:42,666 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(2, 172.18.0.4, 44313, None)
2025-04-30T12:09:42,678 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.5:33111 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.5, 33111, None)
2025-04-30T12:09:42,678 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:09:42,681 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.4:44313 with 434.4 MiB RAM, BlockManagerId(2, 172.18.0.4, 44313, None)
2025-04-30T12:09:42,682 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@51b1068 for default.
2025-04-30T12:09:42,687 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(0, 172.18.0.5, 33111, None)
2025-04-30T12:09:42,690 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(2, 172.18.0.4, 44313, None)
2025-04-30T12:09:42,691 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(0, 172.18.0.5, 33111, None)
2025-04-30T12:09:42,694 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(2, 172.18.0.4, 44313, None)
2025-04-30T12:09:42,704 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:09:42,707 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2f9a6030 for default.
2025-04-30T12:09:42,710 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:09:42,713 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@25c7034b for default.
2025-04-30T12:09:44,084 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T12:09:44,087 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T12:09:44,262 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 200.2 KiB, free 434.2 MiB)
2025-04-30T12:09:44,311 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
2025-04-30T12:09:44,314 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on bb9210fe839d:34187 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T12:09:44,320 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 0 from json at <unknown>:0
2025-04-30T12:09:44,333 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T12:09:44,477 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: json at <unknown>:0
2025-04-30T12:09:44,494 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 0 (json at <unknown>:0) with 1 output partitions
2025-04-30T12:09:44,495 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 0 (json at <unknown>:0)
2025-04-30T12:09:44,496 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T12:09:44,499 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T12:09:44,504 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0), which has no missing parents
2025-04-30T12:09:44,582 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.5 KiB, free 434.2 MiB)
2025-04-30T12:09:44,587 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
2025-04-30T12:09:44,589 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on bb9210fe839d:34187 (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T12:09:44,591 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 1 from broadcast at DAGScheduler.scala:1585
2025-04-30T12:09:44,609 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T12:09:44,610 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 0.0 with 1 tasks resource profile 0
2025-04-30T12:09:44,642 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 9650 bytes) 
2025-04-30T12:09:44,664 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 0
2025-04-30T12:09:44,675 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 0.0 (TID 0)
2025-04-30T12:09:44,779 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:09:44,824 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:34187 after 2 ms (0 ms spent in bootstraps)
2025-04-30T12:09:44,874 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.4 MiB)
2025-04-30T12:09:44,882 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on 172.18.0.5:33111 (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T12:09:44,888 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 1 took 108 ms
2025-04-30T12:09:44,944 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.5 KiB, free 434.4 MiB)
2025-04-30T12:09:45,268 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl, range: 0-35354, partition values: [empty row]
2025-04-30T12:09:45,599 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 201.797458 ms
2025-04-30T12:09:45,601 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:09:45,609 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.3 MiB)
2025-04-30T12:09:45,612 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on 172.18.0.5:33111 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T12:09:45,615 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 0 took 13 ms
2025-04-30T12:09:45,688 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T12:09:45,848 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 0.0 (TID 0). 2488 bytes result sent to driver
2025-04-30T12:09:45,863 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 0.0 (TID 0) in 1232 ms on 172.18.0.5 (executor 0) (1/1)
2025-04-30T12:09:45,865 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-04-30T12:09:45,870 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 0 (json at <unknown>:0) finished in 1.354 s
2025-04-30T12:09:45,874 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T12:09:45,875 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 0: Stage finished
2025-04-30T12:09:45,878 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 finished: json at <unknown>:0, took 1.400232 s
2025-04-30T12:09:45,948 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on bb9210fe839d:34187 in memory (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T12:09:45,959 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on 172.18.0.5:33111 in memory (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T12:09:45,976 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_0_piece0 on bb9210fe839d:34187 in memory (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T12:09:45,981 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_0_piece0 on 172.18.0.5:33111 in memory (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T12:09:46,079 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T12:09:46,080 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T12:09:46,457 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 231.140275 ms
2025-04-30T12:09:46,464 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 200.1 KiB, free 434.2 MiB)
2025-04-30T12:09:46,478 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.2 MiB)
2025-04-30T12:09:46,480 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on bb9210fe839d:34187 (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T12:09:46,482 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 2 from showString at <unknown>:0
2025-04-30T12:09:46,487 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T12:09:46,517 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: showString at <unknown>:0
2025-04-30T12:09:46,520 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 1 (showString at <unknown>:0) with 1 output partitions
2025-04-30T12:09:46,520 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 1 (showString at <unknown>:0)
2025-04-30T12:09:46,521 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T12:09:46,522 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T12:09:46,523 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 1 (MapPartitionsRDD[7] at showString at <unknown>:0), which has no missing parents
2025-04-30T12:09:46,530 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 20.5 KiB, free 434.2 MiB)
2025-04-30T12:09:46,541 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 434.1 MiB)
2025-04-30T12:09:46,542 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on bb9210fe839d:34187 (size: 8.4 KiB, free: 434.4 MiB)
2025-04-30T12:09:46,544 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 3 from broadcast at DAGScheduler.scala:1585
2025-04-30T12:09:46,545 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T12:09:46,546 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 1.0 with 1 tasks resource profile 0
2025-04-30T12:09:46,549 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.4, executor 2, partition 0, PROCESS_LOCAL, 9650 bytes) 
2025-04-30T12:09:46,562 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 1
2025-04-30T12:09:46,575 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 1.0 (TID 1)
2025-04-30T12:09:46,684 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:09:46,726 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:34187 after 2 ms (0 ms spent in bootstraps)
2025-04-30T12:09:46,751 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 434.4 MiB)
2025-04-30T12:09:46,764 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on 172.18.0.4:44313 (size: 8.4 KiB, free: 434.4 MiB)
2025-04-30T12:09:46,769 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 3 took 84 ms
2025-04-30T12:09:46,841 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 20.5 KiB, free 434.4 MiB)
2025-04-30T12:09:47,515 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 284.975379 ms
2025-04-30T12:09:47,521 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl, range: 0-35354, partition values: [empty row]
2025-04-30T12:09:47,653 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 45.388095 ms
2025-04-30T12:09:47,674 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:09:47,684 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.3 MiB)
2025-04-30T12:09:47,687 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on 172.18.0.4:44313 (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T12:09:47,690 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 2 took 14 ms
2025-04-30T12:09:47,770 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T12:09:47,943 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 1.0 (TID 1). 3225 bytes result sent to driver
2025-04-30T12:09:47,952 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 1.0 (TID 1) in 1404 ms on 172.18.0.4 (executor 2) (1/1)
2025-04-30T12:09:47,953 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-04-30T12:09:47,954 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 1 (showString at <unknown>:0) finished in 1.429 s
2025-04-30T12:09:47,956 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T12:09:47,957 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 1: Stage finished
2025-04-30T12:09:47,958 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 1 finished: showString at <unknown>:0, took 1.439555 s
2025-04-30T12:09:48,032 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 43.782617 ms
2025-04-30T12:09:48,053 [Thread-4] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-30T12:09:48,064 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@413b8567{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T12:09:48,070 [Thread-4] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://bb9210fe839d:4040
2025-04-30T12:09:48,077 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-30T12:09:48,078 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-30T12:09:48,086 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:09:48,086 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:09:48,090 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:09:48,091 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250430120935-0009
2025-04-30T12:09:48,092 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250430120935-0009
2025-04-30T12:09:48,100 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430120935-0009/0
2025-04-30T12:09:48,101 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430120935-0009/1
2025-04-30T12:09:48,105 [ExecutorRunner for app-20250430120935-0009/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430120935-0009/0 interrupted
2025-04-30T12:09:48,105 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430120935-0009/2
2025-04-30T12:09:48,105 [ExecutorRunner for app-20250430120935-0009/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:09:48,106 [ExecutorRunner for app-20250430120935-0009/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430120935-0009/1 interrupted
2025-04-30T12:09:48,106 [ExecutorRunner for app-20250430120935-0009/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:09:48,106 [ExecutorRunner for app-20250430120935-0009/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430120935-0009/2 interrupted
2025-04-30T12:09:48,107 [ExecutorRunner for app-20250430120935-0009/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:09:48,110 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:09:48,126 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:09:48,134 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:09:48,135 [shutdown-hook-0] INFO  org.apache.spark.storage.DiskBlockManager [] - Shutdown hook called
2025-04-30T12:09:48,136 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:09:48,144 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:09:48,153 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:09:48,158 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:09:48,164 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:09:48,165 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:09:48,164 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:09:48,168 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:09:48,180 [dispatcher-event-loop-0] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-30T12:09:48,209 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430120935-0009/0 finished with state KILLED exitStatus 143
2025-04-30T12:09:48,210 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-30T12:09:48,211 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430120935-0009, execId=0)
2025-04-30T12:09:48,211 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:09:48,211 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430120935-0009 removed, cleanupLocalDirs = true
2025-04-30T12:09:48,211 [dispatcher-event-loop-1] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430120935-0009/0
2025-04-30T12:09:48,211 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430120935-0009
2025-04-30T12:09:48,213 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:09:48,222 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-30T12:09:48,228 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-30T12:09:48,233 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:55420 got disassociated, removing it.
2025-04-30T12:09:48,237 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - bb9210fe839d:41477 got disassociated, removing it.
2025-04-30T12:09:48,244 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430120935-0009/2 finished with state KILLED exitStatus 143
2025-04-30T12:09:48,245 [dispatcher-event-loop-4] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430120935-0009/2
2025-04-30T12:09:48,245 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-30T12:09:48,245 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430120935-0009, execId=2)
2025-04-30T12:09:48,246 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430120935-0009
2025-04-30T12:09:48,246 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430120935-0009 removed, cleanupLocalDirs = true
2025-04-30T12:09:48,248 [Thread-4] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-30T12:09:48,485 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430120935-0009/1 finished with state KILLED exitStatus 143
2025-04-30T12:09:48,486 [dispatcher-event-loop-5] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430120935-0009/1
2025-04-30T12:09:48,486 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-30T12:09:48,486 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430120935-0009, execId=1)
2025-04-30T12:09:48,487 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430120935-0009 removed, cleanupLocalDirs = true
2025-04-30T12:09:48,487 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430120935-0009
2025-04-30T12:09:48,778 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:09:48,779 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-65322a41-f6c6-4555-9860-e0b9e227bd56
2025-04-30T12:09:48,786 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-f32dac16-76a4-4ecf-b620-4f5d4762423c/pyspark-879d2d97-5a01-41e9-b0b5-5db03ede5b18
2025-04-30T12:09:48,792 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-f32dac16-76a4-4ecf-b620-4f5d4762423c
2025-04-30T12:10:50,281 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-30T12:10:50,287 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:10:50,289 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-30T12:10:50,320 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:10:50,322 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-30T12:10:50,323 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:10:50,325 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: mod-2-pr-4-data-ingestion.py
2025-04-30T12:10:50,346 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-30T12:10:50,358 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-30T12:10:50,360 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-30T12:10:50,420 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-30T12:10:50,422 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-30T12:10:50,424 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:10:50,426 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:10:50,427 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-30T12:10:50,503 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:10:50,804 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 36509.
2025-04-30T12:10:50,838 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-30T12:10:50,878 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-30T12:10:50,900 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-30T12:10:50,902 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-30T12:10:50,908 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-30T12:10:50,934 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-b3bf7308-6cc4-49f2-a85c-b9b42711f1f1
2025-04-30T12:10:50,951 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:10:50,973 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-30T12:10:51,020 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3692ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-30T12:10:51,122 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-30T12:10:51,134 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-30T12:10:51,155 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @3827ms
2025-04-30T12:10:51,189 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@2a2301eb{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T12:10:51,189 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-30T12:10:51,217 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2f72c3c0{/,null,AVAILABLE,@Spark}
2025-04-30T12:10:51,339 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-30T12:10:51,393 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 30 ms (0 ms spent in bootstraps)
2025-04-30T12:10:51,487 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Registering app mod-2-pr-4-data-ingestion.py
2025-04-30T12:10:51,491 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Registered app mod-2-pr-4-data-ingestion.py with ID app-20250430121051-0010
2025-04-30T12:10:51,492 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430121051-0010 with rpId: 0
2025-04-30T12:10:51,493 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430121051-0010/0 on worker worker-20250429114342-172.18.0.5-38821
2025-04-30T12:10:51,495 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430121051-0010/1 on worker worker-20250429114342-172.18.0.3-34443
2025-04-30T12:10:51,497 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430121051-0010/2 on worker worker-20250429114342-172.18.0.4-36391
2025-04-30T12:10:51,501 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430121051-0010/0 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:10:51,501 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250430121051-0010
2025-04-30T12:10:51,503 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430121051-0010/1 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:10:51,507 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430121051-0010/2 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:10:51,508 [ExecutorRunner for app-20250430121051-0010/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:10:51,509 [ExecutorRunner for app-20250430121051-0010/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:10:51,509 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430121051-0010/0 on worker-20250429114342-172.18.0.5-38821 (172.18.0.5:38821) with 2 core(s)
2025-04-30T12:10:51,510 [ExecutorRunner for app-20250430121051-0010/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:10:51,510 [ExecutorRunner for app-20250430121051-0010/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:10:51,511 [ExecutorRunner for app-20250430121051-0010/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:10:51,511 [ExecutorRunner for app-20250430121051-0010/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:10:51,512 [ExecutorRunner for app-20250430121051-0010/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:10:51,512 [ExecutorRunner for app-20250430121051-0010/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:10:51,513 [ExecutorRunner for app-20250430121051-0010/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:10:51,513 [ExecutorRunner for app-20250430121051-0010/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:10:51,514 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430121051-0010/0 on hostPort 172.18.0.5:38821 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:10:51,514 [ExecutorRunner for app-20250430121051-0010/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:10:51,514 [ExecutorRunner for app-20250430121051-0010/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:10:51,516 [ExecutorRunner for app-20250430121051-0010/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:10:51,516 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430121051-0010/1 on worker-20250429114342-172.18.0.3-34443 (172.18.0.3:34443) with 2 core(s)
2025-04-30T12:10:51,519 [ExecutorRunner for app-20250430121051-0010/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:10:51,518 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40553.
2025-04-30T12:10:51,520 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430121051-0010/1 on hostPort 172.18.0.3:34443 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:10:51,521 [ExecutorRunner for app-20250430121051-0010/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:10:51,521 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on bb9210fe839d:40553
2025-04-30T12:10:51,522 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430121051-0010/2 on worker-20250429114342-172.18.0.4-36391 (172.18.0.4:36391) with 2 core(s)
2025-04-30T12:10:51,524 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430121051-0010/2 on hostPort 172.18.0.4:36391 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:10:51,526 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:10:51,539 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, bb9210fe839d, 40553, None)
2025-04-30T12:10:51,544 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager bb9210fe839d:40553 with 434.4 MiB RAM, BlockManagerId(driver, bb9210fe839d, 40553, None)
2025-04-30T12:10:51,545 [ExecutorRunner for app-20250430121051-0010/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=36509" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:36509" "--executor-id" "1" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250430121051-0010" "--worker-url" "spark://Worker@172.18.0.3:34443" "--resourceProfileId" "0"
2025-04-30T12:10:51,549 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, bb9210fe839d, 40553, None)
2025-04-30T12:10:51,551 [ExecutorRunner for app-20250430121051-0010/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=36509" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:36509" "--executor-id" "0" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250430121051-0010" "--worker-url" "spark://Worker@172.18.0.5:38821" "--resourceProfileId" "0"
2025-04-30T12:10:51,553 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, bb9210fe839d, 40553, None)
2025-04-30T12:10:51,556 [ExecutorRunner for app-20250430121051-0010/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=36509" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:36509" "--executor-id" "2" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250430121051-0010" "--worker-url" "spark://Worker@172.18.0.4:36391" "--resourceProfileId" "0"
2025-04-30T12:10:51,569 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430121051-0010 with rpId: 0
2025-04-30T12:10:51,574 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430121051-0010 with rpId: 0
2025-04-30T12:10:51,581 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430121051-0010 with rpId: 0
2025-04-30T12:10:51,611 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430121051-0010/1 is now RUNNING
2025-04-30T12:10:51,617 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430121051-0010/0 is now RUNNING
2025-04-30T12:10:51,628 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430121051-0010/2 is now RUNNING
2025-04-30T12:10:52,129 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250430121051-0010.inprogress
2025-04-30T12:10:52,340 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@2f72c3c0{/,null,STOPPED,@Spark}
2025-04-30T12:10:52,343 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7287b184{/jobs,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,346 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@efb1f4{/jobs/json,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,350 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@43ebfa73{/jobs/job,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,357 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@28e2aeeb{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,362 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3ed8bd3d{/stages,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,373 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3915b297{/stages/json,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,381 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@518a5e73{/stages/stage,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,392 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@69163bf1{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,402 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@315c40fd{/stages/pool,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,408 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7f1ed11d{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,411 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@29fdd570{/storage,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,420 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7a4a5cf0{/storage/json,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,430 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6eb60a32{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,441 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1ad7d5a1{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,452 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@55b9f894{/environment,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,460 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@56041559{/environment/json,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,473 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@43ce77c8{/executors,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,477 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1288e51d{/executors/json,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,480 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@313b917c{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,486 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@42e22e1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,493 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@36e20084{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,497 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4ed622ff{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,527 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4ddb2236{/static,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,530 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3244d86f{/,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,539 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5cbcb4b8{/api,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,543 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3092c8e3{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,545 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6dd65270{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,562 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1db22e23{/metrics/json,null,AVAILABLE,@Spark}
2025-04-30T12:10:52,565 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-30T12:10:52,973 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-04-30T12:10:52,980 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
2025-04-30T12:10:53,013 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6deea9ab{/SQL,null,AVAILABLE,@Spark}
2025-04-30T12:10:53,016 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7ab985f8{/SQL/json,null,AVAILABLE,@Spark}
2025-04-30T12:10:53,027 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2c82e3a6{/SQL/execution,null,AVAILABLE,@Spark}
2025-04-30T12:10:53,039 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@13926be8{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-04-30T12:10:53,044 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1f6d0cd{/static/sql,null,AVAILABLE,@Spark}
2025-04-30T12:10:53,815 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 515@c27de723f0d4
2025-04-30T12:10:53,816 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 598@066f03cf7e53
2025-04-30T12:10:53,836 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:10:53,838 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:10:53,838 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:10:53,839 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:10:53,840 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:10:53,841 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:10:53,882 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 583@6cca10b5c892
2025-04-30T12:10:53,903 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:10:53,905 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:10:53,906 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:10:54,763 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:10:54,773 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:10:54,772 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:10:54,948 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 234 ms to list leaf files for 1 paths.
2025-04-30T12:10:55,035 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:10:55,041 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:10:55,042 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:10:55,043 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:10:55,043 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:10:55,044 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:10:55,045 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:10:55,046 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:10:55,046 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:10:55,047 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:10:55,048 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:10:55,050 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:10:55,051 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:10:55,053 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:10:55,055 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:10:55,142 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 30 ms to list leaf files for 1 paths.
2025-04-30T12:10:55,544 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:36509 after 104 ms (0 ms spent in bootstraps)
2025-04-30T12:10:55,610 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:36509 after 123 ms (0 ms spent in bootstraps)
2025-04-30T12:10:55,624 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:36509 after 135 ms (0 ms spent in bootstraps)
2025-04-30T12:10:55,775 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:10:55,793 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:10:55,807 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:10:55,810 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:10:55,812 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:10:55,832 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:10:55,834 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:10:55,836 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:10:55,838 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:10:55,839 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:10:55,861 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:10:55,865 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:10:55,866 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:10:55,869 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:10:55,870 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:10:55,924 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:36509 after 4 ms (0 ms spent in bootstraps)
2025-04-30T12:10:55,942 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:36509 after 4 ms (0 ms spent in bootstraps)
2025-04-30T12:10:55,968 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:36509 after 4 ms (0 ms spent in bootstraps)
2025-04-30T12:10:56,078 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-8bb716ea-3eee-4da7-a70b-3a99059b4da9/executor-1539b6ef-b0f0-4bbc-ab2c-2325dc4a6c82/blockmgr-83a6063a-273f-40e3-9fa3-164b8adadbb7
2025-04-30T12:10:56,111 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-c5ae99e1-4d9a-42b7-9555-210f29b2c4bb/executor-d931427a-c3b6-4900-a133-bc0c1aeb1327/blockmgr-31a41fc0-11b2-45f2-908d-43799cec33a0
2025-04-30T12:10:56,132 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-00d4d2fa-4667-4dcc-a4a7-1a25b94c3d06/executor-ebb5fc5c-b721-4600-a0c8-a24dd9a47386/blockmgr-1fdae3d4-2692-4a1a-b254-880d6fb355bb
2025-04-30T12:10:56,151 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:10:56,203 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:10:56,220 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:10:56,537 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:36509
2025-04-30T12:10:56,538 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.4:36391
2025-04-30T12:10:56,545 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.4:36391 after 4 ms (0 ms spent in bootstraps)
2025-04-30T12:10:56,549 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.4:36391
2025-04-30T12:10:56,563 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:10:56,565 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:10:56,567 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:10:56,575 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:36509
2025-04-30T12:10:56,576 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.5:38821
2025-04-30T12:10:56,579 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.3:34443
2025-04-30T12:10:56,582 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:36509
2025-04-30T12:10:56,592 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.5:38821 after 11 ms (0 ms spent in bootstraps)
2025-04-30T12:10:56,598 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.5:38821
2025-04-30T12:10:56,600 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:10:56,606 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:10:56,606 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.3:34443 after 19 ms (0 ms spent in bootstraps)
2025-04-30T12:10:56,608 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:10:56,609 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.3:34443
2025-04-30T12:10:56,623 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:10:56,636 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:10:56,638 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:33504) with ID 2,  ResourceProfileId 0
2025-04-30T12:10:56,642 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:10:56,657 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:10:56,672 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 2 on host 172.18.0.4
2025-04-30T12:10:56,674 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:10:56,674 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:53184) with ID 0,  ResourceProfileId 0
2025-04-30T12:10:56,683 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:10:56,703 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:10:56,711 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 0 on host 172.18.0.5
2025-04-30T12:10:56,720 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:10:56,723 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:10:56,724 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:47838) with ID 1,  ResourceProfileId 0
2025-04-30T12:10:56,743 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:10:56,749 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 1 on host 172.18.0.3
2025-04-30T12:10:56,754 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:10:56,756 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:10:56,838 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45977.
2025-04-30T12:10:56,841 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.4:45977
2025-04-30T12:10:56,848 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:10:56,864 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(2, 172.18.0.4, 45977, None)
2025-04-30T12:10:56,889 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44803.
2025-04-30T12:10:56,893 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.5:44803
2025-04-30T12:10:56,897 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40147.
2025-04-30T12:10:56,898 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:10:56,899 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.3:40147
2025-04-30T12:10:56,904 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:10:56,912 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(0, 172.18.0.5, 44803, None)
2025-04-30T12:10:56,913 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.4:45977 with 434.4 MiB RAM, BlockManagerId(2, 172.18.0.4, 45977, None)
2025-04-30T12:10:56,922 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(1, 172.18.0.3, 40147, None)
2025-04-30T12:10:56,943 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.3:40147 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.3, 40147, None)
2025-04-30T12:10:56,945 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(2, 172.18.0.4, 45977, None)
2025-04-30T12:10:56,949 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.5:44803 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.5, 44803, None)
2025-04-30T12:10:56,951 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(1, 172.18.0.3, 40147, None)
2025-04-30T12:10:56,956 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(2, 172.18.0.4, 45977, None)
2025-04-30T12:10:56,967 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(0, 172.18.0.5, 44803, None)
2025-04-30T12:10:56,963 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(1, 172.18.0.3, 40147, None)
2025-04-30T12:10:56,976 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(0, 172.18.0.5, 44803, None)
2025-04-30T12:10:56,976 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:10:56,994 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:10:56,994 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@47c84b10 for default.
2025-04-30T12:10:56,996 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@540144a3 for default.
2025-04-30T12:10:57,009 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:10:57,011 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@573e617c for default.
2025-04-30T12:10:58,504 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T12:10:58,506 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T12:10:58,683 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 200.2 KiB, free 434.2 MiB)
2025-04-30T12:10:58,737 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
2025-04-30T12:10:58,741 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on bb9210fe839d:40553 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T12:10:58,747 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 0 from json at <unknown>:0
2025-04-30T12:10:58,761 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T12:10:58,895 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: json at <unknown>:0
2025-04-30T12:10:58,911 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 0 (json at <unknown>:0) with 1 output partitions
2025-04-30T12:10:58,912 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 0 (json at <unknown>:0)
2025-04-30T12:10:58,913 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T12:10:58,916 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T12:10:58,921 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0), which has no missing parents
2025-04-30T12:10:59,003 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.5 KiB, free 434.2 MiB)
2025-04-30T12:10:59,006 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
2025-04-30T12:10:59,008 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on bb9210fe839d:40553 (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T12:10:59,009 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 1 from broadcast at DAGScheduler.scala:1585
2025-04-30T12:10:59,042 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T12:10:59,046 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 0.0 with 1 tasks resource profile 0
2025-04-30T12:10:59,097 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 9650 bytes) 
2025-04-30T12:10:59,133 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 0
2025-04-30T12:10:59,144 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 0.0 (TID 0)
2025-04-30T12:10:59,258 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:10:59,305 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:40553 after 2 ms (0 ms spent in bootstraps)
2025-04-30T12:10:59,371 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.4 MiB)
2025-04-30T12:10:59,380 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on 172.18.0.3:40147 (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T12:10:59,387 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 1 took 128 ms
2025-04-30T12:10:59,439 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.5 KiB, free 434.4 MiB)
2025-04-30T12:10:59,745 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl, range: 0-35354, partition values: [empty row]
2025-04-30T12:11:00,126 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 234.465486 ms
2025-04-30T12:11:00,128 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:11:00,136 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.3 MiB)
2025-04-30T12:11:00,140 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on 172.18.0.3:40147 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T12:11:00,142 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 0 took 13 ms
2025-04-30T12:11:00,217 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T12:11:00,396 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 0.0 (TID 0). 2488 bytes result sent to driver
2025-04-30T12:11:00,413 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 0.0 (TID 0) in 1332 ms on 172.18.0.3 (executor 1) (1/1)
2025-04-30T12:11:00,416 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-04-30T12:11:00,422 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 0 (json at <unknown>:0) finished in 1.484 s
2025-04-30T12:11:00,426 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T12:11:00,427 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 0: Stage finished
2025-04-30T12:11:00,431 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 finished: json at <unknown>:0, took 1.535237 s
2025-04-30T12:11:00,482 [Thread-4] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-30T12:11:00,493 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@2a2301eb{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T12:11:00,497 [Thread-4] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://bb9210fe839d:4040
2025-04-30T12:11:00,504 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-30T12:11:00,505 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-30T12:11:00,512 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:11:00,514 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:11:00,514 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:11:00,516 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250430121051-0010
2025-04-30T12:11:00,519 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250430121051-0010
2025-04-30T12:11:00,523 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430121051-0010/1
2025-04-30T12:11:00,525 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430121051-0010/0
2025-04-30T12:11:00,525 [ExecutorRunner for app-20250430121051-0010/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430121051-0010/1 interrupted
2025-04-30T12:11:00,526 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430121051-0010/2
2025-04-30T12:11:00,527 [ExecutorRunner for app-20250430121051-0010/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430121051-0010/0 interrupted
2025-04-30T12:11:00,527 [ExecutorRunner for app-20250430121051-0010/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:11:00,528 [ExecutorRunner for app-20250430121051-0010/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430121051-0010/2 interrupted
2025-04-30T12:11:00,528 [ExecutorRunner for app-20250430121051-0010/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:11:00,530 [ExecutorRunner for app-20250430121051-0010/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:11:00,535 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:11:00,540 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:11:00,541 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:11:00,543 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:11:00,551 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:11:00,555 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:11:00,557 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:11:00,559 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:11:00,562 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:11:00,567 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:11:00,570 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:11:00,582 [dispatcher-event-loop-3] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-30T12:11:00,604 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:11:00,605 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:11:00,606 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430121051-0010/0 finished with state KILLED exitStatus 143
2025-04-30T12:11:00,607 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-30T12:11:00,608 [dispatcher-event-loop-2] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430121051-0010/0
2025-04-30T12:11:00,608 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430121051-0010, execId=0)
2025-04-30T12:11:00,609 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430121051-0010
2025-04-30T12:11:00,609 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430121051-0010 removed, cleanupLocalDirs = true
2025-04-30T12:11:00,614 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430121051-0010/1 finished with state KILLED exitStatus 143
2025-04-30T12:11:00,614 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-30T12:11:00,614 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-30T12:11:00,614 [dispatcher-event-loop-3] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430121051-0010/1
2025-04-30T12:11:00,616 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430121051-0010, execId=1)
2025-04-30T12:11:00,617 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430121051-0010
2025-04-30T12:11:00,617 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430121051-0010 removed, cleanupLocalDirs = true
2025-04-30T12:11:00,618 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-30T12:11:00,624 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:38238 got disassociated, removing it.
2025-04-30T12:11:00,625 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.master.Master [] - bb9210fe839d:36509 got disassociated, removing it.
2025-04-30T12:11:00,634 [Thread-4] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-30T12:11:00,701 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:11:00,703 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-9b44f5ce-84fd-4cab-a2f1-3381ed0111e3
2025-04-30T12:11:00,707 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-6546f096-d1f8-4b73-b9c6-88d7ddb520ac
2025-04-30T12:11:00,711 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-6546f096-d1f8-4b73-b9c6-88d7ddb520ac/pyspark-d4fc9bd4-bef4-462b-9b9c-f9f96351c882
2025-04-30T12:11:00,914 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430121051-0010/2 finished with state KILLED exitStatus 143
2025-04-30T12:11:00,915 [dispatcher-event-loop-11] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-30T12:11:00,915 [dispatcher-event-loop-5] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430121051-0010/2
2025-04-30T12:11:00,915 [dispatcher-event-loop-11] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430121051-0010, execId=2)
2025-04-30T12:11:00,916 [dispatcher-event-loop-11] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430121051-0010 removed, cleanupLocalDirs = true
2025-04-30T12:11:00,916 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430121051-0010
2025-04-30T12:13:24,109 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-30T12:13:24,114 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:13:24,115 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-30T12:13:24,149 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:13:24,150 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-30T12:13:24,152 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:13:24,154 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: mod-2-pr-4-data-ingestion.py
2025-04-30T12:13:24,182 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-30T12:13:24,196 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-30T12:13:24,197 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-30T12:13:24,275 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-30T12:13:24,276 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-30T12:13:24,278 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:13:24,279 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:13:24,280 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-30T12:13:24,363 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:13:24,772 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 45747.
2025-04-30T12:13:24,817 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-30T12:13:24,862 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-30T12:13:24,882 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-30T12:13:24,884 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-30T12:13:24,891 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-30T12:13:24,918 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-beb2d0c6-27f3-46d6-b93c-f7da339ec2d4
2025-04-30T12:13:24,940 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:13:24,963 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-30T12:13:25,020 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @4150ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-30T12:13:25,169 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-30T12:13:25,189 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-30T12:13:25,215 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @4346ms
2025-04-30T12:13:25,259 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@413b8567{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T12:13:25,260 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-30T12:13:25,299 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1320cce0{/,null,AVAILABLE,@Spark}
2025-04-30T12:13:25,481 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-30T12:13:25,563 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 47 ms (0 ms spent in bootstraps)
2025-04-30T12:13:25,685 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Registering app mod-2-pr-4-data-ingestion.py
2025-04-30T12:13:25,688 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Registered app mod-2-pr-4-data-ingestion.py with ID app-20250430121325-0011
2025-04-30T12:13:25,690 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430121325-0011 with rpId: 0
2025-04-30T12:13:25,691 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430121325-0011/0 on worker worker-20250429114342-172.18.0.5-38821
2025-04-30T12:13:25,693 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430121325-0011/1 on worker worker-20250429114342-172.18.0.3-34443
2025-04-30T12:13:25,696 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430121325-0011/2 on worker worker-20250429114342-172.18.0.4-36391
2025-04-30T12:13:25,699 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430121325-0011/0 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:13:25,699 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430121325-0011/1 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:13:25,701 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430121325-0011/2 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:13:25,705 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250430121325-0011
2025-04-30T12:13:25,709 [ExecutorRunner for app-20250430121325-0011/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:13:25,710 [ExecutorRunner for app-20250430121325-0011/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:13:25,710 [ExecutorRunner for app-20250430121325-0011/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:13:25,711 [ExecutorRunner for app-20250430121325-0011/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:13:25,712 [ExecutorRunner for app-20250430121325-0011/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:13:25,713 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430121325-0011/0 on worker-20250429114342-172.18.0.5-38821 (172.18.0.5:38821) with 2 core(s)
2025-04-30T12:13:25,713 [ExecutorRunner for app-20250430121325-0011/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:13:25,713 [ExecutorRunner for app-20250430121325-0011/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:13:25,714 [ExecutorRunner for app-20250430121325-0011/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:13:25,714 [ExecutorRunner for app-20250430121325-0011/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:13:25,715 [ExecutorRunner for app-20250430121325-0011/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:13:25,716 [ExecutorRunner for app-20250430121325-0011/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:13:25,717 [ExecutorRunner for app-20250430121325-0011/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:13:25,717 [ExecutorRunner for app-20250430121325-0011/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:13:25,721 [ExecutorRunner for app-20250430121325-0011/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:13:25,721 [ExecutorRunner for app-20250430121325-0011/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:13:25,725 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430121325-0011/0 on hostPort 172.18.0.5:38821 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:13:25,729 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430121325-0011/1 on worker-20250429114342-172.18.0.3-34443 (172.18.0.3:34443) with 2 core(s)
2025-04-30T12:13:25,733 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40233.
2025-04-30T12:13:25,734 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430121325-0011/1 on hostPort 172.18.0.3:34443 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:13:25,737 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on bb9210fe839d:40233
2025-04-30T12:13:25,738 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430121325-0011/2 on worker-20250429114342-172.18.0.4-36391 (172.18.0.4:36391) with 2 core(s)
2025-04-30T12:13:25,742 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430121325-0011/2 on hostPort 172.18.0.4:36391 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:13:25,744 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:13:25,765 [ExecutorRunner for app-20250430121325-0011/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=45747" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:45747" "--executor-id" "2" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250430121325-0011" "--worker-url" "spark://Worker@172.18.0.4:36391" "--resourceProfileId" "0"
2025-04-30T12:13:25,765 [ExecutorRunner for app-20250430121325-0011/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=45747" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:45747" "--executor-id" "0" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250430121325-0011" "--worker-url" "spark://Worker@172.18.0.5:38821" "--resourceProfileId" "0"
2025-04-30T12:13:25,766 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, bb9210fe839d, 40233, None)
2025-04-30T12:13:25,768 [ExecutorRunner for app-20250430121325-0011/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=45747" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:45747" "--executor-id" "1" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250430121325-0011" "--worker-url" "spark://Worker@172.18.0.3:34443" "--resourceProfileId" "0"
2025-04-30T12:13:25,781 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager bb9210fe839d:40233 with 434.4 MiB RAM, BlockManagerId(driver, bb9210fe839d, 40233, None)
2025-04-30T12:13:25,790 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430121325-0011 with rpId: 0
2025-04-30T12:13:25,797 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, bb9210fe839d, 40233, None)
2025-04-30T12:13:25,800 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, bb9210fe839d, 40233, None)
2025-04-30T12:13:25,806 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430121325-0011 with rpId: 0
2025-04-30T12:13:25,811 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430121325-0011 with rpId: 0
2025-04-30T12:13:25,839 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430121325-0011/0 is now RUNNING
2025-04-30T12:13:25,848 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430121325-0011/1 is now RUNNING
2025-04-30T12:13:25,852 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430121325-0011/2 is now RUNNING
2025-04-30T12:13:26,459 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250430121325-0011.inprogress
2025-04-30T12:13:26,704 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@1320cce0{/,null,STOPPED,@Spark}
2025-04-30T12:13:26,710 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40857667{/jobs,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,712 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7fa04fb2{/jobs/json,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,716 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@35083451{/jobs/job,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,720 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@354b22b6{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,724 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@484d933e{/stages,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,730 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7274d8f2{/stages/json,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,736 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2dab95f3{/stages/stage,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,742 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4622b9f0{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,747 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@611e343f{/stages/pool,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,750 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@56578f8c{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,761 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@fa43f57{/storage,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,778 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2a9cdf28{/storage/json,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,784 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@41d8187c{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,792 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@251ea385{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,798 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@d678b60{/environment,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,809 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@ef17b66{/environment/json,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,812 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@58ad57bd{/executors,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,816 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1cadc900{/executors/json,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,819 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40aadd59{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,826 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3eeb6954{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,833 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1964fe85{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,837 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@52124300{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,857 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@76c527e1{/static,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,862 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@548a25dc{/,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,876 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@28825700{/api,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,879 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@611d70f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,890 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@21e7c069{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,913 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@64ddd77d{/metrics/json,null,AVAILABLE,@Spark}
2025-04-30T12:13:26,929 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-30T12:13:27,615 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-04-30T12:13:27,620 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
2025-04-30T12:13:27,665 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@739d96e{/SQL,null,AVAILABLE,@Spark}
2025-04-30T12:13:27,679 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@113354b6{/SQL/json,null,AVAILABLE,@Spark}
2025-04-30T12:13:27,693 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7bb072e9{/SQL/execution,null,AVAILABLE,@Spark}
2025-04-30T12:13:27,703 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2dd2a8f2{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-04-30T12:13:27,733 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@9867130{/static/sql,null,AVAILABLE,@Spark}
2025-04-30T12:13:28,707 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 597@c27de723f0d4
2025-04-30T12:13:28,721 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 660@6cca10b5c892
2025-04-30T12:13:28,724 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:13:28,726 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:13:28,727 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:13:28,735 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:13:28,739 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:13:28,736 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 674@066f03cf7e53
2025-04-30T12:13:28,740 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:13:28,753 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:13:28,755 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:13:28,755 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:13:29,504 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:13:29,527 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:13:29,554 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:13:29,836 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:13:29,839 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:13:29,843 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:13:29,846 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:13:29,849 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:13:29,863 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:13:29,866 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:13:29,871 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:13:29,873 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:13:29,875 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:13:29,899 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:13:29,900 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:13:29,905 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:13:29,914 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:13:29,917 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:13:30,048 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 195 ms to list leaf files for 1 paths.
2025-04-30T12:13:30,448 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 201.0 KiB, free 434.2 MiB)
2025-04-30T12:13:30,614 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 434.2 MiB)
2025-04-30T12:13:30,622 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on bb9210fe839d:40233 (size: 34.7 KiB, free: 434.4 MiB)
2025-04-30T12:13:30,637 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 0 from json at <unknown>:0
2025-04-30T12:13:30,780 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:45747 after 174 ms (0 ms spent in bootstraps)
2025-04-30T12:13:30,814 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:45747 after 130 ms (0 ms spent in bootstraps)
2025-04-30T12:13:30,834 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:45747 after 136 ms (0 ms spent in bootstraps)
2025-04-30T12:13:31,038 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:13:31,040 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:13:31,044 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:13:31,045 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:13:31,047 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:13:31,059 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:13:31,061 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:13:31,062 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:13:31,062 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:13:31,063 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:13:31,072 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:13:31,076 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:13:31,078 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:13:31,080 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:13:31,083 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:13:31,184 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:45747 after 7 ms (0 ms spent in bootstraps)
2025-04-30T12:13:31,229 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:45747 after 14 ms (0 ms spent in bootstraps)
2025-04-30T12:13:31,235 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:45747 after 15 ms (0 ms spent in bootstraps)
2025-04-30T12:13:31,346 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat [] - Total input files to process : 1
2025-04-30T12:13:31,370 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat [] - Total input files to process : 1
2025-04-30T12:13:31,388 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-8bb716ea-3eee-4da7-a70b-3a99059b4da9/executor-353b3d17-1fcd-4c67-821c-fbbd5f98d20d/blockmgr-65a30d12-2a91-43a3-a498-b762727d46d5
2025-04-30T12:13:31,398 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-c5ae99e1-4d9a-42b7-9555-210f29b2c4bb/executor-41ab4f5e-5100-4103-9e98-0a952b5ad044/blockmgr-85731fae-9c77-4ca4-8d4f-49343af80f80
2025-04-30T12:13:31,433 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: json at <unknown>:0
2025-04-30T12:13:31,446 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-00d4d2fa-4667-4dcc-a4a7-1a25b94c3d06/executor-87c74356-00d1-45ea-acfa-9451f53883ad/blockmgr-688b5625-b96e-4967-8a8e-b315e728274d
2025-04-30T12:13:31,479 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:13:31,494 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 0 (json at <unknown>:0) with 1 output partitions
2025-04-30T12:13:31,496 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 0 (json at <unknown>:0)
2025-04-30T12:13:31,498 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T12:13:31,499 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:13:31,503 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T12:13:31,520 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 0 (MapPartitionsRDD[2] at json at <unknown>:0), which has no missing parents
2025-04-30T12:13:31,545 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:13:31,614 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 9.3 KiB, free 434.2 MiB)
2025-04-30T12:13:31,622 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 434.2 MiB)
2025-04-30T12:13:31,625 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on bb9210fe839d:40233 (size: 5.0 KiB, free: 434.4 MiB)
2025-04-30T12:13:31,629 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 1 from broadcast at DAGScheduler.scala:1585
2025-04-30T12:13:31,668 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at json at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T12:13:31,675 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 0.0 with 1 tasks resource profile 0
2025-04-30T12:13:31,850 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:45747
2025-04-30T12:13:31,852 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:45747
2025-04-30T12:13:31,852 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.5:38821
2025-04-30T12:13:31,853 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.4:36391
2025-04-30T12:13:31,861 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.5:38821 after 4 ms (0 ms spent in bootstraps)
2025-04-30T12:13:31,864 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.5:38821
2025-04-30T12:13:31,868 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:13:31,870 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:13:31,871 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:13:31,872 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:13:31,872 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.4:36391 after 13 ms (0 ms spent in bootstraps)
2025-04-30T12:13:31,874 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:13:31,875 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.4:36391
2025-04-30T12:13:31,879 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:13:31,896 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:45747
2025-04-30T12:13:31,900 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.3:34443
2025-04-30T12:13:31,911 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.3:34443 after 5 ms (0 ms spent in bootstraps)
2025-04-30T12:13:31,917 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:56938) with ID 2,  ResourceProfileId 0
2025-04-30T12:13:31,917 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:13:31,923 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:38708) with ID 0,  ResourceProfileId 0
2025-04-30T12:13:31,929 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:13:31,931 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:13:31,931 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:13:31,934 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:13:31,939 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 2 on host 172.18.0.4
2025-04-30T12:13:31,940 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.3:34443
2025-04-30T12:13:31,942 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:13:31,943 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:13:31,944 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 0 on host 172.18.0.5
2025-04-30T12:13:31,946 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:13:31,947 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:13:31,974 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:56974) with ID 1,  ResourceProfileId 0
2025-04-30T12:13:31,983 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:13:31,989 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 1 on host 172.18.0.3
2025-04-30T12:13:31,990 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:13:31,992 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:13:32,020 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35847.
2025-04-30T12:13:32,020 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43403.
2025-04-30T12:13:32,021 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.4:35847
2025-04-30T12:13:32,026 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.5:43403
2025-04-30T12:13:32,027 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:13:32,029 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:13:32,039 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(2, 172.18.0.4, 35847, None)
2025-04-30T12:13:32,039 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(0, 172.18.0.5, 43403, None)
2025-04-30T12:13:32,052 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.5:43403 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.5, 43403, None)
2025-04-30T12:13:32,055 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.4:35847 with 434.4 MiB RAM, BlockManagerId(2, 172.18.0.4, 35847, None)
2025-04-30T12:13:32,059 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(0, 172.18.0.5, 43403, None)
2025-04-30T12:13:32,062 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(0, 172.18.0.5, 43403, None)
2025-04-30T12:13:32,062 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(2, 172.18.0.4, 35847, None)
2025-04-30T12:13:32,064 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(2, 172.18.0.4, 35847, None)
2025-04-30T12:13:32,065 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37257.
2025-04-30T12:13:32,066 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.3:37257
2025-04-30T12:13:32,071 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:13:32,075 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:13:32,075 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:13:32,077 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@540144a3 for default.
2025-04-30T12:13:32,078 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6441b79f for default.
2025-04-30T12:13:32,079 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(1, 172.18.0.3, 37257, None)
2025-04-30T12:13:32,092 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.3:37257 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.3, 37257, None)
2025-04-30T12:13:32,099 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(1, 172.18.0.3, 37257, None)
2025-04-30T12:13:32,101 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(1, 172.18.0.3, 37257, None)
2025-04-30T12:13:32,114 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:13:32,117 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2f18e76d for default.
2025-04-30T12:13:32,161 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.4, executor 2, partition 0, PROCESS_LOCAL, 9177 bytes) 
2025-04-30T12:13:32,191 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 0
2025-04-30T12:13:32,201 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 0.0 (TID 0)
2025-04-30T12:13:32,309 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:13:32,355 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:40233 after 2 ms (0 ms spent in bootstraps)
2025-04-30T12:13:32,397 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 434.4 MiB)
2025-04-30T12:13:32,404 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on 172.18.0.4:35847 (size: 5.0 KiB, free: 434.4 MiB)
2025-04-30T12:13:32,410 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 1 took 100 ms
2025-04-30T12:13:32,493 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 9.3 KiB, free 434.4 MiB)
2025-04-30T12:13:32,657 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.rdd.BinaryFileRDD [] - Input split: Paths:/opt/bitnami/spark/storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl:0+35354
2025-04-30T12:13:32,659 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:13:32,667 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 434.4 MiB)
2025-04-30T12:13:32,671 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on 172.18.0.4:35847 (size: 34.7 KiB, free: 434.4 MiB)
2025-04-30T12:13:32,673 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 0 took 13 ms
2025-04-30T12:13:32,746 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T12:13:32,922 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 0.0 (TID 0). 2222 bytes result sent to driver
2025-04-30T12:13:32,940 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 0.0 (TID 0) in 803 ms on 172.18.0.4 (executor 2) (1/1)
2025-04-30T12:13:32,942 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-04-30T12:13:32,947 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 0 (json at <unknown>:0) finished in 1.390 s
2025-04-30T12:13:32,952 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T12:13:32,953 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 0: Stage finished
2025-04-30T12:13:32,962 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 finished: json at <unknown>:0, took 1.524688 s
2025-04-30T12:13:33,105 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_0_piece0 on bb9210fe839d:40233 in memory (size: 34.7 KiB, free: 434.4 MiB)
2025-04-30T12:13:33,117 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_0_piece0 on 172.18.0.4:35847 in memory (size: 34.7 KiB, free: 434.4 MiB)
2025-04-30T12:13:33,132 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on bb9210fe839d:40233 in memory (size: 5.0 KiB, free: 434.4 MiB)
2025-04-30T12:13:33,143 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on 172.18.0.4:35847 in memory (size: 5.0 KiB, free: 434.4 MiB)
2025-04-30T12:13:33,580 [Thread-4] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-30T12:13:33,590 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@413b8567{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T12:13:33,594 [Thread-4] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://bb9210fe839d:4040
2025-04-30T12:13:33,599 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-30T12:13:33,600 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-30T12:13:33,609 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:13:33,610 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:13:33,611 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:13:33,613 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250430121325-0011
2025-04-30T12:13:33,614 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250430121325-0011
2025-04-30T12:13:33,617 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430121325-0011/1
2025-04-30T12:13:33,618 [ExecutorRunner for app-20250430121325-0011/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430121325-0011/1 interrupted
2025-04-30T12:13:33,624 [ExecutorRunner for app-20250430121325-0011/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:13:33,624 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430121325-0011/0
2025-04-30T12:13:33,626 [ExecutorRunner for app-20250430121325-0011/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430121325-0011/0 interrupted
2025-04-30T12:13:33,623 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430121325-0011/2
2025-04-30T12:13:33,627 [ExecutorRunner for app-20250430121325-0011/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:13:33,636 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:13:33,638 [ExecutorRunner for app-20250430121325-0011/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430121325-0011/2 interrupted
2025-04-30T12:13:33,645 [ExecutorRunner for app-20250430121325-0011/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:13:33,639 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:13:33,649 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:13:33,649 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:13:33,648 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:13:33,652 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:13:33,655 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:13:33,656 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:13:33,655 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:13:33,659 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:13:33,665 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:13:33,667 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:13:33,673 [dispatcher-event-loop-2] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-30T12:13:33,693 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:13:33,694 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:13:33,698 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-30T12:13:33,701 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-30T12:13:33,706 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:37184 got disassociated, removing it.
2025-04-30T12:13:33,707 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - bb9210fe839d:45747 got disassociated, removing it.
2025-04-30T12:13:33,710 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430121325-0011/2 finished with state KILLED exitStatus 143
2025-04-30T12:13:33,711 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-30T12:13:33,711 [dispatcher-event-loop-2] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430121325-0011/2
2025-04-30T12:13:33,712 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430121325-0011, execId=2)
2025-04-30T12:13:33,713 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430121325-0011
2025-04-30T12:13:33,713 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430121325-0011 removed, cleanupLocalDirs = true
2025-04-30T12:13:33,716 [Thread-4] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-30T12:13:33,717 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430121325-0011/1 finished with state KILLED exitStatus 143
2025-04-30T12:13:33,718 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-30T12:13:33,718 [dispatcher-event-loop-3] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430121325-0011/1
2025-04-30T12:13:33,719 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430121325-0011, execId=1)
2025-04-30T12:13:33,720 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430121325-0011 removed, cleanupLocalDirs = true
2025-04-30T12:13:33,720 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430121325-0011
2025-04-30T12:13:33,997 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430121325-0011/0 finished with state KILLED exitStatus 143
2025-04-30T12:13:33,998 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-30T12:13:33,998 [dispatcher-event-loop-0] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430121325-0011/0
2025-04-30T12:13:33,998 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430121325-0011, execId=0)
2025-04-30T12:13:33,999 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430121325-0011
2025-04-30T12:13:33,999 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430121325-0011 removed, cleanupLocalDirs = true
2025-04-30T12:13:34,071 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:13:34,072 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-0d3482dd-a851-4f30-a852-ddedb429fe68
2025-04-30T12:13:34,076 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-6e029954-dfc1-44f8-93a0-1c2c338f06e1
2025-04-30T12:13:34,081 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-0d3482dd-a851-4f30-a852-ddedb429fe68/pyspark-14570719-cf07-468a-924b-61f08674c0c7
2025-04-30T12:21:43,941 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-30T12:21:43,945 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:21:43,947 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-30T12:21:43,979 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:21:43,981 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-30T12:21:43,982 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:21:43,984 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: mod-2-pr-4-data-ingestion.py
2025-04-30T12:21:44,006 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-30T12:21:44,016 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-30T12:21:44,018 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-30T12:21:44,069 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-30T12:21:44,071 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-30T12:21:44,072 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:21:44,073 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:21:44,075 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-30T12:21:44,142 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:21:44,525 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 35421.
2025-04-30T12:21:44,558 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-30T12:21:44,629 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-30T12:21:44,659 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-30T12:21:44,661 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-30T12:21:44,677 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-30T12:21:44,704 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-b85c7041-4d9c-4c94-b708-134dbc2e1a82
2025-04-30T12:21:44,726 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:21:44,748 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-30T12:21:44,795 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3833ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-30T12:21:44,905 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-30T12:21:44,921 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-30T12:21:44,944 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @3984ms
2025-04-30T12:21:44,984 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@2a2301eb{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T12:21:44,985 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-30T12:21:45,013 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2f72c3c0{/,null,AVAILABLE,@Spark}
2025-04-30T12:21:45,174 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-30T12:21:45,243 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 43 ms (0 ms spent in bootstraps)
2025-04-30T12:21:45,335 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Registering app mod-2-pr-4-data-ingestion.py
2025-04-30T12:21:45,337 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Registered app mod-2-pr-4-data-ingestion.py with ID app-20250430122145-0012
2025-04-30T12:21:45,339 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430122145-0012 with rpId: 0
2025-04-30T12:21:45,341 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430122145-0012/0 on worker worker-20250429114342-172.18.0.5-38821
2025-04-30T12:21:45,343 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430122145-0012/1 on worker worker-20250429114342-172.18.0.3-34443
2025-04-30T12:21:45,344 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430122145-0012/2 on worker worker-20250429114342-172.18.0.4-36391
2025-04-30T12:21:45,347 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250430122145-0012
2025-04-30T12:21:45,348 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430122145-0012/0 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:21:45,350 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430122145-0012/1 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:21:45,354 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430122145-0012/2 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:21:45,358 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430122145-0012/0 on worker-20250429114342-172.18.0.5-38821 (172.18.0.5:38821) with 2 core(s)
2025-04-30T12:21:45,359 [ExecutorRunner for app-20250430122145-0012/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:21:45,361 [ExecutorRunner for app-20250430122145-0012/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:21:45,361 [ExecutorRunner for app-20250430122145-0012/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:21:45,362 [ExecutorRunner for app-20250430122145-0012/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:21:45,362 [ExecutorRunner for app-20250430122145-0012/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:21:45,364 [ExecutorRunner for app-20250430122145-0012/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:21:45,365 [ExecutorRunner for app-20250430122145-0012/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:21:45,366 [ExecutorRunner for app-20250430122145-0012/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:21:45,363 [ExecutorRunner for app-20250430122145-0012/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:21:45,368 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430122145-0012/0 on hostPort 172.18.0.5:38821 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:21:45,369 [ExecutorRunner for app-20250430122145-0012/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:21:45,368 [ExecutorRunner for app-20250430122145-0012/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:21:45,369 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34881.
2025-04-30T12:21:45,370 [ExecutorRunner for app-20250430122145-0012/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:21:45,370 [ExecutorRunner for app-20250430122145-0012/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:21:45,369 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430122145-0012/1 on worker-20250429114342-172.18.0.3-34443 (172.18.0.3:34443) with 2 core(s)
2025-04-30T12:21:45,371 [ExecutorRunner for app-20250430122145-0012/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:21:45,371 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430122145-0012/1 on hostPort 172.18.0.3:34443 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:21:45,371 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on bb9210fe839d:34881
2025-04-30T12:21:45,373 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430122145-0012/2 on worker-20250429114342-172.18.0.4-36391 (172.18.0.4:36391) with 2 core(s)
2025-04-30T12:21:45,372 [ExecutorRunner for app-20250430122145-0012/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:21:45,374 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430122145-0012/2 on hostPort 172.18.0.4:36391 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:21:45,376 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:21:45,391 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, bb9210fe839d, 34881, None)
2025-04-30T12:21:45,399 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager bb9210fe839d:34881 with 434.4 MiB RAM, BlockManagerId(driver, bb9210fe839d, 34881, None)
2025-04-30T12:21:45,404 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, bb9210fe839d, 34881, None)
2025-04-30T12:21:45,406 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, bb9210fe839d, 34881, None)
2025-04-30T12:21:45,406 [ExecutorRunner for app-20250430122145-0012/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=35421" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:35421" "--executor-id" "1" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250430122145-0012" "--worker-url" "spark://Worker@172.18.0.3:34443" "--resourceProfileId" "0"
2025-04-30T12:21:45,411 [ExecutorRunner for app-20250430122145-0012/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=35421" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:35421" "--executor-id" "2" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250430122145-0012" "--worker-url" "spark://Worker@172.18.0.4:36391" "--resourceProfileId" "0"
2025-04-30T12:21:45,417 [ExecutorRunner for app-20250430122145-0012/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=35421" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:35421" "--executor-id" "0" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250430122145-0012" "--worker-url" "spark://Worker@172.18.0.5:38821" "--resourceProfileId" "0"
2025-04-30T12:21:45,422 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430122145-0012 with rpId: 0
2025-04-30T12:21:45,431 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430122145-0012 with rpId: 0
2025-04-30T12:21:45,438 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430122145-0012 with rpId: 0
2025-04-30T12:21:45,466 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430122145-0012/1 is now RUNNING
2025-04-30T12:21:45,471 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430122145-0012/2 is now RUNNING
2025-04-30T12:21:45,475 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430122145-0012/0 is now RUNNING
2025-04-30T12:21:45,878 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250430122145-0012.inprogress
2025-04-30T12:21:46,137 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@2f72c3c0{/,null,STOPPED,@Spark}
2025-04-30T12:21:46,141 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7287b184{/jobs,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,143 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@efb1f4{/jobs/json,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,149 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@43ebfa73{/jobs/job,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,153 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@28e2aeeb{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,156 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3ed8bd3d{/stages,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,159 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3915b297{/stages/json,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,167 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@518a5e73{/stages/stage,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,171 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@69163bf1{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,174 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@315c40fd{/stages/pool,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,185 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7f1ed11d{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,188 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@29fdd570{/storage,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,191 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7a4a5cf0{/storage/json,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,194 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6eb60a32{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,200 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1ad7d5a1{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,204 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@55b9f894{/environment,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,207 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@56041559{/environment/json,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,211 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@43ce77c8{/executors,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,223 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1288e51d{/executors/json,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,227 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@313b917c{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,231 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@42e22e1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,240 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@36e20084{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,244 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4ed622ff{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,274 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4ddb2236{/static,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,277 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3244d86f{/,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,283 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5cbcb4b8{/api,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,291 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3092c8e3{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,297 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6dd65270{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,309 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1db22e23{/metrics/json,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,317 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-30T12:21:46,835 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-04-30T12:21:46,840 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
2025-04-30T12:21:46,879 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6deea9ab{/SQL,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,889 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7ab985f8{/SQL/json,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,894 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2c82e3a6{/SQL/execution,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,905 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@13926be8{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-04-30T12:21:46,916 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1f6d0cd{/static/sql,null,AVAILABLE,@Spark}
2025-04-30T12:21:48,207 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 683@c27de723f0d4
2025-04-30T12:21:48,237 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:21:48,240 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:21:48,241 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:21:48,299 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 759@066f03cf7e53
2025-04-30T12:21:48,305 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 747@6cca10b5c892
2025-04-30T12:21:48,321 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:21:48,323 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:21:48,323 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:21:48,324 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:21:48,326 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:21:48,326 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:21:48,995 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:21:49,071 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:21:49,153 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:21:49,246 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 131 ms to list leaf files for 1 paths.
2025-04-30T12:21:49,252 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:21:49,254 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:21:49,256 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:21:49,259 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:21:49,274 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:21:49,349 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:21:49,350 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:21:49,353 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:21:49,359 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:21:49,367 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:21:49,381 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:21:49,384 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:21:49,388 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:21:49,394 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:21:49,397 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:21:50,005 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:35421 after 122 ms (0 ms spent in bootstraps)
2025-04-30T12:21:50,019 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:35421 after 103 ms (0 ms spent in bootstraps)
2025-04-30T12:21:50,058 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:35421 after 132 ms (0 ms spent in bootstraps)
2025-04-30T12:21:50,291 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:21:50,291 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:21:50,293 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:21:50,293 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:21:50,298 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:21:50,298 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:21:50,299 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:21:50,300 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:21:50,301 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:21:50,302 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:21:50,313 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:21:50,315 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:21:50,320 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:21:50,321 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:21:50,322 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:21:50,400 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:35421 after 6 ms (0 ms spent in bootstraps)
2025-04-30T12:21:50,414 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:35421 after 5 ms (0 ms spent in bootstraps)
2025-04-30T12:21:50,417 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:35421 after 5 ms (0 ms spent in bootstraps)
2025-04-30T12:21:50,545 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-8bb716ea-3eee-4da7-a70b-3a99059b4da9/executor-6c443e05-60cc-46a4-8ac8-d68019fddcf6/blockmgr-3e7c635d-d803-434b-a69c-e6c0c30af6d3
2025-04-30T12:21:50,544 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-00d4d2fa-4667-4dcc-a4a7-1a25b94c3d06/executor-67488581-6dc6-4f49-b458-0919069a091a/blockmgr-b0cb3ac2-9f23-485a-9619-c77b9f23216c
2025-04-30T12:21:50,604 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-c5ae99e1-4d9a-42b7-9555-210f29b2c4bb/executor-0965b861-1734-41b1-800b-5c0af399263e/blockmgr-867cb75a-6a30-4afd-b392-01c668d5cdb9
2025-04-30T12:21:50,640 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:21:50,672 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:21:50,706 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:21:51,055 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:35421
2025-04-30T12:21:51,056 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.4:36391
2025-04-30T12:21:51,063 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.4:36391 after 3 ms (0 ms spent in bootstraps)
2025-04-30T12:21:51,070 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.4:36391
2025-04-30T12:21:51,074 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:35421
2025-04-30T12:21:51,074 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:21:51,075 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:21:51,076 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.3:34443
2025-04-30T12:21:51,077 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:21:51,086 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.3:34443 after 6 ms (0 ms spent in bootstraps)
2025-04-30T12:21:51,092 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.3:34443
2025-04-30T12:21:51,096 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:21:51,098 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:21:51,100 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:21:51,104 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.5:38821
2025-04-30T12:21:51,108 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:35421
2025-04-30T12:21:51,117 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.5:38821 after 6 ms (0 ms spent in bootstraps)
2025-04-30T12:21:51,132 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.5:38821
2025-04-30T12:21:51,133 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:21:51,135 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:21:51,136 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:21:51,145 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:38070) with ID 2,  ResourceProfileId 0
2025-04-30T12:21:51,182 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:48738) with ID 1,  ResourceProfileId 0
2025-04-30T12:21:51,186 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:51680) with ID 0,  ResourceProfileId 0
2025-04-30T12:21:51,193 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:21:51,198 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:21:51,198 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:21:51,204 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 1 on host 172.18.0.3
2025-04-30T12:21:51,205 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 0 on host 172.18.0.5
2025-04-30T12:21:51,207 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:21:51,207 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:21:51,208 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 2 on host 172.18.0.4
2025-04-30T12:21:51,210 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:21:51,211 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:21:51,214 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:21:51,210 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:21:51,300 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40153.
2025-04-30T12:21:51,301 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.3:40153
2025-04-30T12:21:51,302 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44829.
2025-04-30T12:21:51,303 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.5:44829
2025-04-30T12:21:51,304 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:21:51,306 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:21:51,314 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(1, 172.18.0.3, 40153, None)
2025-04-30T12:21:51,314 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32803.
2025-04-30T12:21:51,315 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(0, 172.18.0.5, 44829, None)
2025-04-30T12:21:51,315 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.4:32803
2025-04-30T12:21:51,319 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:21:51,331 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.3:40153 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.3, 40153, None)
2025-04-30T12:21:51,333 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(2, 172.18.0.4, 32803, None)
2025-04-30T12:21:51,333 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.5:44829 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.5, 44829, None)
2025-04-30T12:21:51,336 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(1, 172.18.0.3, 40153, None)
2025-04-30T12:21:51,338 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(0, 172.18.0.5, 44829, None)
2025-04-30T12:21:51,338 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(1, 172.18.0.3, 40153, None)
2025-04-30T12:21:51,340 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(0, 172.18.0.5, 44829, None)
2025-04-30T12:21:51,350 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:21:51,354 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.4:32803 with 434.4 MiB RAM, BlockManagerId(2, 172.18.0.4, 32803, None)
2025-04-30T12:21:51,354 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@25c7034b for default.
2025-04-30T12:21:51,356 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:21:51,361 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(2, 172.18.0.4, 32803, None)
2025-04-30T12:21:51,361 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@38b07f3b for default.
2025-04-30T12:21:51,364 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(2, 172.18.0.4, 32803, None)
2025-04-30T12:21:51,380 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:21:51,383 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6441b79f for default.
2025-04-30T12:21:51,640 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T12:21:51,642 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T12:21:52,208 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 252.117168 ms
2025-04-30T12:21:52,281 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 200.1 KiB, free 434.2 MiB)
2025-04-30T12:21:52,360 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.2 MiB)
2025-04-30T12:21:52,365 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on bb9210fe839d:34881 (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T12:21:52,370 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 0 from showString at <unknown>:0
2025-04-30T12:21:52,400 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T12:21:52,527 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: showString at <unknown>:0
2025-04-30T12:21:52,545 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 0 (showString at <unknown>:0) with 1 output partitions
2025-04-30T12:21:52,547 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 0 (showString at <unknown>:0)
2025-04-30T12:21:52,548 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T12:21:52,553 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T12:21:52,559 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 0 (MapPartitionsRDD[3] at showString at <unknown>:0), which has no missing parents
2025-04-30T12:21:52,645 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 20.6 KiB, free 434.2 MiB)
2025-04-30T12:21:52,656 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 434.1 MiB)
2025-04-30T12:21:52,658 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on bb9210fe839d:34881 (size: 8.5 KiB, free: 434.4 MiB)
2025-04-30T12:21:52,661 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 1 from broadcast at DAGScheduler.scala:1585
2025-04-30T12:21:52,681 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T12:21:52,683 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 0.0 with 1 tasks resource profile 0
2025-04-30T12:21:52,722 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 9650 bytes) 
2025-04-30T12:21:52,752 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 0
2025-04-30T12:21:52,765 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 0.0 (TID 0)
2025-04-30T12:21:52,898 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:21:52,954 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:34881 after 3 ms (0 ms spent in bootstraps)
2025-04-30T12:21:53,006 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 434.4 MiB)
2025-04-30T12:21:53,014 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on 172.18.0.3:40153 (size: 8.5 KiB, free: 434.4 MiB)
2025-04-30T12:21:53,022 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 1 took 123 ms
2025-04-30T12:21:53,105 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 20.6 KiB, free 434.4 MiB)
2025-04-30T12:21:53,794 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 264.263221 ms
2025-04-30T12:21:53,799 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl, range: 0-35354, partition values: [empty row]
2025-04-30T12:21:53,950 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 43.972105 ms
2025-04-30T12:21:53,971 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:21:53,982 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.3 MiB)
2025-04-30T12:21:53,986 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on 172.18.0.3:40153 (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T12:21:53,988 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 0 took 16 ms
2025-04-30T12:21:54,072 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T12:21:54,255 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 0.0 (TID 0). 2704 bytes result sent to driver
2025-04-30T12:21:54,271 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 0.0 (TID 0) in 1565 ms on 172.18.0.3 (executor 1) (1/1)
2025-04-30T12:21:54,274 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-04-30T12:21:54,280 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 0 (showString at <unknown>:0) finished in 1.706 s
2025-04-30T12:21:54,286 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T12:21:54,287 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 0: Stage finished
2025-04-30T12:21:54,291 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 finished: showString at <unknown>:0, took 1.763264 s
2025-04-30T12:21:54,660 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on bb9210fe839d:34881 in memory (size: 8.5 KiB, free: 434.4 MiB)
2025-04-30T12:21:54,662 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on 172.18.0.3:40153 in memory (size: 8.5 KiB, free: 434.4 MiB)
2025-04-30T12:21:55,354 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 39.479469 ms
2025-04-30T12:21:55,372 [Thread-4] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-30T12:21:55,384 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@2a2301eb{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T12:21:55,389 [Thread-4] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://bb9210fe839d:4040
2025-04-30T12:21:55,394 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-30T12:21:55,397 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-30T12:21:55,403 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:21:55,404 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:21:55,405 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:21:55,407 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250430122145-0012
2025-04-30T12:21:55,409 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250430122145-0012
2025-04-30T12:21:55,414 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430122145-0012/0
2025-04-30T12:21:55,417 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430122145-0012/1
2025-04-30T12:21:55,417 [ExecutorRunner for app-20250430122145-0012/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430122145-0012/0 interrupted
2025-04-30T12:21:55,418 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430122145-0012/2
2025-04-30T12:21:55,421 [ExecutorRunner for app-20250430122145-0012/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430122145-0012/1 interrupted
2025-04-30T12:21:55,421 [ExecutorRunner for app-20250430122145-0012/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:21:55,424 [ExecutorRunner for app-20250430122145-0012/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430122145-0012/2 interrupted
2025-04-30T12:21:55,425 [ExecutorRunner for app-20250430122145-0012/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:21:55,427 [ExecutorRunner for app-20250430122145-0012/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:21:55,433 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:21:55,438 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:21:55,437 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:21:55,446 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:21:55,446 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:21:55,449 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:21:55,456 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:21:55,456 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:21:55,454 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:21:55,458 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:21:55,459 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:21:55,460 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:21:55,479 [dispatcher-event-loop-8] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-30T12:21:55,494 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430122145-0012/0 finished with state KILLED exitStatus 143
2025-04-30T12:21:55,495 [dispatcher-event-loop-9] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430122145-0012/0
2025-04-30T12:21:55,497 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:21:55,497 [dispatcher-event-loop-6] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-30T12:21:55,498 [dispatcher-event-loop-6] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430122145-0012, execId=0)
2025-04-30T12:21:55,499 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:21:55,499 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430122145-0012
2025-04-30T12:21:55,499 [dispatcher-event-loop-6] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430122145-0012 removed, cleanupLocalDirs = true
2025-04-30T12:21:55,505 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-30T12:21:55,511 [dispatcher-event-loop-11] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-30T12:21:55,523 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:52624 got disassociated, removing it.
2025-04-30T12:21:55,525 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430122145-0012/2 finished with state KILLED exitStatus 143
2025-04-30T12:21:55,526 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-30T12:21:55,526 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430122145-0012/1 finished with state KILLED exitStatus 143
2025-04-30T12:21:55,527 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430122145-0012, execId=2)
2025-04-30T12:21:55,528 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-30T12:21:55,528 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430122145-0012
2025-04-30T12:21:55,529 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430122145-0012, execId=1)
2025-04-30T12:21:55,529 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430122145-0012 removed, cleanupLocalDirs = true
2025-04-30T12:21:55,531 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430122145-0012
2025-04-30T12:21:55,531 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430122145-0012 removed, cleanupLocalDirs = true
2025-04-30T12:21:55,539 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.master.Master [] - bb9210fe839d:35421 got disassociated, removing it.
2025-04-30T12:21:55,540 [dispatcher-event-loop-4] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430122145-0012/2
2025-04-30T12:21:55,541 [dispatcher-event-loop-4] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430122145-0012/1
2025-04-30T12:21:55,560 [Thread-4] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-30T12:21:55,956 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:21:55,957 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-28c307fa-c2b8-4553-8f5f-91fbf6b3eba5/pyspark-bf1937bd-5c29-4092-bf4b-5b2dcccfb12b
2025-04-30T12:21:55,963 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-68db9165-cc6f-4ab6-b2c4-19a2f1189d0e
2025-04-30T12:21:55,968 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-28c307fa-c2b8-4553-8f5f-91fbf6b3eba5
2025-04-30T12:23:10,517 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-30T12:23:10,522 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:23:10,523 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-30T12:23:10,557 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:23:10,558 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-30T12:23:10,560 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:23:10,561 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: mod-2-pr-4-data-ingestion.py
2025-04-30T12:23:10,584 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-30T12:23:10,595 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-30T12:23:10,596 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-30T12:23:10,647 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-30T12:23:10,649 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-30T12:23:10,651 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:23:10,653 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:23:10,654 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-30T12:23:10,726 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:23:11,053 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 37435.
2025-04-30T12:23:11,088 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-30T12:23:11,136 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-30T12:23:11,157 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-30T12:23:11,160 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-30T12:23:11,167 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-30T12:23:11,188 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-2e6b5f17-0766-4598-80c1-333d4e554560
2025-04-30T12:23:11,206 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:23:11,227 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-30T12:23:11,275 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3766ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-30T12:23:11,375 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-30T12:23:11,386 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-30T12:23:11,407 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @3899ms
2025-04-30T12:23:11,442 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@413b8567{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T12:23:11,443 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-30T12:23:11,471 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1320cce0{/,null,AVAILABLE,@Spark}
2025-04-30T12:23:11,590 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-30T12:23:11,641 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 30 ms (0 ms spent in bootstraps)
2025-04-30T12:23:11,737 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Registering app mod-2-pr-4-data-ingestion.py
2025-04-30T12:23:11,739 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Registered app mod-2-pr-4-data-ingestion.py with ID app-20250430122311-0013
2025-04-30T12:23:11,740 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430122311-0013 with rpId: 0
2025-04-30T12:23:11,742 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430122311-0013/0 on worker worker-20250429114342-172.18.0.5-38821
2025-04-30T12:23:11,743 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430122311-0013/1 on worker worker-20250429114342-172.18.0.3-34443
2025-04-30T12:23:11,747 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430122311-0013/2 on worker worker-20250429114342-172.18.0.4-36391
2025-04-30T12:23:11,747 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430122311-0013/0 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:23:11,748 [dispatcher-event-loop-10] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250430122311-0013
2025-04-30T12:23:11,751 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430122311-0013/1 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:23:11,752 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430122311-0013/2 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:23:11,755 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430122311-0013/0 on worker-20250429114342-172.18.0.5-38821 (172.18.0.5:38821) with 2 core(s)
2025-04-30T12:23:11,755 [ExecutorRunner for app-20250430122311-0013/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:23:11,760 [ExecutorRunner for app-20250430122311-0013/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:23:11,760 [ExecutorRunner for app-20250430122311-0013/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:23:11,761 [ExecutorRunner for app-20250430122311-0013/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:23:11,761 [ExecutorRunner for app-20250430122311-0013/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:23:11,762 [ExecutorRunner for app-20250430122311-0013/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:23:11,762 [ExecutorRunner for app-20250430122311-0013/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:23:11,762 [ExecutorRunner for app-20250430122311-0013/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:23:11,763 [ExecutorRunner for app-20250430122311-0013/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:23:11,764 [ExecutorRunner for app-20250430122311-0013/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:23:11,764 [ExecutorRunner for app-20250430122311-0013/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:23:11,764 [ExecutorRunner for app-20250430122311-0013/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:23:11,765 [ExecutorRunner for app-20250430122311-0013/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:23:11,766 [ExecutorRunner for app-20250430122311-0013/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:23:11,766 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430122311-0013/0 on hostPort 172.18.0.5:38821 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:23:11,767 [ExecutorRunner for app-20250430122311-0013/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:23:11,770 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430122311-0013/1 on worker-20250429114342-172.18.0.3-34443 (172.18.0.3:34443) with 2 core(s)
2025-04-30T12:23:11,771 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430122311-0013/1 on hostPort 172.18.0.3:34443 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:23:11,772 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43883.
2025-04-30T12:23:11,772 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430122311-0013/2 on worker-20250429114342-172.18.0.4-36391 (172.18.0.4:36391) with 2 core(s)
2025-04-30T12:23:11,773 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on bb9210fe839d:43883
2025-04-30T12:23:11,773 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430122311-0013/2 on hostPort 172.18.0.4:36391 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:23:11,776 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:23:11,785 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, bb9210fe839d, 43883, None)
2025-04-30T12:23:11,795 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager bb9210fe839d:43883 with 434.4 MiB RAM, BlockManagerId(driver, bb9210fe839d, 43883, None)
2025-04-30T12:23:11,800 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, bb9210fe839d, 43883, None)
2025-04-30T12:23:11,801 [ExecutorRunner for app-20250430122311-0013/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=37435" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:37435" "--executor-id" "0" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250430122311-0013" "--worker-url" "spark://Worker@172.18.0.5:38821" "--resourceProfileId" "0"
2025-04-30T12:23:11,801 [ExecutorRunner for app-20250430122311-0013/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=37435" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:37435" "--executor-id" "2" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250430122311-0013" "--worker-url" "spark://Worker@172.18.0.4:36391" "--resourceProfileId" "0"
2025-04-30T12:23:11,803 [ExecutorRunner for app-20250430122311-0013/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=37435" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:37435" "--executor-id" "1" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250430122311-0013" "--worker-url" "spark://Worker@172.18.0.3:34443" "--resourceProfileId" "0"
2025-04-30T12:23:11,804 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, bb9210fe839d, 43883, None)
2025-04-30T12:23:11,816 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430122311-0013 with rpId: 0
2025-04-30T12:23:11,819 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430122311-0013 with rpId: 0
2025-04-30T12:23:11,822 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430122311-0013 with rpId: 0
2025-04-30T12:23:11,849 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430122311-0013/0 is now RUNNING
2025-04-30T12:23:11,854 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430122311-0013/2 is now RUNNING
2025-04-30T12:23:11,859 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430122311-0013/1 is now RUNNING
2025-04-30T12:23:12,341 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250430122311-0013.inprogress
2025-04-30T12:23:12,601 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@1320cce0{/,null,STOPPED,@Spark}
2025-04-30T12:23:12,605 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40857667{/jobs,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,610 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7fa04fb2{/jobs/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,615 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@35083451{/jobs/job,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,620 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@354b22b6{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,623 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@484d933e{/stages,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,626 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7274d8f2{/stages/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,631 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2dab95f3{/stages/stage,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,642 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4622b9f0{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,646 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@611e343f{/stages/pool,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,649 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@56578f8c{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,653 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@fa43f57{/storage,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,660 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2a9cdf28{/storage/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,666 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@41d8187c{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,671 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@251ea385{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,674 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@d678b60{/environment,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,677 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@ef17b66{/environment/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,688 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@58ad57bd{/executors,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,691 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1cadc900{/executors/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,694 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40aadd59{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,705 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3eeb6954{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,713 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1964fe85{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,716 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@52124300{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,745 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@76c527e1{/static,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,748 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@548a25dc{/,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,753 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@28825700{/api,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,756 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@611d70f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,762 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@21e7c069{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,779 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@64ddd77d{/metrics/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:12,785 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-30T12:23:13,377 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-04-30T12:23:13,388 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
2025-04-30T12:23:13,442 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@739d96e{/SQL,null,AVAILABLE,@Spark}
2025-04-30T12:23:13,475 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@113354b6{/SQL/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:13,479 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7bb072e9{/SQL/execution,null,AVAILABLE,@Spark}
2025-04-30T12:23:13,484 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2dd2a8f2{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:13,493 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@9867130{/static/sql,null,AVAILABLE,@Spark}
2025-04-30T12:23:14,634 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 842@066f03cf7e53
2025-04-30T12:23:14,653 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:23:14,655 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:23:14,656 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:23:14,661 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 768@c27de723f0d4
2025-04-30T12:23:14,677 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:23:14,679 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:23:14,679 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:23:14,830 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 830@6cca10b5c892
2025-04-30T12:23:14,856 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:23:14,858 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:23:14,859 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:23:15,469 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:23:15,654 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:23:15,736 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:23:15,741 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:23:15,746 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:23:15,754 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:23:15,757 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:23:15,890 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:23:15,891 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:23:15,894 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:23:15,897 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:23:15,899 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:23:15,903 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:23:16,169 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:23:16,174 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:23:16,176 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:23:16,177 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:23:16,179 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:23:16,194 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 195 ms to list leaf files for 2 paths.
2025-04-30T12:23:16,672 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:37435 after 157 ms (0 ms spent in bootstraps)
2025-04-30T12:23:16,875 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:37435 after 186 ms (0 ms spent in bootstraps)
2025-04-30T12:23:16,975 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:23:16,985 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:23:16,987 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:23:16,989 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:23:16,991 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:23:17,048 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:37435 after 174 ms (0 ms spent in bootstraps)
2025-04-30T12:23:17,147 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:37435 after 12 ms (0 ms spent in bootstraps)
2025-04-30T12:23:17,153 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:23:17,154 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:23:17,156 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:23:17,157 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:23:17,160 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:23:17,284 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:37435 after 13 ms (0 ms spent in bootstraps)
2025-04-30T12:23:17,301 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:23:17,302 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:23:17,303 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:23:17,305 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:23:17,306 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:23:17,334 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-c5ae99e1-4d9a-42b7-9555-210f29b2c4bb/executor-664bf39e-db50-4ff5-a66b-83e80809608c/blockmgr-3a874a0d-a5a9-4bdf-a960-9714dafd553f
2025-04-30T12:23:17,411 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:23:17,421 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:37435 after 4 ms (0 ms spent in bootstraps)
2025-04-30T12:23:17,441 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-00d4d2fa-4667-4dcc-a4a7-1a25b94c3d06/executor-bf50a7e9-8330-4b80-9f47-e22d67b4d07d/blockmgr-f68222fc-295b-461b-923d-cbe822f0a2ae
2025-04-30T12:23:17,522 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:23:17,576 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-8bb716ea-3eee-4da7-a70b-3a99059b4da9/executor-fcaa392d-e5a5-45fe-a221-680ac259a4a0/blockmgr-511083f5-422a-4a8c-a763-176fb2cf44ab
2025-04-30T12:23:17,655 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:23:17,873 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:37435
2025-04-30T12:23:17,875 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.5:38821
2025-04-30T12:23:17,928 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:23:17,934 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.5:38821
2025-04-30T12:23:17,932 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.5:38821 after 32 ms (0 ms spent in bootstraps)
2025-04-30T12:23:17,934 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:23:17,954 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:23:18,019 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.3:34443
2025-04-30T12:23:18,021 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:37435
2025-04-30T12:23:18,047 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:40906) with ID 0,  ResourceProfileId 0
2025-04-30T12:23:18,060 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:23:18,063 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.3:34443 after 18 ms (0 ms spent in bootstraps)
2025-04-30T12:23:18,065 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.3:34443
2025-04-30T12:23:18,063 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:23:18,073 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:23:18,075 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:23:18,091 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 0 on host 172.18.0.5
2025-04-30T12:23:18,098 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:23:18,103 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:23:18,139 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:41774) with ID 1,  ResourceProfileId 0
2025-04-30T12:23:18,169 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:23:18,175 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 1 on host 172.18.0.3
2025-04-30T12:23:18,187 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:23:18,196 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:23:18,232 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:37435
2025-04-30T12:23:18,234 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.4:36391
2025-04-30T12:23:18,253 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:23:18,254 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:23:18,256 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:23:18,271 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44579.
2025-04-30T12:23:18,272 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.5:44579
2025-04-30T12:23:18,272 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.4:36391 after 25 ms (0 ms spent in bootstraps)
2025-04-30T12:23:18,276 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:23:18,283 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.4:36391
2025-04-30T12:23:18,315 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(0, 172.18.0.5, 44579, None)
2025-04-30T12:23:18,321 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38875.
2025-04-30T12:23:18,322 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.3:38875
2025-04-30T12:23:18,326 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:23:18,350 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(1, 172.18.0.3, 38875, None)
2025-04-30T12:23:18,364 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.5:44579 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.5, 44579, None)
2025-04-30T12:23:18,380 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(0, 172.18.0.5, 44579, None)
2025-04-30T12:23:18,385 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.3:38875 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.3, 38875, None)
2025-04-30T12:23:18,390 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(0, 172.18.0.5, 44579, None)
2025-04-30T12:23:18,400 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:35248) with ID 2,  ResourceProfileId 0
2025-04-30T12:23:18,408 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(1, 172.18.0.3, 38875, None)
2025-04-30T12:23:18,415 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:23:18,416 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:23:18,417 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(1, 172.18.0.3, 38875, None)
2025-04-30T12:23:18,422 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 2 on host 172.18.0.4
2025-04-30T12:23:18,433 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2f9a6030 for default.
2025-04-30T12:23:18,443 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:23:18,440 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:23:18,453 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:23:18,453 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@72d85a9a for default.
2025-04-30T12:23:18,548 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35741.
2025-04-30T12:23:18,554 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.4:35741
2025-04-30T12:23:18,559 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:23:18,583 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(2, 172.18.0.4, 35741, None)
2025-04-30T12:23:18,601 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.4:35741 with 434.4 MiB RAM, BlockManagerId(2, 172.18.0.4, 35741, None)
2025-04-30T12:23:18,610 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(2, 172.18.0.4, 35741, None)
2025-04-30T12:23:18,620 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(2, 172.18.0.4, 35741, None)
2025-04-30T12:23:18,638 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:23:18,640 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@617f904f for default.
2025-04-30T12:23:19,157 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T12:23:19,162 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T12:23:19,902 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 301.474286 ms
2025-04-30T12:23:19,965 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 200.1 KiB, free 434.2 MiB)
2025-04-30T12:23:20,054 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.2 MiB)
2025-04-30T12:23:20,057 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on bb9210fe839d:43883 (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T12:23:20,064 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 0 from showString at <unknown>:0
2025-04-30T12:23:20,094 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T12:23:20,243 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: showString at <unknown>:0
2025-04-30T12:23:20,264 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 0 (showString at <unknown>:0) with 1 output partitions
2025-04-30T12:23:20,265 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 0 (showString at <unknown>:0)
2025-04-30T12:23:20,266 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T12:23:20,268 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T12:23:20,274 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 0 (MapPartitionsRDD[3] at showString at <unknown>:0), which has no missing parents
2025-04-30T12:23:20,364 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 20.6 KiB, free 434.2 MiB)
2025-04-30T12:23:20,369 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 434.1 MiB)
2025-04-30T12:23:20,370 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on bb9210fe839d:43883 (size: 8.5 KiB, free: 434.4 MiB)
2025-04-30T12:23:20,372 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 1 from broadcast at DAGScheduler.scala:1585
2025-04-30T12:23:20,391 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T12:23:20,394 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 0.0 with 1 tasks resource profile 0
2025-04-30T12:23:20,438 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 9657 bytes) 
2025-04-30T12:23:20,474 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 0
2025-04-30T12:23:20,490 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 0.0 (TID 0)
2025-04-30T12:23:20,666 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:23:20,732 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:43883 after 2 ms (0 ms spent in bootstraps)
2025-04-30T12:23:20,805 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 434.4 MiB)
2025-04-30T12:23:20,817 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on 172.18.0.3:38875 (size: 8.5 KiB, free: 434.4 MiB)
2025-04-30T12:23:20,825 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 1 took 157 ms
2025-04-30T12:23:20,952 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 20.6 KiB, free 434.4 MiB)
2025-04-30T12:23:21,600 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 249.780048 ms
2025-04-30T12:23:21,604 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY%20copy.jsonl, range: 0-35354, partition values: [empty row]
2025-04-30T12:23:21,729 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 38.500985 ms
2025-04-30T12:23:21,752 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:23:21,764 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.3 MiB)
2025-04-30T12:23:21,767 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on 172.18.0.3:38875 (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T12:23:21,769 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 0 took 16 ms
2025-04-30T12:23:21,863 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T12:23:22,075 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 0.0 (TID 0). 2704 bytes result sent to driver
2025-04-30T12:23:22,096 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 0.0 (TID 0) in 1674 ms on 172.18.0.3 (executor 1) (1/1)
2025-04-30T12:23:22,098 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-04-30T12:23:22,105 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 0 (showString at <unknown>:0) finished in 1.813 s
2025-04-30T12:23:22,110 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T12:23:22,112 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 0: Stage finished
2025-04-30T12:23:22,116 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 finished: showString at <unknown>:0, took 1.872089 s
2025-04-30T12:23:22,719 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on bb9210fe839d:43883 in memory (size: 8.5 KiB, free: 434.4 MiB)
2025-04-30T12:23:22,731 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on 172.18.0.3:38875 in memory (size: 8.5 KiB, free: 434.4 MiB)
2025-04-30T12:23:23,112 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 33.036402 ms
2025-04-30T12:23:23,127 [Thread-4] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-30T12:23:23,137 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@413b8567{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T12:23:23,142 [Thread-4] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://bb9210fe839d:4040
2025-04-30T12:23:23,149 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-30T12:23:23,151 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-30T12:23:23,160 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:23:23,164 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:23:23,164 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250430122311-0013
2025-04-30T12:23:23,166 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250430122311-0013
2025-04-30T12:23:23,165 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:23:23,170 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430122311-0013/0
2025-04-30T12:23:23,172 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430122311-0013/1
2025-04-30T12:23:23,173 [ExecutorRunner for app-20250430122311-0013/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430122311-0013/1 interrupted
2025-04-30T12:23:23,174 [ExecutorRunner for app-20250430122311-0013/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:23:23,178 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430122311-0013/2
2025-04-30T12:23:23,179 [ExecutorRunner for app-20250430122311-0013/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430122311-0013/0 interrupted
2025-04-30T12:23:23,183 [ExecutorRunner for app-20250430122311-0013/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430122311-0013/2 interrupted
2025-04-30T12:23:23,183 [ExecutorRunner for app-20250430122311-0013/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:23:23,187 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:23:23,189 [ExecutorRunner for app-20250430122311-0013/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:23:23,196 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:23:23,197 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:23:23,205 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:23:23,209 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:23:23,209 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:23:23,213 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:23:23,213 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:23:23,216 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:23:23,217 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:23:23,221 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:23:23,223 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:23:23,233 [dispatcher-event-loop-6] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-30T12:23:23,251 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:23:23,254 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:23:23,258 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-30T12:23:23,264 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-30T12:23:23,266 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430122311-0013/2 finished with state KILLED exitStatus 143
2025-04-30T12:23:23,267 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430122311-0013/0 finished with state KILLED exitStatus 143
2025-04-30T12:23:23,267 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-30T12:23:23,268 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-30T12:23:23,268 [dispatcher-event-loop-6] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430122311-0013/0
2025-04-30T12:23:23,269 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430122311-0013, execId=2)
2025-04-30T12:23:23,269 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430122311-0013, execId=0)
2025-04-30T12:23:23,270 [dispatcher-event-loop-6] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430122311-0013/2
2025-04-30T12:23:23,270 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430122311-0013 removed, cleanupLocalDirs = true
2025-04-30T12:23:23,271 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:59050 got disassociated, removing it.
2025-04-30T12:23:23,270 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430122311-0013
2025-04-30T12:23:23,270 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430122311-0013 removed, cleanupLocalDirs = true
2025-04-30T12:23:23,271 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430122311-0013
2025-04-30T12:23:23,273 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - bb9210fe839d:37435 got disassociated, removing it.
2025-04-30T12:23:23,282 [Thread-4] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-30T12:23:23,459 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:23:23,461 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-b1937c18-8049-47e2-bfbc-1d22df72f260
2025-04-30T12:23:23,466 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-b1937c18-8049-47e2-bfbc-1d22df72f260/pyspark-bc61c986-1075-48f7-8c58-3321bcda6082
2025-04-30T12:23:23,470 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-724c4f41-e7da-446e-abbe-19b1d8fef763
2025-04-30T12:23:23,554 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430122311-0013/1 finished with state KILLED exitStatus 143
2025-04-30T12:23:23,555 [dispatcher-event-loop-8] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-30T12:23:23,555 [dispatcher-event-loop-9] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430122311-0013/1
2025-04-30T12:23:23,556 [dispatcher-event-loop-8] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430122311-0013, execId=1)
2025-04-30T12:23:23,556 [dispatcher-event-loop-8] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430122311-0013 removed, cleanupLocalDirs = true
2025-04-30T12:23:23,556 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430122311-0013
2025-04-30T12:23:54,400 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-30T12:23:54,405 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:23:54,406 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-30T12:23:54,440 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:23:54,441 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-30T12:23:54,442 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:23:54,443 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: mod-2-pr-4-data-ingestion.py
2025-04-30T12:23:54,472 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-30T12:23:54,482 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-30T12:23:54,484 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-30T12:23:54,543 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-30T12:23:54,544 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-30T12:23:54,546 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:23:54,548 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:23:54,550 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-30T12:23:54,637 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:23:54,946 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 42779.
2025-04-30T12:23:54,982 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-30T12:23:55,019 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-30T12:23:55,044 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-30T12:23:55,045 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-30T12:23:55,052 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-30T12:23:55,076 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-60baf5ef-b99b-49f8-a0ba-42b4202ced7a
2025-04-30T12:23:55,094 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:23:55,113 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-30T12:23:55,162 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3519ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-30T12:23:55,262 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-30T12:23:55,275 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-30T12:23:55,296 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @3653ms
2025-04-30T12:23:55,330 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@413b8567{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T12:23:55,332 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-30T12:23:55,358 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1320cce0{/,null,AVAILABLE,@Spark}
2025-04-30T12:23:55,481 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-30T12:23:55,539 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 32 ms (0 ms spent in bootstraps)
2025-04-30T12:23:55,629 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Registering app mod-2-pr-4-data-ingestion.py
2025-04-30T12:23:55,631 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Registered app mod-2-pr-4-data-ingestion.py with ID app-20250430122355-0014
2025-04-30T12:23:55,632 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430122355-0014 with rpId: 0
2025-04-30T12:23:55,634 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430122355-0014/0 on worker worker-20250429114342-172.18.0.5-38821
2025-04-30T12:23:55,635 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430122355-0014/1 on worker worker-20250429114342-172.18.0.3-34443
2025-04-30T12:23:55,637 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430122355-0014/2 on worker worker-20250429114342-172.18.0.4-36391
2025-04-30T12:23:55,638 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430122355-0014/0 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:23:55,640 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430122355-0014/1 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:23:55,640 [dispatcher-event-loop-10] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250430122355-0014
2025-04-30T12:23:55,643 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430122355-0014/2 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:23:55,646 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430122355-0014/0 on worker-20250429114342-172.18.0.5-38821 (172.18.0.5:38821) with 2 core(s)
2025-04-30T12:23:55,650 [ExecutorRunner for app-20250430122355-0014/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:23:55,650 [ExecutorRunner for app-20250430122355-0014/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:23:55,651 [ExecutorRunner for app-20250430122355-0014/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:23:55,651 [ExecutorRunner for app-20250430122355-0014/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:23:55,651 [ExecutorRunner for app-20250430122355-0014/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:23:55,652 [ExecutorRunner for app-20250430122355-0014/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:23:55,652 [ExecutorRunner for app-20250430122355-0014/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:23:55,652 [ExecutorRunner for app-20250430122355-0014/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:23:55,654 [ExecutorRunner for app-20250430122355-0014/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:23:55,654 [ExecutorRunner for app-20250430122355-0014/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:23:55,654 [ExecutorRunner for app-20250430122355-0014/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:23:55,654 [dispatcher-event-loop-10] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430122355-0014/0 on hostPort 172.18.0.5:38821 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:23:55,655 [ExecutorRunner for app-20250430122355-0014/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:23:55,655 [ExecutorRunner for app-20250430122355-0014/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:23:55,655 [ExecutorRunner for app-20250430122355-0014/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:23:55,656 [ExecutorRunner for app-20250430122355-0014/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:23:55,656 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430122355-0014/1 on worker-20250429114342-172.18.0.3-34443 (172.18.0.3:34443) with 2 core(s)
2025-04-30T12:23:55,658 [dispatcher-event-loop-10] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430122355-0014/1 on hostPort 172.18.0.3:34443 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:23:55,659 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430122355-0014/2 on worker-20250429114342-172.18.0.4-36391 (172.18.0.4:36391) with 2 core(s)
2025-04-30T12:23:55,661 [dispatcher-event-loop-10] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430122355-0014/2 on hostPort 172.18.0.4:36391 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:23:55,666 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38313.
2025-04-30T12:23:55,667 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on bb9210fe839d:38313
2025-04-30T12:23:55,672 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:23:55,683 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, bb9210fe839d, 38313, None)
2025-04-30T12:23:55,688 [ExecutorRunner for app-20250430122355-0014/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=42779" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:42779" "--executor-id" "1" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250430122355-0014" "--worker-url" "spark://Worker@172.18.0.3:34443" "--resourceProfileId" "0"
2025-04-30T12:23:55,690 [ExecutorRunner for app-20250430122355-0014/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=42779" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:42779" "--executor-id" "0" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250430122355-0014" "--worker-url" "spark://Worker@172.18.0.5:38821" "--resourceProfileId" "0"
2025-04-30T12:23:55,691 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager bb9210fe839d:38313 with 434.4 MiB RAM, BlockManagerId(driver, bb9210fe839d, 38313, None)
2025-04-30T12:23:55,691 [ExecutorRunner for app-20250430122355-0014/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=42779" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:42779" "--executor-id" "2" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250430122355-0014" "--worker-url" "spark://Worker@172.18.0.4:36391" "--resourceProfileId" "0"
2025-04-30T12:23:55,696 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, bb9210fe839d, 38313, None)
2025-04-30T12:23:55,703 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, bb9210fe839d, 38313, None)
2025-04-30T12:23:55,705 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430122355-0014 with rpId: 0
2025-04-30T12:23:55,711 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430122355-0014 with rpId: 0
2025-04-30T12:23:55,712 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430122355-0014 with rpId: 0
2025-04-30T12:23:55,738 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430122355-0014/1 is now RUNNING
2025-04-30T12:23:55,742 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430122355-0014/2 is now RUNNING
2025-04-30T12:23:55,747 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430122355-0014/0 is now RUNNING
2025-04-30T12:23:56,204 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250430122355-0014.inprogress
2025-04-30T12:23:56,442 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@1320cce0{/,null,STOPPED,@Spark}
2025-04-30T12:23:56,458 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40857667{/jobs,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,467 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7fa04fb2{/jobs/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,481 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@35083451{/jobs/job,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,501 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@354b22b6{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,512 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@484d933e{/stages,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,518 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7274d8f2{/stages/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,529 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2dab95f3{/stages/stage,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,564 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4622b9f0{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,618 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@611e343f{/stages/pool,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,639 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@56578f8c{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,648 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@fa43f57{/storage,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,658 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2a9cdf28{/storage/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,662 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@41d8187c{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,667 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@251ea385{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,685 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@d678b60{/environment,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,703 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@ef17b66{/environment/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,706 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@58ad57bd{/executors,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,710 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1cadc900{/executors/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,713 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40aadd59{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,720 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3eeb6954{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,724 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1964fe85{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,731 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@52124300{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,762 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@76c527e1{/static,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,767 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@548a25dc{/,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,777 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@28825700{/api,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,780 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@611d70f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,785 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@21e7c069{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,803 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@64ddd77d{/metrics/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:56,806 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-30T12:23:57,415 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-04-30T12:23:57,428 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
2025-04-30T12:23:57,491 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@739d96e{/SQL,null,AVAILABLE,@Spark}
2025-04-30T12:23:57,495 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@113354b6{/SQL/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:57,506 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7bb072e9{/SQL/execution,null,AVAILABLE,@Spark}
2025-04-30T12:23:57,511 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2dd2a8f2{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-04-30T12:23:57,521 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@9867130{/static/sql,null,AVAILABLE,@Spark}
2025-04-30T12:23:58,751 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 914@6cca10b5c892
2025-04-30T12:23:58,756 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 856@c27de723f0d4
2025-04-30T12:23:58,761 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 926@066f03cf7e53
2025-04-30T12:23:58,774 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:23:58,776 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:23:58,776 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:23:58,777 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:23:58,778 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:23:58,778 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:23:58,779 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:23:58,782 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:23:58,783 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:23:59,512 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:23:59,526 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:23:59,579 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:23:59,855 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:23:59,862 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:23:59,869 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:23:59,871 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:23:59,877 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:23:59,878 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:23:59,881 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:23:59,886 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:23:59,889 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:23:59,897 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:23:59,930 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:23:59,942 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:23:59,948 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:23:59,957 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:23:59,960 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:23:59,987 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 215 ms to list leaf files for 2 paths.
2025-04-30T12:24:00,561 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:42779 after 119 ms (0 ms spent in bootstraps)
2025-04-30T12:24:00,636 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:42779 after 165 ms (0 ms spent in bootstraps)
2025-04-30T12:24:00,691 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:42779 after 189 ms (0 ms spent in bootstraps)
2025-04-30T12:24:00,985 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:24:00,987 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:24:00,992 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:24:01,004 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:24:01,005 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:24:01,011 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:24:01,012 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:24:01,013 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:24:01,018 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:24:01,019 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:24:01,042 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:24:01,045 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:24:01,047 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:24:01,050 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:24:01,051 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:24:01,149 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:42779 after 11 ms (0 ms spent in bootstraps)
2025-04-30T12:24:01,157 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:42779 after 10 ms (0 ms spent in bootstraps)
2025-04-30T12:24:01,198 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:42779 after 4 ms (0 ms spent in bootstraps)
2025-04-30T12:24:01,326 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-8bb716ea-3eee-4da7-a70b-3a99059b4da9/executor-92575bb0-89d6-4228-9ef6-e716489f0102/blockmgr-22c319df-cbe5-4ac1-ab56-4d2c45c0db83
2025-04-30T12:24:01,326 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-c5ae99e1-4d9a-42b7-9555-210f29b2c4bb/executor-de95b366-63a0-4e44-9da1-5b67abadf0a0/blockmgr-e4120ccb-5c36-49e6-afc2-bb07e2a8d63a
2025-04-30T12:24:01,356 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-00d4d2fa-4667-4dcc-a4a7-1a25b94c3d06/executor-ea24668d-3d41-4ab7-8e86-5982562d2ad4/blockmgr-ecac9c2b-c5cb-49b0-8371-7adfa156e43d
2025-04-30T12:24:01,381 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:24:01,388 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:24:01,412 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:24:01,732 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.4:36391
2025-04-30T12:24:01,734 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:42779
2025-04-30T12:24:01,761 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.4:36391 after 17 ms (0 ms spent in bootstraps)
2025-04-30T12:24:01,763 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.5:38821
2025-04-30T12:24:01,764 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.4:36391
2025-04-30T12:24:01,762 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:42779
2025-04-30T12:24:01,773 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:24:01,778 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:24:01,792 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:24:01,792 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.5:38821 after 16 ms (0 ms spent in bootstraps)
2025-04-30T12:24:01,791 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.5:38821
2025-04-30T12:24:01,799 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:24:01,814 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:24:01,821 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:24:01,822 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:42779
2025-04-30T12:24:01,824 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.3:34443
2025-04-30T12:24:01,857 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:24:01,866 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.3:34443
2025-04-30T12:24:01,860 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:24:01,866 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.3:34443 after 20 ms (0 ms spent in bootstraps)
2025-04-30T12:24:01,881 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:24:01,897 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:50276) with ID 0,  ResourceProfileId 0
2025-04-30T12:24:01,920 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:60406) with ID 2,  ResourceProfileId 0
2025-04-30T12:24:01,953 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:24:01,957 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:24:01,980 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 2 on host 172.18.0.4
2025-04-30T12:24:01,986 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 0 on host 172.18.0.5
2025-04-30T12:24:01,988 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:24:01,990 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:24:01,990 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:24:01,997 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:59016) with ID 1,  ResourceProfileId 0
2025-04-30T12:24:01,997 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:24:02,023 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:24:02,033 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 1 on host 172.18.0.3
2025-04-30T12:24:02,042 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:24:02,057 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:24:02,133 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43171.
2025-04-30T12:24:02,135 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.5:43171
2025-04-30T12:24:02,142 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:24:02,151 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45567.
2025-04-30T12:24:02,154 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.4:45567
2025-04-30T12:24:02,160 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(0, 172.18.0.5, 43171, None)
2025-04-30T12:24:02,160 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:24:02,164 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34285.
2025-04-30T12:24:02,173 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.3:34285
2025-04-30T12:24:02,177 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(2, 172.18.0.4, 45567, None)
2025-04-30T12:24:02,178 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:24:02,195 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.5:43171 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.5, 43171, None)
2025-04-30T12:24:02,206 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(1, 172.18.0.3, 34285, None)
2025-04-30T12:24:02,210 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.4:45567 with 434.4 MiB RAM, BlockManagerId(2, 172.18.0.4, 45567, None)
2025-04-30T12:24:02,218 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(0, 172.18.0.5, 43171, None)
2025-04-30T12:24:02,225 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(2, 172.18.0.4, 45567, None)
2025-04-30T12:24:02,226 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(0, 172.18.0.5, 43171, None)
2025-04-30T12:24:02,228 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(2, 172.18.0.4, 45567, None)
2025-04-30T12:24:02,230 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.3:34285 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.3, 34285, None)
2025-04-30T12:24:02,243 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(1, 172.18.0.3, 34285, None)
2025-04-30T12:24:02,247 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:24:02,247 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(1, 172.18.0.3, 34285, None)
2025-04-30T12:24:02,252 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:24:02,252 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@51b1068 for default.
2025-04-30T12:24:02,257 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@7a6deec0 for default.
2025-04-30T12:24:02,282 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:24:02,293 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2f9a6030 for default.
2025-04-30T12:24:02,797 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T12:24:02,800 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T12:24:03,455 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 249.98047 ms
2025-04-30T12:24:03,530 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 200.1 KiB, free 434.2 MiB)
2025-04-30T12:24:03,606 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.2 MiB)
2025-04-30T12:24:03,610 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on bb9210fe839d:38313 (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T12:24:03,616 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 0 from count at <unknown>:0
2025-04-30T12:24:03,661 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T12:24:03,794 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Registering RDD 3 (count at <unknown>:0) as input to shuffle 0
2025-04-30T12:24:03,800 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got map stage job 0 (count at <unknown>:0) with 2 output partitions
2025-04-30T12:24:03,801 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ShuffleMapStage 0 (count at <unknown>:0)
2025-04-30T12:24:03,802 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T12:24:03,806 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T12:24:03,811 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at <unknown>:0), which has no missing parents
2025-04-30T12:24:03,901 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 17.0 KiB, free 434.2 MiB)
2025-04-30T12:24:03,904 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 434.1 MiB)
2025-04-30T12:24:03,905 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on bb9210fe839d:38313 (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T12:24:03,907 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 1 from broadcast at DAGScheduler.scala:1585
2025-04-30T12:24:03,929 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
2025-04-30T12:24:03,931 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 0.0 with 2 tasks resource profile 0
2025-04-30T12:24:03,963 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 9646 bytes) 
2025-04-30T12:24:03,968 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 1.0 in stage 0.0 (TID 1) (172.18.0.5, executor 0, partition 1, PROCESS_LOCAL, 9639 bytes) 
2025-04-30T12:24:03,998 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 0
2025-04-30T12:24:03,997 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 1
2025-04-30T12:24:04,011 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 0.0 (TID 0)
2025-04-30T12:24:04,013 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Running task 1.0 in stage 0.0 (TID 1)
2025-04-30T12:24:04,158 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:24:04,162 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:24:04,246 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:38313 after 3 ms (0 ms spent in bootstraps)
2025-04-30T12:24:04,250 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:38313 after 4 ms (0 ms spent in bootstraps)
2025-04-30T12:24:04,336 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 434.4 MiB)
2025-04-30T12:24:04,338 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 434.4 MiB)
2025-04-30T12:24:04,347 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on 172.18.0.3:34285 (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T12:24:04,351 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on 172.18.0.5:43171 (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T12:24:04,356 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 1 took 197 ms
2025-04-30T12:24:04,358 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 1 took 194 ms
2025-04-30T12:24:04,449 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 17.0 KiB, free 434.4 MiB)
2025-04-30T12:24:04,449 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 17.0 KiB, free 434.4 MiB)
2025-04-30T12:24:05,366 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 322.603668 ms
2025-04-30T12:24:05,377 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 325.247506 ms
2025-04-30T12:24:05,380 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl, range: 0-35354, partition values: [empty row]
2025-04-30T12:24:05,393 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY%20copy.jsonl, range: 0-35354, partition values: [empty row]
2025-04-30T12:24:05,427 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 15.276099 ms
2025-04-30T12:24:05,445 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 16.162945 ms
2025-04-30T12:24:05,452 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:24:05,463 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.3 MiB)
2025-04-30T12:24:05,468 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on 172.18.0.5:43171 (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T12:24:05,471 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 0 took 18 ms
2025-04-30T12:24:05,475 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:24:05,488 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.3 MiB)
2025-04-30T12:24:05,492 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on 172.18.0.3:34285 (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T12:24:05,495 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 0 took 19 ms
2025-04-30T12:24:05,574 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T12:24:05,591 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T12:24:05,818 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Finished task 1.0 in stage 0.0 (TID 1). 2007 bytes result sent to driver
2025-04-30T12:24:05,824 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 0.0 (TID 0). 2007 bytes result sent to driver
2025-04-30T12:24:05,837 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 1.0 in stage 0.0 (TID 1) in 1869 ms on 172.18.0.5 (executor 0) (1/2)
2025-04-30T12:24:05,839 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 0.0 (TID 0) in 1889 ms on 172.18.0.3 (executor 1) (2/2)
2025-04-30T12:24:05,840 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-04-30T12:24:05,848 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ShuffleMapStage 0 (count at <unknown>:0) finished in 2.020 s
2025-04-30T12:24:05,849 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - looking for newly runnable stages
2025-04-30T12:24:05,851 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - running: Set()
2025-04-30T12:24:05,852 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - waiting: Set()
2025-04-30T12:24:05,854 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - failed: Set()
2025-04-30T12:24:05,910 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 16.662971 ms
2025-04-30T12:24:05,961 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: count at <unknown>:0
2025-04-30T12:24:05,966 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 1 (count at <unknown>:0) with 1 output partitions
2025-04-30T12:24:05,968 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 2 (count at <unknown>:0)
2025-04-30T12:24:05,969 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List(ShuffleMapStage 1)
2025-04-30T12:24:05,971 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T12:24:05,973 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 2 (MapPartitionsRDD[6] at count at <unknown>:0), which has no missing parents
2025-04-30T12:24:05,986 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 12.5 KiB, free 434.1 MiB)
2025-04-30T12:24:05,992 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.1 MiB)
2025-04-30T12:24:05,995 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on bb9210fe839d:38313 (size: 6.0 KiB, free: 434.4 MiB)
2025-04-30T12:24:05,996 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 2 from broadcast at DAGScheduler.scala:1585
2025-04-30T12:24:06,000 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T12:24:06,001 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 2.0 with 1 tasks resource profile 0
2025-04-30T12:24:06,009 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 2.0 (TID 2) (172.18.0.3, executor 1, partition 0, NODE_LOCAL, 9003 bytes) 
2025-04-30T12:24:06,013 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 2
2025-04-30T12:24:06,015 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 2.0 (TID 2)
2025-04-30T12:24:06,025 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2025-04-30T12:24:06,044 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:24:06,054 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.0 MiB)
2025-04-30T12:24:06,058 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on 172.18.0.3:34285 (size: 6.0 KiB, free: 434.4 MiB)
2025-04-30T12:24:06,060 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 2 took 15 ms
2025-04-30T12:24:06,062 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 12.5 KiB, free 434.0 MiB)
2025-04-30T12:24:06,140 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Don't have map outputs for shuffle 0, fetching them
2025-04-30T12:24:06,142 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@bb9210fe839d:42779)
2025-04-30T12:24:06,148 [dispatcher-event-loop-8] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - Asked to send map output locations for shuffle 0 to 172.18.0.3:59016
2025-04-30T12:24:06,192 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on bb9210fe839d:38313 in memory (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T12:24:06,200 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on 172.18.0.3:34285 in memory (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T12:24:06,207 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on 172.18.0.5:43171 in memory (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T12:24:06,213 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Got the map output locations
2025-04-30T12:24:06,263 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Getting 2 (120.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 1 (60.0 B) remote blocks
2025-04-30T12:24:06,273 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.5:43171 after 2 ms (0 ms spent in bootstraps)
2025-04-30T12:24:06,282 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Started 1 remote fetches in 41 ms
2025-04-30T12:24:06,319 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 25.988359 ms
2025-04-30T12:24:06,339 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 2.0 (TID 2). 4081 bytes result sent to driver
2025-04-30T12:24:06,348 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 2.0 (TID 2) in 341 ms on 172.18.0.3 (executor 1) (1/1)
2025-04-30T12:24:06,349 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2025-04-30T12:24:06,352 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 2 (count at <unknown>:0) finished in 0.370 s
2025-04-30T12:24:06,355 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T12:24:06,356 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 2: Stage finished
2025-04-30T12:24:06,359 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 1 finished: count at <unknown>:0, took 0.397596 s
2025-04-30T12:24:06,374 [Thread-4] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-30T12:24:06,388 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@413b8567{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T12:24:06,394 [Thread-4] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://bb9210fe839d:4040
2025-04-30T12:24:06,402 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-30T12:24:06,404 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-30T12:24:06,413 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:24:06,414 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:24:06,415 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:24:06,417 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250430122355-0014
2025-04-30T12:24:06,419 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250430122355-0014
2025-04-30T12:24:06,436 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430122355-0014/1
2025-04-30T12:24:06,424 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430122355-0014/2
2025-04-30T12:24:06,441 [ExecutorRunner for app-20250430122355-0014/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430122355-0014/2 interrupted
2025-04-30T12:24:06,441 [ExecutorRunner for app-20250430122355-0014/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:24:06,445 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430122355-0014/0
2025-04-30T12:24:06,446 [ExecutorRunner for app-20250430122355-0014/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430122355-0014/1 interrupted
2025-04-30T12:24:06,447 [ExecutorRunner for app-20250430122355-0014/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430122355-0014/0 interrupted
2025-04-30T12:24:06,453 [ExecutorRunner for app-20250430122355-0014/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:24:06,455 [ExecutorRunner for app-20250430122355-0014/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:24:06,447 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:24:06,461 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:24:06,464 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:24:06,473 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:24:06,477 [shutdown-hook-0] INFO  org.apache.spark.storage.DiskBlockManager [] - Shutdown hook called
2025-04-30T12:24:06,478 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:24:06,479 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:24:06,485 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:24:06,482 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:24:06,488 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:24:06,495 [dispatcher-event-loop-5] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-30T12:24:06,497 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:24:06,508 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:24:06,508 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:24:06,521 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:24:06,522 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:24:06,528 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-30T12:24:06,534 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-30T12:24:06,535 [dispatcher-event-loop-4] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430122355-0014/2
2025-04-30T12:24:06,539 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430122355-0014/2 finished with state KILLED exitStatus 143
2025-04-30T12:24:06,540 [dispatcher-event-loop-11] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-30T12:24:06,541 [dispatcher-event-loop-11] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430122355-0014, execId=2)
2025-04-30T12:24:06,542 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:59834 got disassociated, removing it.
2025-04-30T12:24:06,542 [dispatcher-event-loop-11] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430122355-0014 removed, cleanupLocalDirs = true
2025-04-30T12:24:06,542 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430122355-0014
2025-04-30T12:24:06,543 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - bb9210fe839d:42779 got disassociated, removing it.
2025-04-30T12:24:06,551 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430122355-0014/1 finished with state KILLED exitStatus 143
2025-04-30T12:24:06,552 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-30T12:24:06,552 [dispatcher-event-loop-0] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430122355-0014/1
2025-04-30T12:24:06,554 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430122355-0014, execId=1)
2025-04-30T12:24:06,555 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430122355-0014
2025-04-30T12:24:06,555 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430122355-0014 removed, cleanupLocalDirs = true
2025-04-30T12:24:06,557 [Thread-4] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-30T12:24:06,562 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430122355-0014/0 finished with state KILLED exitStatus 143
2025-04-30T12:24:06,563 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-30T12:24:06,563 [dispatcher-event-loop-9] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430122355-0014/0
2025-04-30T12:24:06,564 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430122355-0014, execId=0)
2025-04-30T12:24:06,565 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430122355-0014 removed, cleanupLocalDirs = true
2025-04-30T12:24:06,565 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430122355-0014
2025-04-30T12:24:06,934 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:24:06,935 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-139b29ec-463a-4bef-b59f-b8d83463341e
2025-04-30T12:24:06,939 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-59bab81c-710c-4bad-b4a5-3106051caebf
2025-04-30T12:24:06,943 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-59bab81c-710c-4bad-b4a5-3106051caebf/pyspark-108696b9-f452-4905-939a-baeee37b877b
2025-04-30T12:25:16,796 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-30T12:25:16,802 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:25:16,803 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-30T12:25:16,839 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:25:16,840 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-30T12:25:16,841 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:25:16,842 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: mod-2-pr-4-data-ingestion.py
2025-04-30T12:25:16,870 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-30T12:25:16,882 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-30T12:25:16,886 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-30T12:25:16,954 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-30T12:25:16,955 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-30T12:25:16,956 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:25:16,958 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:25:16,960 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-30T12:25:17,031 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:25:17,351 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 37411.
2025-04-30T12:25:17,388 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-30T12:25:17,429 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-30T12:25:17,454 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-30T12:25:17,456 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-30T12:25:17,461 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-30T12:25:17,504 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-4b7e8f2e-fbe0-46e2-9913-58b3d8ac48b7
2025-04-30T12:25:17,540 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:25:17,567 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-30T12:25:17,621 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3454ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-30T12:25:17,737 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-30T12:25:17,759 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-30T12:25:17,825 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @3658ms
2025-04-30T12:25:17,903 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@413b8567{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T12:25:17,904 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-30T12:25:17,968 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1320cce0{/,null,AVAILABLE,@Spark}
2025-04-30T12:25:18,204 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-30T12:25:18,317 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 57 ms (0 ms spent in bootstraps)
2025-04-30T12:25:18,458 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - Registering app mod-2-pr-4-data-ingestion.py
2025-04-30T12:25:18,460 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - Registered app mod-2-pr-4-data-ingestion.py with ID app-20250430122518-0015
2025-04-30T12:25:18,461 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430122518-0015 with rpId: 0
2025-04-30T12:25:18,462 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430122518-0015/0 on worker worker-20250429114342-172.18.0.5-38821
2025-04-30T12:25:18,463 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430122518-0015/1 on worker worker-20250429114342-172.18.0.3-34443
2025-04-30T12:25:18,464 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430122518-0015/2 on worker worker-20250429114342-172.18.0.4-36391
2025-04-30T12:25:18,467 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430122518-0015/1 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:25:18,469 [dispatcher-event-loop-11] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250430122518-0015
2025-04-30T12:25:18,470 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430122518-0015/0 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:25:18,473 [ExecutorRunner for app-20250430122518-0015/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:25:18,474 [ExecutorRunner for app-20250430122518-0015/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:25:18,475 [ExecutorRunner for app-20250430122518-0015/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:25:18,475 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430122518-0015/2 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:25:18,476 [ExecutorRunner for app-20250430122518-0015/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:25:18,477 [ExecutorRunner for app-20250430122518-0015/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:25:18,478 [ExecutorRunner for app-20250430122518-0015/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:25:18,479 [ExecutorRunner for app-20250430122518-0015/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:25:18,480 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430122518-0015/0 on worker-20250429114342-172.18.0.5-38821 (172.18.0.5:38821) with 2 core(s)
2025-04-30T12:25:18,480 [ExecutorRunner for app-20250430122518-0015/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:25:18,480 [ExecutorRunner for app-20250430122518-0015/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:25:18,482 [ExecutorRunner for app-20250430122518-0015/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:25:18,482 [ExecutorRunner for app-20250430122518-0015/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:25:18,484 [ExecutorRunner for app-20250430122518-0015/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:25:18,484 [ExecutorRunner for app-20250430122518-0015/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:25:18,484 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430122518-0015/0 on hostPort 172.18.0.5:38821 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:25:18,485 [ExecutorRunner for app-20250430122518-0015/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:25:18,485 [ExecutorRunner for app-20250430122518-0015/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:25:18,486 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430122518-0015/1 on worker-20250429114342-172.18.0.3-34443 (172.18.0.3:34443) with 2 core(s)
2025-04-30T12:25:18,488 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430122518-0015/1 on hostPort 172.18.0.3:34443 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:25:18,491 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430122518-0015/2 on worker-20250429114342-172.18.0.4-36391 (172.18.0.4:36391) with 2 core(s)
2025-04-30T12:25:18,493 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430122518-0015/2 on hostPort 172.18.0.4:36391 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:25:18,498 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40191.
2025-04-30T12:25:18,499 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on bb9210fe839d:40191
2025-04-30T12:25:18,504 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:25:18,513 [ExecutorRunner for app-20250430122518-0015/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=37411" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:37411" "--executor-id" "1" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250430122518-0015" "--worker-url" "spark://Worker@172.18.0.3:34443" "--resourceProfileId" "0"
2025-04-30T12:25:18,515 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, bb9210fe839d, 40191, None)
2025-04-30T12:25:18,519 [ExecutorRunner for app-20250430122518-0015/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=37411" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:37411" "--executor-id" "2" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250430122518-0015" "--worker-url" "spark://Worker@172.18.0.4:36391" "--resourceProfileId" "0"
2025-04-30T12:25:18,520 [ExecutorRunner for app-20250430122518-0015/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=37411" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:37411" "--executor-id" "0" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250430122518-0015" "--worker-url" "spark://Worker@172.18.0.5:38821" "--resourceProfileId" "0"
2025-04-30T12:25:18,523 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager bb9210fe839d:40191 with 434.4 MiB RAM, BlockManagerId(driver, bb9210fe839d, 40191, None)
2025-04-30T12:25:18,531 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, bb9210fe839d, 40191, None)
2025-04-30T12:25:18,531 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430122518-0015 with rpId: 0
2025-04-30T12:25:18,535 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, bb9210fe839d, 40191, None)
2025-04-30T12:25:18,535 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430122518-0015 with rpId: 0
2025-04-30T12:25:18,541 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430122518-0015 with rpId: 0
2025-04-30T12:25:18,561 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430122518-0015/1 is now RUNNING
2025-04-30T12:25:18,566 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430122518-0015/2 is now RUNNING
2025-04-30T12:25:18,575 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430122518-0015/0 is now RUNNING
2025-04-30T12:25:19,034 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250430122518-0015.inprogress
2025-04-30T12:25:19,285 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@1320cce0{/,null,STOPPED,@Spark}
2025-04-30T12:25:19,290 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40857667{/jobs,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,295 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7fa04fb2{/jobs/json,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,297 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@35083451{/jobs/job,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,303 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@354b22b6{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,306 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@484d933e{/stages,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,310 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7274d8f2{/stages/json,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,314 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2dab95f3{/stages/stage,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,321 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4622b9f0{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,326 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@611e343f{/stages/pool,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,329 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@56578f8c{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,331 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@fa43f57{/storage,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,340 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2a9cdf28{/storage/json,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,349 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@41d8187c{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,358 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@251ea385{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,364 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@d678b60{/environment,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,375 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@ef17b66{/environment/json,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,381 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@58ad57bd{/executors,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,391 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1cadc900{/executors/json,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,396 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40aadd59{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,400 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3eeb6954{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,408 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1964fe85{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,417 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@52124300{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,442 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@76c527e1{/static,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,456 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@548a25dc{/,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,461 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@28825700{/api,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,464 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@611d70f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,471 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@21e7c069{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,486 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@64ddd77d{/metrics/json,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,491 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-30T12:25:19,909 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-04-30T12:25:19,913 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
2025-04-30T12:25:19,935 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@739d96e{/SQL,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,940 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@113354b6{/SQL/json,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,943 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7bb072e9{/SQL/execution,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,947 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2dd2a8f2{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-04-30T12:25:19,960 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@9867130{/static/sql,null,AVAILABLE,@Spark}
2025-04-30T12:25:20,737 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 943@c27de723f0d4
2025-04-30T12:25:20,754 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:25:20,756 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:25:20,758 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:25:20,777 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 994@6cca10b5c892
2025-04-30T12:25:20,792 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:25:20,794 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:25:20,795 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:25:20,825 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 1012@066f03cf7e53
2025-04-30T12:25:20,840 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:25:20,842 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:25:20,843 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:25:21,497 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:25:21,533 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:25:21,609 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:25:21,735 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:25:21,742 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:25:21,752 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:25:21,755 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:25:21,756 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:25:21,771 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:25:21,773 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:25:21,776 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:25:21,778 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:25:21,780 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:25:21,836 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:25:21,841 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:25:21,844 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:25:21,846 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:25:21,852 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:25:21,870 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 125 ms to list leaf files for 2 paths.
2025-04-30T12:25:22,413 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:37411 after 95 ms (0 ms spent in bootstraps)
2025-04-30T12:25:22,433 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:37411 after 91 ms (0 ms spent in bootstraps)
2025-04-30T12:25:22,519 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:37411 after 112 ms (0 ms spent in bootstraps)
2025-04-30T12:25:22,613 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:25:22,614 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:25:22,618 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:25:22,619 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:25:22,621 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:25:22,627 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:25:22,628 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:25:22,629 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:25:22,632 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:25:22,633 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:25:22,698 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:25:22,708 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:25:22,710 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:25:22,711 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:25:22,712 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:25:22,716 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:37411 after 11 ms (0 ms spent in bootstraps)
2025-04-30T12:25:22,717 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:37411 after 7 ms (0 ms spent in bootstraps)
2025-04-30T12:25:22,801 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:37411 after 3 ms (0 ms spent in bootstraps)
2025-04-30T12:25:22,835 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-00d4d2fa-4667-4dcc-a4a7-1a25b94c3d06/executor-e696f2bb-27af-4a16-9086-ea42b1157a8f/blockmgr-4fec458c-4343-4b2e-aaaf-168bc7882e01
2025-04-30T12:25:22,837 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-8bb716ea-3eee-4da7-a70b-3a99059b4da9/executor-b9100676-2189-469e-ab0a-9410338bf239/blockmgr-b9e02825-5681-4182-9f8d-4d163ad00090
2025-04-30T12:25:22,881 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:25:22,887 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:25:22,902 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-c5ae99e1-4d9a-42b7-9555-210f29b2c4bb/executor-92e63747-47be-4d47-b4b2-af9e3676d54b/blockmgr-b4a23ee3-c283-4ba3-ad67-250738512b17
2025-04-30T12:25:22,945 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:25:23,147 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:37411
2025-04-30T12:25:23,149 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.3:34443
2025-04-30T12:25:23,163 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.3:34443 after 7 ms (0 ms spent in bootstraps)
2025-04-30T12:25:23,164 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.3:34443
2025-04-30T12:25:23,165 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:37411
2025-04-30T12:25:23,164 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:25:23,166 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.4:36391
2025-04-30T12:25:23,169 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:25:23,174 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:25:23,184 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.4:36391 after 13 ms (0 ms spent in bootstraps)
2025-04-30T12:25:23,187 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.4:36391
2025-04-30T12:25:23,193 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:25:23,194 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:25:23,196 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:25:23,230 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:37411
2025-04-30T12:25:23,231 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.5:38821
2025-04-30T12:25:23,238 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.5:38821 after 3 ms (0 ms spent in bootstraps)
2025-04-30T12:25:23,247 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.5:38821
2025-04-30T12:25:23,250 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:58648) with ID 1,  ResourceProfileId 0
2025-04-30T12:25:23,257 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:41928) with ID 2,  ResourceProfileId 0
2025-04-30T12:25:23,267 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:25:23,268 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:25:23,268 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:25:23,275 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:25:23,276 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 2 on host 172.18.0.4
2025-04-30T12:25:23,276 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 1 on host 172.18.0.3
2025-04-30T12:25:23,277 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:25:23,278 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:25:23,279 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:25:23,280 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:25:23,280 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:25:23,317 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:37000) with ID 0,  ResourceProfileId 0
2025-04-30T12:25:23,330 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:25:23,335 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 0 on host 172.18.0.5
2025-04-30T12:25:23,339 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:25:23,341 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:25:23,362 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37301.
2025-04-30T12:25:23,365 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35365.
2025-04-30T12:25:23,364 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.4:37301
2025-04-30T12:25:23,366 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.3:35365
2025-04-30T12:25:23,370 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:25:23,372 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:25:23,380 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(2, 172.18.0.4, 37301, None)
2025-04-30T12:25:23,382 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(1, 172.18.0.3, 35365, None)
2025-04-30T12:25:23,396 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.4:37301 with 434.4 MiB RAM, BlockManagerId(2, 172.18.0.4, 37301, None)
2025-04-30T12:25:23,402 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.3:35365 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.3, 35365, None)
2025-04-30T12:25:23,413 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(2, 172.18.0.4, 37301, None)
2025-04-30T12:25:23,416 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(1, 172.18.0.3, 35365, None)
2025-04-30T12:25:23,418 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(2, 172.18.0.4, 37301, None)
2025-04-30T12:25:23,419 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(1, 172.18.0.3, 35365, None)
2025-04-30T12:25:23,433 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45713.
2025-04-30T12:25:23,434 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.5:45713
2025-04-30T12:25:23,436 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:25:23,437 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:25:23,438 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@47c84b10 for default.
2025-04-30T12:25:23,439 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:25:23,441 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@25c7034b for default.
2025-04-30T12:25:23,452 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(0, 172.18.0.5, 45713, None)
2025-04-30T12:25:23,469 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.5:45713 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.5, 45713, None)
2025-04-30T12:25:23,478 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(0, 172.18.0.5, 45713, None)
2025-04-30T12:25:23,485 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(0, 172.18.0.5, 45713, None)
2025-04-30T12:25:23,496 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:25:23,505 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@540144a3 for default.
2025-04-30T12:25:23,916 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T12:25:23,919 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T12:25:24,572 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 310.711602 ms
2025-04-30T12:25:24,671 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 200.1 KiB, free 434.2 MiB)
2025-04-30T12:25:24,748 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.2 MiB)
2025-04-30T12:25:24,753 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on bb9210fe839d:40191 (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T12:25:24,759 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 0 from showString at <unknown>:0
2025-04-30T12:25:24,783 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T12:25:24,901 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: showString at <unknown>:0
2025-04-30T12:25:24,923 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 0 (showString at <unknown>:0) with 1 output partitions
2025-04-30T12:25:24,925 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 0 (showString at <unknown>:0)
2025-04-30T12:25:24,926 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T12:25:24,929 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T12:25:24,936 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 0 (MapPartitionsRDD[3] at showString at <unknown>:0), which has no missing parents
2025-04-30T12:25:25,038 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 20.6 KiB, free 434.2 MiB)
2025-04-30T12:25:25,041 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 434.1 MiB)
2025-04-30T12:25:25,043 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on bb9210fe839d:40191 (size: 8.5 KiB, free: 434.4 MiB)
2025-04-30T12:25:25,044 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 1 from broadcast at DAGScheduler.scala:1585
2025-04-30T12:25:25,064 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T12:25:25,066 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 0.0 with 1 tasks resource profile 0
2025-04-30T12:25:25,112 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 9657 bytes) 
2025-04-30T12:25:25,147 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 0
2025-04-30T12:25:25,161 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 0.0 (TID 0)
2025-04-30T12:25:25,328 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:25:25,385 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:40191 after 2 ms (0 ms spent in bootstraps)
2025-04-30T12:25:25,443 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 434.4 MiB)
2025-04-30T12:25:25,452 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on 172.18.0.5:45713 (size: 8.5 KiB, free: 434.4 MiB)
2025-04-30T12:25:25,458 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 1 took 128 ms
2025-04-30T12:25:25,518 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 20.6 KiB, free 434.4 MiB)
2025-04-30T12:25:26,166 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 256.035739 ms
2025-04-30T12:25:26,170 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY%20copy.jsonl, range: 0-35354, partition values: [empty row]
2025-04-30T12:25:26,285 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 37.131276 ms
2025-04-30T12:25:26,310 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:25:26,322 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.3 MiB)
2025-04-30T12:25:26,326 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on 172.18.0.5:45713 (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T12:25:26,329 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 0 took 18 ms
2025-04-30T12:25:26,428 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T12:25:26,624 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 0.0 (TID 0). 2704 bytes result sent to driver
2025-04-30T12:25:26,643 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 0.0 (TID 0) in 1548 ms on 172.18.0.5 (executor 0) (1/1)
2025-04-30T12:25:26,645 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-04-30T12:25:26,651 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 0 (showString at <unknown>:0) finished in 1.696 s
2025-04-30T12:25:26,656 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T12:25:26,657 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 0: Stage finished
2025-04-30T12:25:26,660 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 finished: showString at <unknown>:0, took 1.757875 s
2025-04-30T12:25:27,002 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on bb9210fe839d:40191 in memory (size: 8.5 KiB, free: 434.4 MiB)
2025-04-30T12:25:27,012 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on 172.18.0.5:45713 in memory (size: 8.5 KiB, free: 434.4 MiB)
2025-04-30T12:25:27,720 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 40.034523 ms
2025-04-30T12:25:27,869 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T12:25:27,870 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T12:25:28,010 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 15.293673 ms
2025-04-30T12:25:28,022 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 200.1 KiB, free 434.0 MiB)
2025-04-30T12:25:28,045 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.9 MiB)
2025-04-30T12:25:28,048 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on bb9210fe839d:40191 (size: 34.4 KiB, free: 434.3 MiB)
2025-04-30T12:25:28,050 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 2 from count at <unknown>:0
2025-04-30T12:25:28,056 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T12:25:28,104 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Registering RDD 7 (count at <unknown>:0) as input to shuffle 0
2025-04-30T12:25:28,110 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got map stage job 1 (count at <unknown>:0) with 2 output partitions
2025-04-30T12:25:28,111 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ShuffleMapStage 1 (count at <unknown>:0)
2025-04-30T12:25:28,113 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T12:25:28,115 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T12:25:28,120 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0), which has no missing parents
2025-04-30T12:25:28,152 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 433.9 MiB)
2025-04-30T12:25:28,157 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 433.9 MiB)
2025-04-30T12:25:28,158 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on bb9210fe839d:40191 (size: 8.3 KiB, free: 434.3 MiB)
2025-04-30T12:25:28,159 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 3 from broadcast at DAGScheduler.scala:1585
2025-04-30T12:25:28,164 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
2025-04-30T12:25:28,164 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 1.0 with 2 tasks resource profile 0
2025-04-30T12:25:28,171 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 9646 bytes) 
2025-04-30T12:25:28,173 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 1.0 in stage 1.0 (TID 2) (172.18.0.3, executor 1, partition 1, PROCESS_LOCAL, 9639 bytes) 
2025-04-30T12:25:28,179 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 1
2025-04-30T12:25:28,181 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 1.0 (TID 1)
2025-04-30T12:25:28,203 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 2
2025-04-30T12:25:28,218 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:25:28,225 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Running task 1.0 in stage 1.0 (TID 2)
2025-04-30T12:25:28,234 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 434.0 MiB)
2025-04-30T12:25:28,239 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on 172.18.0.5:45713 (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T12:25:28,242 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 3 took 22 ms
2025-04-30T12:25:28,244 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 434.0 MiB)
2025-04-30T12:25:28,312 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 16.500934 ms
2025-04-30T12:25:28,326 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY%20copy.jsonl, range: 0-35354, partition values: [empty row]
2025-04-30T12:25:28,340 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 11.151863 ms
2025-04-30T12:25:28,342 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:25:28,350 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.9 MiB)
2025-04-30T12:25:28,353 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:25:28,354 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on 172.18.0.5:45713 (size: 34.4 KiB, free: 434.3 MiB)
2025-04-30T12:25:28,356 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 2 took 13 ms
2025-04-30T12:25:28,374 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 377.3 KiB, free 433.6 MiB)
2025-04-30T12:25:28,408 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:40191 after 2 ms (0 ms spent in bootstraps)
2025-04-30T12:25:28,443 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 434.4 MiB)
2025-04-30T12:25:28,449 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on 172.18.0.3:35365 (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T12:25:28,456 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 3 took 102 ms
2025-04-30T12:25:28,482 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 1.0 (TID 1). 1964 bytes result sent to driver
2025-04-30T12:25:28,493 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 1.0 (TID 1) in 324 ms on 172.18.0.5 (executor 0) (1/2)
2025-04-30T12:25:28,553 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 434.4 MiB)
2025-04-30T12:25:29,278 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 224.667754 ms
2025-04-30T12:25:29,294 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl, range: 0-35354, partition values: [empty row]
2025-04-30T12:25:29,334 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 13.26257 ms
2025-04-30T12:25:29,353 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:25:29,360 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.5:45713 after 1 ms (0 ms spent in bootstraps)
2025-04-30T12:25:29,375 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.3 MiB)
2025-04-30T12:25:29,379 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on 172.18.0.3:35365 (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T12:25:29,381 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 2 took 27 ms
2025-04-30T12:25:29,458 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T12:25:29,651 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Finished task 1.0 in stage 1.0 (TID 2). 2007 bytes result sent to driver
2025-04-30T12:25:29,658 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 1.0 in stage 1.0 (TID 2) in 1485 ms on 172.18.0.3 (executor 1) (2/2)
2025-04-30T12:25:29,659 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-04-30T12:25:29,660 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ShuffleMapStage 1 (count at <unknown>:0) finished in 1.533 s
2025-04-30T12:25:29,661 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - looking for newly runnable stages
2025-04-30T12:25:29,662 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - running: Set()
2025-04-30T12:25:29,664 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - waiting: Set()
2025-04-30T12:25:29,664 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - failed: Set()
2025-04-30T12:25:29,709 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 16.280622 ms
2025-04-30T12:25:29,740 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: count at <unknown>:0
2025-04-30T12:25:29,744 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 2 (count at <unknown>:0) with 1 output partitions
2025-04-30T12:25:29,744 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 3 (count at <unknown>:0)
2025-04-30T12:25:29,745 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List(ShuffleMapStage 2)
2025-04-30T12:25:29,746 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T12:25:29,747 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0), which has no missing parents
2025-04-30T12:25:29,760 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 433.9 MiB)
2025-04-30T12:25:29,792 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 433.9 MiB)
2025-04-30T12:25:29,796 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_4_piece0 in memory on bb9210fe839d:40191 (size: 6.0 KiB, free: 434.3 MiB)
2025-04-30T12:25:29,798 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_3_piece0 on bb9210fe839d:40191 in memory (size: 8.3 KiB, free: 434.3 MiB)
2025-04-30T12:25:29,799 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 4 from broadcast at DAGScheduler.scala:1585
2025-04-30T12:25:29,808 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T12:25:29,809 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_3_piece0 on 172.18.0.5:45713 in memory (size: 8.3 KiB, free: 434.3 MiB)
2025-04-30T12:25:29,809 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 3.0 with 1 tasks resource profile 0
2025-04-30T12:25:29,815 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_3_piece0 on 172.18.0.3:35365 in memory (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T12:25:29,825 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 3.0 (TID 3) (172.18.0.5, executor 0, partition 0, NODE_LOCAL, 9003 bytes) 
2025-04-30T12:25:29,830 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 3
2025-04-30T12:25:29,832 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 3.0 (TID 3)
2025-04-30T12:25:29,841 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2025-04-30T12:25:29,845 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:25:29,856 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 433.6 MiB)
2025-04-30T12:25:29,861 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_4_piece0 in memory on 172.18.0.5:45713 (size: 6.0 KiB, free: 434.3 MiB)
2025-04-30T12:25:29,864 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 4 took 18 ms
2025-04-30T12:25:29,866 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 433.6 MiB)
2025-04-30T12:25:29,879 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Don't have map outputs for shuffle 0, fetching them
2025-04-30T12:25:29,881 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@bb9210fe839d:37411)
2025-04-30T12:25:29,890 [dispatcher-event-loop-5] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - Asked to send map output locations for shuffle 0 to 172.18.0.5:37000
2025-04-30T12:25:29,946 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Got the map output locations
2025-04-30T12:25:29,978 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Getting 2 (120.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 1 (60.0 B) remote blocks
2025-04-30T12:25:29,983 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.3:35365 after 1 ms (0 ms spent in bootstraps)
2025-04-30T12:25:29,988 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Started 1 remote fetches in 23 ms
2025-04-30T12:25:30,007 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 12.731356 ms
2025-04-30T12:25:30,021 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 3.0 (TID 3). 4038 bytes result sent to driver
2025-04-30T12:25:30,027 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 3.0 (TID 3) in 203 ms on 172.18.0.5 (executor 0) (1/1)
2025-04-30T12:25:30,028 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2025-04-30T12:25:30,029 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 3 (count at <unknown>:0) finished in 0.271 s
2025-04-30T12:25:30,030 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T12:25:30,031 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 3: Stage finished
2025-04-30T12:25:30,032 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 2 finished: count at <unknown>:0, took 0.290741 s
2025-04-30T12:25:30,050 [Thread-4] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-30T12:25:30,074 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@413b8567{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T12:25:30,081 [Thread-4] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://bb9210fe839d:4040
2025-04-30T12:25:30,093 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-30T12:25:30,095 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-30T12:25:30,105 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:25:30,106 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:25:30,107 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:25:30,113 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250430122518-0015
2025-04-30T12:25:30,114 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250430122518-0015
2025-04-30T12:25:30,117 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430122518-0015/2
2025-04-30T12:25:30,119 [ExecutorRunner for app-20250430122518-0015/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430122518-0015/2 interrupted
2025-04-30T12:25:30,121 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430122518-0015/1
2025-04-30T12:25:30,122 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430122518-0015/0
2025-04-30T12:25:30,120 [ExecutorRunner for app-20250430122518-0015/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:25:30,139 [ExecutorRunner for app-20250430122518-0015/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430122518-0015/1 interrupted
2025-04-30T12:25:30,139 [ExecutorRunner for app-20250430122518-0015/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430122518-0015/0 interrupted
2025-04-30T12:25:30,140 [ExecutorRunner for app-20250430122518-0015/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:25:30,140 [ExecutorRunner for app-20250430122518-0015/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:25:30,142 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:25:30,143 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:25:30,144 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:25:30,145 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:25:30,146 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:25:30,149 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:25:30,150 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:25:30,152 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:25:30,155 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:25:30,163 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:25:30,164 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:25:30,165 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:25:30,177 [dispatcher-event-loop-9] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-30T12:25:30,197 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:25:30,200 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:25:30,211 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-30T12:25:30,214 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430122518-0015/2 finished with state KILLED exitStatus 143
2025-04-30T12:25:30,214 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430122518-0015/1 finished with state KILLED exitStatus 143
2025-04-30T12:25:30,215 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-30T12:25:30,215 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-30T12:25:30,215 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-30T12:25:30,216 [dispatcher-event-loop-11] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430122518-0015/1
2025-04-30T12:25:30,216 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430122518-0015, execId=1)
2025-04-30T12:25:30,217 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430122518-0015 removed, cleanupLocalDirs = true
2025-04-30T12:25:30,217 [dispatcher-event-loop-11] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430122518-0015/2
2025-04-30T12:25:30,217 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430122518-0015
2025-04-30T12:25:30,220 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430122518-0015/0 finished with state KILLED exitStatus 143
2025-04-30T12:25:30,216 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430122518-0015, execId=2)
2025-04-30T12:25:30,221 [dispatcher-event-loop-8] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430122518-0015/0
2025-04-30T12:25:30,221 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-30T12:25:30,222 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430122518-0015, execId=0)
2025-04-30T12:25:30,222 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430122518-0015 removed, cleanupLocalDirs = true
2025-04-30T12:25:30,222 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430122518-0015
2025-04-30T12:25:30,223 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430122518-0015 removed, cleanupLocalDirs = true
2025-04-30T12:25:30,223 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430122518-0015
2025-04-30T12:25:30,227 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:55828 got disassociated, removing it.
2025-04-30T12:25:30,228 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.master.Master [] - bb9210fe839d:37411 got disassociated, removing it.
2025-04-30T12:25:30,235 [Thread-4] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-30T12:25:30,618 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:25:30,619 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-c0677e87-8a43-4e69-89c3-c59f51f45054
2025-04-30T12:25:30,624 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-6f628cd7-0ce2-47c6-9295-7bf55f31d0cd
2025-04-30T12:25:30,629 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-c0677e87-8a43-4e69-89c3-c59f51f45054/pyspark-ac0fcd4a-a527-4de2-9dec-9bb56a0bf68e
2025-04-30T12:26:35,056 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-30T12:26:35,061 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:26:35,062 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-30T12:26:35,089 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:26:35,090 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-30T12:26:35,092 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:26:35,093 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: mod-2-pr-4-data-ingestion.py
2025-04-30T12:26:35,114 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-30T12:26:35,124 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-30T12:26:35,125 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-30T12:26:35,178 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-30T12:26:35,180 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-30T12:26:35,181 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:26:35,183 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:26:35,184 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-30T12:26:35,257 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:26:35,531 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 33503.
2025-04-30T12:26:35,561 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-30T12:26:35,600 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-30T12:26:35,621 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-30T12:26:35,623 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-30T12:26:35,627 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-30T12:26:35,646 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-2b01cf93-d373-413b-b646-6213181856b4
2025-04-30T12:26:35,662 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:26:35,681 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-30T12:26:35,730 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3724ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-30T12:26:35,851 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-30T12:26:35,866 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-30T12:26:35,894 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @3888ms
2025-04-30T12:26:35,935 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@7cdbf7d{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T12:26:35,936 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-30T12:26:35,965 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@12f61412{/,null,AVAILABLE,@Spark}
2025-04-30T12:26:36,082 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-30T12:26:36,132 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 28 ms (0 ms spent in bootstraps)
2025-04-30T12:26:36,219 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - Registering app mod-2-pr-4-data-ingestion.py
2025-04-30T12:26:36,221 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - Registered app mod-2-pr-4-data-ingestion.py with ID app-20250430122636-0016
2025-04-30T12:26:36,222 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430122636-0016 with rpId: 0
2025-04-30T12:26:36,223 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430122636-0016/0 on worker worker-20250429114342-172.18.0.5-38821
2025-04-30T12:26:36,225 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430122636-0016/1 on worker worker-20250429114342-172.18.0.3-34443
2025-04-30T12:26:36,226 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430122636-0016/2 on worker worker-20250429114342-172.18.0.4-36391
2025-04-30T12:26:36,229 [dispatcher-event-loop-10] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250430122636-0016
2025-04-30T12:26:36,230 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430122636-0016/1 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:26:36,230 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430122636-0016/2 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:26:36,233 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430122636-0016/0 on worker-20250429114342-172.18.0.5-38821 (172.18.0.5:38821) with 2 core(s)
2025-04-30T12:26:36,234 [ExecutorRunner for app-20250430122636-0016/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:26:36,235 [ExecutorRunner for app-20250430122636-0016/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:26:36,236 [ExecutorRunner for app-20250430122636-0016/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:26:36,237 [ExecutorRunner for app-20250430122636-0016/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:26:36,237 [ExecutorRunner for app-20250430122636-0016/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:26:36,238 [ExecutorRunner for app-20250430122636-0016/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:26:36,238 [ExecutorRunner for app-20250430122636-0016/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:26:36,240 [ExecutorRunner for app-20250430122636-0016/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:26:36,240 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430122636-0016/0 on hostPort 172.18.0.5:38821 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:26:36,241 [ExecutorRunner for app-20250430122636-0016/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:26:36,240 [ExecutorRunner for app-20250430122636-0016/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:26:36,243 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430122636-0016/1 on worker-20250429114342-172.18.0.3-34443 (172.18.0.3:34443) with 2 core(s)
2025-04-30T12:26:36,245 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430122636-0016/1 on hostPort 172.18.0.3:34443 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:26:36,246 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430122636-0016/2 on worker-20250429114342-172.18.0.4-36391 (172.18.0.4:36391) with 2 core(s)
2025-04-30T12:26:36,247 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430122636-0016/2 on hostPort 172.18.0.4:36391 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:26:36,247 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42343.
2025-04-30T12:26:36,248 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on bb9210fe839d:42343
2025-04-30T12:26:36,251 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:26:36,254 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430122636-0016/0 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:26:36,258 [ExecutorRunner for app-20250430122636-0016/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:26:36,259 [ExecutorRunner for app-20250430122636-0016/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:26:36,260 [ExecutorRunner for app-20250430122636-0016/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:26:36,261 [ExecutorRunner for app-20250430122636-0016/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:26:36,261 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, bb9210fe839d, 42343, None)
2025-04-30T12:26:36,261 [ExecutorRunner for app-20250430122636-0016/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:26:36,267 [ExecutorRunner for app-20250430122636-0016/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=33503" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:33503" "--executor-id" "1" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250430122636-0016" "--worker-url" "spark://Worker@172.18.0.3:34443" "--resourceProfileId" "0"
2025-04-30T12:26:36,268 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager bb9210fe839d:42343 with 434.4 MiB RAM, BlockManagerId(driver, bb9210fe839d, 42343, None)
2025-04-30T12:26:36,269 [ExecutorRunner for app-20250430122636-0016/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=33503" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:33503" "--executor-id" "2" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250430122636-0016" "--worker-url" "spark://Worker@172.18.0.4:36391" "--resourceProfileId" "0"
2025-04-30T12:26:36,272 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, bb9210fe839d, 42343, None)
2025-04-30T12:26:36,275 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, bb9210fe839d, 42343, None)
2025-04-30T12:26:36,281 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430122636-0016 with rpId: 0
2025-04-30T12:26:36,288 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430122636-0016 with rpId: 0
2025-04-30T12:26:36,299 [ExecutorRunner for app-20250430122636-0016/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=33503" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:33503" "--executor-id" "0" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250430122636-0016" "--worker-url" "spark://Worker@172.18.0.5:38821" "--resourceProfileId" "0"
2025-04-30T12:26:36,311 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430122636-0016/1 is now RUNNING
2025-04-30T12:26:36,312 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430122636-0016 with rpId: 0
2025-04-30T12:26:36,316 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430122636-0016/2 is now RUNNING
2025-04-30T12:26:36,320 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430122636-0016/0 is now RUNNING
2025-04-30T12:26:36,741 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250430122636-0016.inprogress
2025-04-30T12:26:36,996 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@12f61412{/,null,STOPPED,@Spark}
2025-04-30T12:26:37,000 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@297befdc{/jobs,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,004 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@bc19d02{/jobs/json,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,008 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3ef68c48{/jobs/job,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,013 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@503d5c5f{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,015 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@22fcf373{/stages,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,018 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4bdd11d{/stages/json,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,024 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@388c6ceb{/stages/stage,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,027 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2a279c54{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,030 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3226e90c{/stages/pool,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,032 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@270fafbe{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,036 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7af5297b{/storage,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,039 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@73d5fdae{/storage/json,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,042 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@29271c33{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,044 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3252b2c7{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,047 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@b80844c{/environment,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,049 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@725aac6e{/environment/json,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,055 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@59a94931{/executors,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,063 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@64d08a03{/executors/json,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,066 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1fbc6bc{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,072 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@51db85ca{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,075 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5bbfcafe{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,078 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3b9a3647{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,100 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6b9bf9e1{/static,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,105 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@51102f03{/,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,110 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4227e747{/api,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,113 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1f3f96f2{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,116 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@9ae6795{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,125 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3adbe449{/metrics/json,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,128 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-30T12:26:37,499 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-04-30T12:26:37,526 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
2025-04-30T12:26:37,572 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4316aaf2{/SQL,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,581 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@146c2e0d{/SQL/json,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,587 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@57fc25f7{/SQL/execution,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,589 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@151b8458{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-04-30T12:26:37,595 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4e959c8b{/static/sql,null,AVAILABLE,@Spark}
2025-04-30T12:26:38,660 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 1031@c27de723f0d4
2025-04-30T12:26:38,679 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:26:38,681 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:26:38,682 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:26:38,838 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 1103@066f03cf7e53
2025-04-30T12:26:38,854 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:26:38,856 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:26:38,857 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:26:38,861 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 1077@6cca10b5c892
2025-04-30T12:26:38,875 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:26:38,876 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:26:38,877 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:26:39,467 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:26:39,575 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:26:39,626 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:26:39,763 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:26:39,766 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:26:39,768 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:26:39,770 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:26:39,776 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:26:39,779 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:26:39,782 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:26:39,783 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:26:39,785 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:26:39,787 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:26:39,863 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 145 ms to list leaf files for 2 paths.
2025-04-30T12:26:39,908 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:26:39,912 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:26:39,914 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:26:39,918 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:26:39,921 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:26:40,339 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:33503 after 103 ms (0 ms spent in bootstraps)
2025-04-30T12:26:40,341 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:33503 after 110 ms (0 ms spent in bootstraps)
2025-04-30T12:26:40,415 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:33503 after 113 ms (0 ms spent in bootstraps)
2025-04-30T12:26:40,527 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:26:40,533 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:26:40,537 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:26:40,537 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:26:40,538 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:26:40,539 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:26:40,539 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:26:40,540 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:26:40,541 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:26:40,542 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:26:40,600 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:26:40,601 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:26:40,605 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:26:40,607 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:26:40,607 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:26:40,646 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:33503 after 4 ms (0 ms spent in bootstraps)
2025-04-30T12:26:40,653 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:33503 after 6 ms (0 ms spent in bootstraps)
2025-04-30T12:26:40,693 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:33503 after 3 ms (0 ms spent in bootstraps)
2025-04-30T12:26:40,760 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-00d4d2fa-4667-4dcc-a4a7-1a25b94c3d06/executor-cdf870e4-8a47-4f39-8d32-5a6c3c23e6ba/blockmgr-43fc5e1b-4005-4554-b0ad-b79c31a62d79
2025-04-30T12:26:40,777 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-c5ae99e1-4d9a-42b7-9555-210f29b2c4bb/executor-3733b652-64ba-4f7d-b1da-79650dd93956/blockmgr-4840a0ba-7e5f-4c7d-85f7-448d13482e24
2025-04-30T12:26:40,834 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-8bb716ea-3eee-4da7-a70b-3a99059b4da9/executor-0b221e53-0741-4ec8-ac1e-c908f444bb04/blockmgr-cde43cea-5b79-4787-94e1-dd1e5ac162b4
2025-04-30T12:26:40,836 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:26:40,850 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:26:40,897 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:26:41,139 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:33503
2025-04-30T12:26:41,142 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.3:34443
2025-04-30T12:26:41,145 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:33503
2025-04-30T12:26:41,147 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.5:38821
2025-04-30T12:26:41,150 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.3:34443 after 4 ms (0 ms spent in bootstraps)
2025-04-30T12:26:41,160 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.3:34443
2025-04-30T12:26:41,162 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:26:41,165 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:26:41,165 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.5:38821 after 8 ms (0 ms spent in bootstraps)
2025-04-30T12:26:41,167 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.5:38821
2025-04-30T12:26:41,171 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:26:41,170 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:26:41,173 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:26:41,175 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:26:41,199 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:33503
2025-04-30T12:26:41,206 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.4:36391
2025-04-30T12:26:41,222 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:26:41,224 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:42172) with ID 0,  ResourceProfileId 0
2025-04-30T12:26:41,223 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.4:36391
2025-04-30T12:26:41,222 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.4:36391 after 6 ms (0 ms spent in bootstraps)
2025-04-30T12:26:41,225 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:26:41,228 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:50180) with ID 1,  ResourceProfileId 0
2025-04-30T12:26:41,232 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:26:41,237 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:26:41,242 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:26:41,247 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 1 on host 172.18.0.3
2025-04-30T12:26:41,249 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:26:41,253 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:26:41,253 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 0 on host 172.18.0.5
2025-04-30T12:26:41,258 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:26:41,259 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:26:41,292 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:33558) with ID 2,  ResourceProfileId 0
2025-04-30T12:26:41,315 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:26:41,322 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 2 on host 172.18.0.4
2025-04-30T12:26:41,323 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:26:41,325 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:26:41,351 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42845.
2025-04-30T12:26:41,352 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36777.
2025-04-30T12:26:41,354 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.5:42845
2025-04-30T12:26:41,356 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.3:36777
2025-04-30T12:26:41,360 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:26:41,362 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:26:41,370 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(0, 172.18.0.5, 42845, None)
2025-04-30T12:26:41,371 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(1, 172.18.0.3, 36777, None)
2025-04-30T12:26:41,389 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.3:36777 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.3, 36777, None)
2025-04-30T12:26:41,392 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.5:42845 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.5, 42845, None)
2025-04-30T12:26:41,399 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(1, 172.18.0.3, 36777, None)
2025-04-30T12:26:41,400 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(0, 172.18.0.5, 42845, None)
2025-04-30T12:26:41,401 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(1, 172.18.0.3, 36777, None)
2025-04-30T12:26:41,402 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46127.
2025-04-30T12:26:41,403 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(0, 172.18.0.5, 42845, None)
2025-04-30T12:26:41,404 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.4:46127
2025-04-30T12:26:41,408 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:26:41,416 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:26:41,417 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:26:41,419 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@51b1068 for default.
2025-04-30T12:26:41,421 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(2, 172.18.0.4, 46127, None)
2025-04-30T12:26:41,421 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6441b79f for default.
2025-04-30T12:26:41,433 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.4:46127 with 434.4 MiB RAM, BlockManagerId(2, 172.18.0.4, 46127, None)
2025-04-30T12:26:41,442 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(2, 172.18.0.4, 46127, None)
2025-04-30T12:26:41,445 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(2, 172.18.0.4, 46127, None)
2025-04-30T12:26:41,460 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:26:41,463 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2e0ecba5 for default.
2025-04-30T12:26:41,887 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T12:26:41,889 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T12:26:42,416 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 226.341898 ms
2025-04-30T12:26:42,473 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 200.1 KiB, free 434.2 MiB)
2025-04-30T12:26:42,532 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.2 MiB)
2025-04-30T12:26:42,536 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on bb9210fe839d:42343 (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T12:26:42,543 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 0 from showString at <unknown>:0
2025-04-30T12:26:42,568 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T12:26:42,692 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: showString at <unknown>:0
2025-04-30T12:26:42,707 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 0 (showString at <unknown>:0) with 1 output partitions
2025-04-30T12:26:42,708 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 0 (showString at <unknown>:0)
2025-04-30T12:26:42,709 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T12:26:42,711 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T12:26:42,717 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 0 (MapPartitionsRDD[3] at showString at <unknown>:0), which has no missing parents
2025-04-30T12:26:42,796 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 20.6 KiB, free 434.2 MiB)
2025-04-30T12:26:42,807 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 434.1 MiB)
2025-04-30T12:26:42,808 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on bb9210fe839d:42343 (size: 8.5 KiB, free: 434.4 MiB)
2025-04-30T12:26:42,809 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 1 from broadcast at DAGScheduler.scala:1585
2025-04-30T12:26:42,831 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T12:26:42,834 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 0.0 with 1 tasks resource profile 0
2025-04-30T12:26:42,868 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.4, executor 2, partition 0, PROCESS_LOCAL, 9657 bytes) 
2025-04-30T12:26:42,895 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 0
2025-04-30T12:26:42,904 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 0.0 (TID 0)
2025-04-30T12:26:43,031 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:26:43,087 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:42343 after 3 ms (0 ms spent in bootstraps)
2025-04-30T12:26:43,138 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 434.4 MiB)
2025-04-30T12:26:43,145 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on 172.18.0.4:46127 (size: 8.5 KiB, free: 434.4 MiB)
2025-04-30T12:26:43,150 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 1 took 118 ms
2025-04-30T12:26:43,227 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 20.6 KiB, free 434.4 MiB)
2025-04-30T12:26:43,845 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 220.529408 ms
2025-04-30T12:26:43,849 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY%20copy.jsonl, range: 0-35354, partition values: [empty row]
2025-04-30T12:26:43,960 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 34.843339 ms
2025-04-30T12:26:43,979 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:26:43,990 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.3 MiB)
2025-04-30T12:26:43,993 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on 172.18.0.4:46127 (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T12:26:43,996 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 0 took 16 ms
2025-04-30T12:26:44,073 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T12:26:44,237 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 0.0 (TID 0). 2704 bytes result sent to driver
2025-04-30T12:26:44,251 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 0.0 (TID 0) in 1397 ms on 172.18.0.4 (executor 2) (1/1)
2025-04-30T12:26:44,254 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-04-30T12:26:44,259 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 0 (showString at <unknown>:0) finished in 1.528 s
2025-04-30T12:26:44,264 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T12:26:44,265 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 0: Stage finished
2025-04-30T12:26:44,269 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 finished: showString at <unknown>:0, took 1.576676 s
2025-04-30T12:26:44,591 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on bb9210fe839d:42343 in memory (size: 8.5 KiB, free: 434.4 MiB)
2025-04-30T12:26:44,593 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on 172.18.0.4:46127 in memory (size: 8.5 KiB, free: 434.4 MiB)
2025-04-30T12:26:45,258 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 40.298211 ms
2025-04-30T12:26:45,356 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T12:26:45,357 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T12:26:45,455 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 12.786239 ms
2025-04-30T12:26:45,463 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 200.1 KiB, free 434.0 MiB)
2025-04-30T12:26:45,480 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.9 MiB)
2025-04-30T12:26:45,486 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on bb9210fe839d:42343 (size: 34.4 KiB, free: 434.3 MiB)
2025-04-30T12:26:45,490 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 2 from count at <unknown>:0
2025-04-30T12:26:45,496 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T12:26:45,546 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Registering RDD 7 (count at <unknown>:0) as input to shuffle 0
2025-04-30T12:26:45,550 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got map stage job 1 (count at <unknown>:0) with 2 output partitions
2025-04-30T12:26:45,551 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ShuffleMapStage 1 (count at <unknown>:0)
2025-04-30T12:26:45,553 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T12:26:45,556 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T12:26:45,562 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0), which has no missing parents
2025-04-30T12:26:45,585 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 433.9 MiB)
2025-04-30T12:26:45,597 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 433.9 MiB)
2025-04-30T12:26:45,599 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on bb9210fe839d:42343 (size: 8.3 KiB, free: 434.3 MiB)
2025-04-30T12:26:45,600 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 3 from broadcast at DAGScheduler.scala:1585
2025-04-30T12:26:45,604 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
2025-04-30T12:26:45,605 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 1.0 with 2 tasks resource profile 0
2025-04-30T12:26:45,610 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.4, executor 2, partition 0, PROCESS_LOCAL, 9646 bytes) 
2025-04-30T12:26:45,611 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 1.0 in stage 1.0 (TID 2) (172.18.0.3, executor 1, partition 1, PROCESS_LOCAL, 9639 bytes) 
2025-04-30T12:26:45,615 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 1
2025-04-30T12:26:45,616 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 1.0 (TID 1)
2025-04-30T12:26:45,625 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 2
2025-04-30T12:26:45,641 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Running task 1.0 in stage 1.0 (TID 2)
2025-04-30T12:26:45,656 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:26:45,667 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 434.0 MiB)
2025-04-30T12:26:45,671 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on 172.18.0.4:46127 (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T12:26:45,675 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 3 took 18 ms
2025-04-30T12:26:45,677 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 434.0 MiB)
2025-04-30T12:26:45,740 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 17.969397 ms
2025-04-30T12:26:45,750 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY%20copy.jsonl, range: 0-35354, partition values: [empty row]
2025-04-30T12:26:45,762 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:26:45,763 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 9.934096 ms
2025-04-30T12:26:45,766 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:26:45,776 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.9 MiB)
2025-04-30T12:26:45,780 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on 172.18.0.4:46127 (size: 34.4 KiB, free: 434.3 MiB)
2025-04-30T12:26:45,782 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 2 took 16 ms
2025-04-30T12:26:45,803 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 377.3 KiB, free 433.6 MiB)
2025-04-30T12:26:45,813 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.4:46127 after 2 ms (0 ms spent in bootstraps)
2025-04-30T12:26:45,879 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 434.4 MiB)
2025-04-30T12:26:45,885 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on 172.18.0.3:36777 (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T12:26:45,890 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 3 took 127 ms
2025-04-30T12:26:45,899 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 1.0 (TID 1). 2007 bytes result sent to driver
2025-04-30T12:26:45,910 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 1.0 (TID 1) in 301 ms on 172.18.0.4 (executor 2) (1/2)
2025-04-30T12:26:45,945 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 434.4 MiB)
2025-04-30T12:26:46,527 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 189.048736 ms
2025-04-30T12:26:46,538 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl, range: 0-35354, partition values: [empty row]
2025-04-30T12:26:46,572 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 12.673933 ms
2025-04-30T12:26:46,590 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:26:46,597 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:42343 after 1 ms (0 ms spent in bootstraps)
2025-04-30T12:26:46,605 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.3 MiB)
2025-04-30T12:26:46,608 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on 172.18.0.3:36777 (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T12:26:46,610 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 2 took 19 ms
2025-04-30T12:26:46,678 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T12:26:46,877 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Finished task 1.0 in stage 1.0 (TID 2). 2007 bytes result sent to driver
2025-04-30T12:26:46,885 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 1.0 in stage 1.0 (TID 2) in 1275 ms on 172.18.0.3 (executor 1) (2/2)
2025-04-30T12:26:46,886 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-04-30T12:26:46,888 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ShuffleMapStage 1 (count at <unknown>:0) finished in 1.317 s
2025-04-30T12:26:46,890 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - looking for newly runnable stages
2025-04-30T12:26:46,891 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - running: Set()
2025-04-30T12:26:46,892 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - waiting: Set()
2025-04-30T12:26:46,894 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - failed: Set()
2025-04-30T12:26:46,953 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 14.993249 ms
2025-04-30T12:26:46,988 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: count at <unknown>:0
2025-04-30T12:26:46,991 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 2 (count at <unknown>:0) with 1 output partitions
2025-04-30T12:26:46,992 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 3 (count at <unknown>:0)
2025-04-30T12:26:46,992 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List(ShuffleMapStage 2)
2025-04-30T12:26:46,993 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T12:26:46,995 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0), which has no missing parents
2025-04-30T12:26:47,008 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 433.9 MiB)
2025-04-30T12:26:47,024 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 433.9 MiB)
2025-04-30T12:26:47,027 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_4_piece0 in memory on bb9210fe839d:42343 (size: 6.0 KiB, free: 434.3 MiB)
2025-04-30T12:26:47,029 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_3_piece0 on bb9210fe839d:42343 in memory (size: 8.3 KiB, free: 434.3 MiB)
2025-04-30T12:26:47,029 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 4 from broadcast at DAGScheduler.scala:1585
2025-04-30T12:26:47,034 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T12:26:47,035 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_3_piece0 on 172.18.0.4:46127 in memory (size: 8.3 KiB, free: 434.3 MiB)
2025-04-30T12:26:47,042 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 3.0 with 1 tasks resource profile 0
2025-04-30T12:26:47,045 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_3_piece0 on 172.18.0.3:36777 in memory (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T12:26:47,053 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 3.0 (TID 3) (172.18.0.4, executor 2, partition 0, NODE_LOCAL, 9003 bytes) 
2025-04-30T12:26:47,058 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 3
2025-04-30T12:26:47,060 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 3.0 (TID 3)
2025-04-30T12:26:47,071 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2025-04-30T12:26:47,075 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:26:47,088 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 433.6 MiB)
2025-04-30T12:26:47,091 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_4_piece0 in memory on 172.18.0.4:46127 (size: 6.0 KiB, free: 434.3 MiB)
2025-04-30T12:26:47,095 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 4 took 18 ms
2025-04-30T12:26:47,097 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 433.6 MiB)
2025-04-30T12:26:47,112 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Don't have map outputs for shuffle 0, fetching them
2025-04-30T12:26:47,114 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@bb9210fe839d:33503)
2025-04-30T12:26:47,122 [dispatcher-event-loop-9] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - Asked to send map output locations for shuffle 0 to 172.18.0.4:33558
2025-04-30T12:26:47,188 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Got the map output locations
2025-04-30T12:26:47,191 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_0_piece0 on bb9210fe839d:42343 in memory (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T12:26:47,196 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_0_piece0 on 172.18.0.4:46127 in memory (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T12:26:47,233 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Getting 2 (120.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 1 (60.0 B) remote blocks
2025-04-30T12:26:47,241 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.3:36777 after 1 ms (0 ms spent in bootstraps)
2025-04-30T12:26:47,247 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Started 1 remote fetches in 30 ms
2025-04-30T12:26:47,274 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 16.874242 ms
2025-04-30T12:26:47,289 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 3.0 (TID 3). 4038 bytes result sent to driver
2025-04-30T12:26:47,294 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 3.0 (TID 3) in 245 ms on 172.18.0.4 (executor 2) (1/1)
2025-04-30T12:26:47,294 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2025-04-30T12:26:47,297 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 3 (count at <unknown>:0) finished in 0.292 s
2025-04-30T12:26:47,298 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T12:26:47,298 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 3: Stage finished
2025-04-30T12:26:47,300 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 2 finished: count at <unknown>:0, took 0.310398 s
2025-04-30T12:26:47,307 [Thread-4] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-30T12:26:47,319 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@7cdbf7d{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T12:26:47,324 [Thread-4] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://bb9210fe839d:4040
2025-04-30T12:26:47,330 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-30T12:26:47,331 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-30T12:26:47,339 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:26:47,340 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:26:47,341 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:26:47,345 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250430122636-0016
2025-04-30T12:26:47,345 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250430122636-0016
2025-04-30T12:26:47,353 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430122636-0016/2
2025-04-30T12:26:47,356 [ExecutorRunner for app-20250430122636-0016/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430122636-0016/2 interrupted
2025-04-30T12:26:47,358 [ExecutorRunner for app-20250430122636-0016/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:26:47,359 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430122636-0016/1
2025-04-30T12:26:47,361 [ExecutorRunner for app-20250430122636-0016/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430122636-0016/1 interrupted
2025-04-30T12:26:47,363 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:26:47,362 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430122636-0016/0
2025-04-30T12:26:47,364 [ExecutorRunner for app-20250430122636-0016/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:26:47,365 [ExecutorRunner for app-20250430122636-0016/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430122636-0016/0 interrupted
2025-04-30T12:26:47,369 [ExecutorRunner for app-20250430122636-0016/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:26:47,369 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:26:47,376 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:26:47,377 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:26:47,381 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:26:47,382 [shutdown-hook-0] INFO  org.apache.spark.storage.DiskBlockManager [] - Shutdown hook called
2025-04-30T12:26:47,383 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:26:47,389 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:26:47,390 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:26:47,392 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:26:47,399 [dispatcher-event-loop-0] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-30T12:26:47,399 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:26:47,401 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:26:47,400 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:26:47,425 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:26:47,426 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:26:47,430 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430122636-0016/2 finished with state KILLED exitStatus 143
2025-04-30T12:26:47,431 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-30T12:26:47,431 [dispatcher-event-loop-3] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-30T12:26:47,432 [dispatcher-event-loop-9] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430122636-0016/2
2025-04-30T12:26:47,432 [dispatcher-event-loop-3] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430122636-0016, execId=2)
2025-04-30T12:26:47,434 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430122636-0016
2025-04-30T12:26:47,434 [dispatcher-event-loop-3] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430122636-0016 removed, cleanupLocalDirs = true
2025-04-30T12:26:47,437 [dispatcher-event-loop-11] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-30T12:26:47,442 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430122636-0016/0 finished with state KILLED exitStatus 143
2025-04-30T12:26:47,442 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:52504 got disassociated, removing it.
2025-04-30T12:26:47,443 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - bb9210fe839d:33503 got disassociated, removing it.
2025-04-30T12:26:47,443 [dispatcher-event-loop-3] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-30T12:26:47,444 [dispatcher-event-loop-11] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430122636-0016/0
2025-04-30T12:26:47,444 [dispatcher-event-loop-3] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430122636-0016, execId=0)
2025-04-30T12:26:47,445 [dispatcher-event-loop-3] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430122636-0016 removed, cleanupLocalDirs = true
2025-04-30T12:26:47,445 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430122636-0016
2025-04-30T12:26:47,453 [Thread-4] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-30T12:26:47,461 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430122636-0016/1 finished with state KILLED exitStatus 143
2025-04-30T12:26:47,461 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-30T12:26:47,462 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430122636-0016, execId=1)
2025-04-30T12:26:47,462 [dispatcher-event-loop-10] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430122636-0016/1
2025-04-30T12:26:47,462 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430122636-0016 removed, cleanupLocalDirs = true
2025-04-30T12:26:47,463 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430122636-0016
2025-04-30T12:26:47,723 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:26:47,725 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-852e43de-6b20-453d-b60a-ba275716ac84
2025-04-30T12:26:47,729 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-e0d433be-ee67-4001-9731-693a82800de6/pyspark-ab6d9c0c-7ab2-444c-8733-612ed410511d
2025-04-30T12:26:47,734 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-e0d433be-ee67-4001-9731-693a82800de6
2025-04-30T12:28:57,662 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-30T12:28:57,667 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:28:57,668 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-30T12:28:57,700 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:28:57,701 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-30T12:28:57,702 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:28:57,703 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: mod-2-pr-4-data-ingestion.py
2025-04-30T12:28:57,731 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-30T12:28:57,741 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-30T12:28:57,743 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-30T12:28:57,803 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-30T12:28:57,805 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-30T12:28:57,806 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:28:57,808 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:28:57,810 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-30T12:28:57,878 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:28:58,271 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 32893.
2025-04-30T12:28:58,316 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-30T12:28:58,371 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-30T12:28:58,393 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-30T12:28:58,394 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-30T12:28:58,399 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-30T12:28:58,421 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-934f94b0-1576-456a-a073-ba9461c6d16b
2025-04-30T12:28:58,437 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:28:58,457 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-30T12:28:58,507 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3910ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-30T12:28:58,612 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-30T12:28:58,625 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-30T12:28:58,650 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @4054ms
2025-04-30T12:28:58,685 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@413b8567{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T12:28:58,686 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-30T12:28:58,715 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1320cce0{/,null,AVAILABLE,@Spark}
2025-04-30T12:28:58,846 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-30T12:28:58,908 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 37 ms (0 ms spent in bootstraps)
2025-04-30T12:28:59,003 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Registering app mod-2-pr-4-data-ingestion.py
2025-04-30T12:28:59,005 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Registered app mod-2-pr-4-data-ingestion.py with ID app-20250430122859-0017
2025-04-30T12:28:59,006 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430122859-0017 with rpId: 0
2025-04-30T12:28:59,008 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430122859-0017/0 on worker worker-20250429114342-172.18.0.5-38821
2025-04-30T12:28:59,009 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430122859-0017/1 on worker worker-20250429114342-172.18.0.3-34443
2025-04-30T12:28:59,011 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430122859-0017/2 on worker worker-20250429114342-172.18.0.4-36391
2025-04-30T12:28:59,013 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430122859-0017/0 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:28:59,018 [ExecutorRunner for app-20250430122859-0017/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:28:59,019 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430122859-0017/1 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:28:59,019 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430122859-0017/2 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:28:59,019 [dispatcher-event-loop-10] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250430122859-0017
2025-04-30T12:28:59,020 [ExecutorRunner for app-20250430122859-0017/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:28:59,022 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430122859-0017/0 on worker-20250429114342-172.18.0.5-38821 (172.18.0.5:38821) with 2 core(s)
2025-04-30T12:28:59,022 [ExecutorRunner for app-20250430122859-0017/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:28:59,023 [ExecutorRunner for app-20250430122859-0017/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:28:59,024 [ExecutorRunner for app-20250430122859-0017/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:28:59,026 [dispatcher-event-loop-10] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430122859-0017/0 on hostPort 172.18.0.5:38821 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:28:59,026 [ExecutorRunner for app-20250430122859-0017/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:28:59,027 [ExecutorRunner for app-20250430122859-0017/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:28:59,027 [ExecutorRunner for app-20250430122859-0017/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:28:59,027 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430122859-0017/1 on worker-20250429114342-172.18.0.3-34443 (172.18.0.3:34443) with 2 core(s)
2025-04-30T12:28:59,029 [ExecutorRunner for app-20250430122859-0017/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:28:59,030 [ExecutorRunner for app-20250430122859-0017/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:28:59,031 [ExecutorRunner for app-20250430122859-0017/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:28:59,031 [ExecutorRunner for app-20250430122859-0017/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:28:59,031 [dispatcher-event-loop-10] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430122859-0017/1 on hostPort 172.18.0.3:34443 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:28:59,032 [ExecutorRunner for app-20250430122859-0017/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:28:59,032 [ExecutorRunner for app-20250430122859-0017/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:28:59,033 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430122859-0017/2 on worker-20250429114342-172.18.0.4-36391 (172.18.0.4:36391) with 2 core(s)
2025-04-30T12:28:59,033 [ExecutorRunner for app-20250430122859-0017/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:28:59,034 [dispatcher-event-loop-10] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430122859-0017/2 on hostPort 172.18.0.4:36391 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:28:59,039 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37749.
2025-04-30T12:28:59,040 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on bb9210fe839d:37749
2025-04-30T12:28:59,046 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:28:59,055 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, bb9210fe839d, 37749, None)
2025-04-30T12:28:59,060 [ExecutorRunner for app-20250430122859-0017/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=32893" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:32893" "--executor-id" "0" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250430122859-0017" "--worker-url" "spark://Worker@172.18.0.5:38821" "--resourceProfileId" "0"
2025-04-30T12:28:59,062 [ExecutorRunner for app-20250430122859-0017/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=32893" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:32893" "--executor-id" "2" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250430122859-0017" "--worker-url" "spark://Worker@172.18.0.4:36391" "--resourceProfileId" "0"
2025-04-30T12:28:59,063 [ExecutorRunner for app-20250430122859-0017/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=32893" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:32893" "--executor-id" "1" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250430122859-0017" "--worker-url" "spark://Worker@172.18.0.3:34443" "--resourceProfileId" "0"
2025-04-30T12:28:59,064 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager bb9210fe839d:37749 with 434.4 MiB RAM, BlockManagerId(driver, bb9210fe839d, 37749, None)
2025-04-30T12:28:59,077 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, bb9210fe839d, 37749, None)
2025-04-30T12:28:59,077 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430122859-0017 with rpId: 0
2025-04-30T12:28:59,081 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, bb9210fe839d, 37749, None)
2025-04-30T12:28:59,082 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430122859-0017 with rpId: 0
2025-04-30T12:28:59,084 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430122859-0017 with rpId: 0
2025-04-30T12:28:59,115 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430122859-0017/0 is now RUNNING
2025-04-30T12:28:59,118 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430122859-0017/2 is now RUNNING
2025-04-30T12:28:59,125 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430122859-0017/1 is now RUNNING
2025-04-30T12:28:59,583 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250430122859-0017.inprogress
2025-04-30T12:28:59,847 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@1320cce0{/,null,STOPPED,@Spark}
2025-04-30T12:28:59,850 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40857667{/jobs,null,AVAILABLE,@Spark}
2025-04-30T12:28:59,853 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7fa04fb2{/jobs/json,null,AVAILABLE,@Spark}
2025-04-30T12:28:59,856 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@35083451{/jobs/job,null,AVAILABLE,@Spark}
2025-04-30T12:28:59,860 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@354b22b6{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-30T12:28:59,863 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@484d933e{/stages,null,AVAILABLE,@Spark}
2025-04-30T12:28:59,867 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7274d8f2{/stages/json,null,AVAILABLE,@Spark}
2025-04-30T12:28:59,872 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2dab95f3{/stages/stage,null,AVAILABLE,@Spark}
2025-04-30T12:28:59,876 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4622b9f0{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-30T12:28:59,880 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@611e343f{/stages/pool,null,AVAILABLE,@Spark}
2025-04-30T12:28:59,884 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@56578f8c{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-30T12:28:59,890 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@fa43f57{/storage,null,AVAILABLE,@Spark}
2025-04-30T12:28:59,893 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2a9cdf28{/storage/json,null,AVAILABLE,@Spark}
2025-04-30T12:28:59,896 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@41d8187c{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-30T12:28:59,906 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@251ea385{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-30T12:28:59,910 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@d678b60{/environment,null,AVAILABLE,@Spark}
2025-04-30T12:28:59,916 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@ef17b66{/environment/json,null,AVAILABLE,@Spark}
2025-04-30T12:28:59,921 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@58ad57bd{/executors,null,AVAILABLE,@Spark}
2025-04-30T12:28:59,925 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1cadc900{/executors/json,null,AVAILABLE,@Spark}
2025-04-30T12:28:59,927 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40aadd59{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-30T12:28:59,932 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3eeb6954{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-30T12:28:59,935 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1964fe85{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-30T12:28:59,941 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@52124300{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-30T12:28:59,962 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@76c527e1{/static,null,AVAILABLE,@Spark}
2025-04-30T12:28:59,964 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@548a25dc{/,null,AVAILABLE,@Spark}
2025-04-30T12:28:59,969 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@28825700{/api,null,AVAILABLE,@Spark}
2025-04-30T12:28:59,975 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@611d70f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-30T12:28:59,979 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@21e7c069{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-30T12:29:00,001 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@64ddd77d{/metrics/json,null,AVAILABLE,@Spark}
2025-04-30T12:29:00,008 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-30T12:29:00,503 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-04-30T12:29:00,507 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
2025-04-30T12:29:00,538 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@739d96e{/SQL,null,AVAILABLE,@Spark}
2025-04-30T12:29:00,541 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@113354b6{/SQL/json,null,AVAILABLE,@Spark}
2025-04-30T12:29:00,549 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7bb072e9{/SQL/execution,null,AVAILABLE,@Spark}
2025-04-30T12:29:00,554 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2dd2a8f2{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-04-30T12:29:00,577 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@9867130{/static/sql,null,AVAILABLE,@Spark}
2025-04-30T12:29:01,506 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 1122@c27de723f0d4
2025-04-30T12:29:01,527 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:29:01,529 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:29:01,530 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:29:01,671 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 1191@066f03cf7e53
2025-04-30T12:29:01,683 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 1169@6cca10b5c892
2025-04-30T12:29:01,697 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:29:01,699 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:29:01,700 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:29:01,704 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:29:01,706 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:29:01,707 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:29:02,346 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:29:02,521 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:29:02,583 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:29:02,585 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:29:02,587 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:29:02,591 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:29:02,595 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:29:02,631 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:29:02,693 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 102 ms to list leaf files for 2 paths.
2025-04-30T12:29:02,753 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:29:02,756 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:29:02,759 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:29:02,762 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:29:02,769 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:29:02,856 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:29:02,857 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:29:02,860 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:29:02,863 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:29:02,866 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:29:03,199 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:32893 after 132 ms (0 ms spent in bootstraps)
2025-04-30T12:29:03,412 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:32893 after 138 ms (0 ms spent in bootstraps)
2025-04-30T12:29:03,413 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:29:03,414 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:29:03,415 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:29:03,416 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:29:03,417 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:29:03,478 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:32893 after 149 ms (0 ms spent in bootstraps)
2025-04-30T12:29:03,552 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:32893 after 22 ms (0 ms spent in bootstraps)
2025-04-30T12:29:03,694 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:29:03,695 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:29:03,696 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:29:03,697 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:29:03,697 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:29:03,708 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-00d4d2fa-4667-4dcc-a4a7-1a25b94c3d06/executor-d712d661-242e-4ce8-84b4-8b7ab0584d2f/blockmgr-9c53f2ed-5d84-489f-8956-4d0c97153b89
2025-04-30T12:29:03,710 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:29:03,711 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:29:03,712 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:29:03,713 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:29:03,714 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:29:03,767 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:29:03,788 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:32893 after 9 ms (0 ms spent in bootstraps)
2025-04-30T12:29:03,809 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:32893 after 8 ms (0 ms spent in bootstraps)
2025-04-30T12:29:03,907 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-c5ae99e1-4d9a-42b7-9555-210f29b2c4bb/executor-b1936b42-8772-4ef6-8d9e-63a4ed5273f5/blockmgr-dda22464-bf76-4090-bf66-6dd2cacd9238
2025-04-30T12:29:03,912 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-8bb716ea-3eee-4da7-a70b-3a99059b4da9/executor-260df6cd-986a-45d2-83cb-5aacf60cc8a6/blockmgr-cfab1b99-47d6-4e6f-83f1-eac45d26ec1f
2025-04-30T12:29:03,959 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:29:03,963 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:29:04,102 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:32893
2025-04-30T12:29:04,103 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.3:34443
2025-04-30T12:29:04,118 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.3:34443 after 5 ms (0 ms spent in bootstraps)
2025-04-30T12:29:04,126 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.3:34443
2025-04-30T12:29:04,127 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:29:04,130 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:29:04,131 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:29:04,182 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:42600) with ID 1,  ResourceProfileId 0
2025-04-30T12:29:04,207 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:29:04,220 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 1 on host 172.18.0.3
2025-04-30T12:29:04,222 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:29:04,223 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:29:04,365 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:32893
2025-04-30T12:29:04,363 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43011.
2025-04-30T12:29:04,367 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.5:38821
2025-04-30T12:29:04,367 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.3:43011
2025-04-30T12:29:04,372 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:29:04,387 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.5:38821 after 11 ms (0 ms spent in bootstraps)
2025-04-30T12:29:04,390 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(1, 172.18.0.3, 43011, None)
2025-04-30T12:29:04,395 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.5:38821
2025-04-30T12:29:04,397 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:29:04,399 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:29:04,401 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:29:04,414 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.3:43011 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.3, 43011, None)
2025-04-30T12:29:04,423 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(1, 172.18.0.3, 43011, None)
2025-04-30T12:29:04,427 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(1, 172.18.0.3, 43011, None)
2025-04-30T12:29:04,461 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:29:04,463 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:55234) with ID 0,  ResourceProfileId 0
2025-04-30T12:29:04,471 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@617f904f for default.
2025-04-30T12:29:04,477 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:32893
2025-04-30T12:29:04,478 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.4:36391
2025-04-30T12:29:04,482 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:29:04,491 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 0 on host 172.18.0.5
2025-04-30T12:29:04,495 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.4:36391 after 6 ms (0 ms spent in bootstraps)
2025-04-30T12:29:04,498 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:29:04,501 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:29:04,503 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:29:04,513 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:29:04,517 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:29:04,526 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.4:36391
2025-04-30T12:29:04,589 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:58132) with ID 2,  ResourceProfileId 0
2025-04-30T12:29:04,596 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42235.
2025-04-30T12:29:04,598 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:29:04,599 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.5:42235
2025-04-30T12:29:04,602 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:29:04,604 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 2 on host 172.18.0.4
2025-04-30T12:29:04,606 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:29:04,608 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:29:04,614 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(0, 172.18.0.5, 42235, None)
2025-04-30T12:29:04,629 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.5:42235 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.5, 42235, None)
2025-04-30T12:29:04,641 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(0, 172.18.0.5, 42235, None)
2025-04-30T12:29:04,645 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(0, 172.18.0.5, 42235, None)
2025-04-30T12:29:04,657 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:29:04,661 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@51b1068 for default.
2025-04-30T12:29:04,677 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40499.
2025-04-30T12:29:04,679 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.4:40499
2025-04-30T12:29:04,682 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:29:04,693 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(2, 172.18.0.4, 40499, None)
2025-04-30T12:29:04,702 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.4:40499 with 434.4 MiB RAM, BlockManagerId(2, 172.18.0.4, 40499, None)
2025-04-30T12:29:04,709 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(2, 172.18.0.4, 40499, None)
2025-04-30T12:29:04,711 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(2, 172.18.0.4, 40499, None)
2025-04-30T12:29:04,717 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:29:04,719 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@51b1068 for default.
2025-04-30T12:29:04,944 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T12:29:04,947 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T12:29:05,457 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 233.777189 ms
2025-04-30T12:29:05,518 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 200.1 KiB, free 434.2 MiB)
2025-04-30T12:29:05,579 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.2 MiB)
2025-04-30T12:29:05,583 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on bb9210fe839d:37749 (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T12:29:05,589 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 0 from showString at <unknown>:0
2025-04-30T12:29:05,625 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T12:29:05,739 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: showString at <unknown>:0
2025-04-30T12:29:05,756 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 0 (showString at <unknown>:0) with 1 output partitions
2025-04-30T12:29:05,757 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 0 (showString at <unknown>:0)
2025-04-30T12:29:05,758 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T12:29:05,760 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T12:29:05,766 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 0 (MapPartitionsRDD[3] at showString at <unknown>:0), which has no missing parents
2025-04-30T12:29:05,850 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 20.6 KiB, free 434.2 MiB)
2025-04-30T12:29:05,853 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 434.1 MiB)
2025-04-30T12:29:05,855 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on bb9210fe839d:37749 (size: 8.5 KiB, free: 434.4 MiB)
2025-04-30T12:29:05,856 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 1 from broadcast at DAGScheduler.scala:1585
2025-04-30T12:29:05,873 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T12:29:05,875 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 0.0 with 1 tasks resource profile 0
2025-04-30T12:29:05,910 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 9657 bytes) 
2025-04-30T12:29:05,934 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 0
2025-04-30T12:29:05,946 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 0.0 (TID 0)
2025-04-30T12:29:06,060 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:29:06,106 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:37749 after 3 ms (0 ms spent in bootstraps)
2025-04-30T12:29:06,157 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 434.4 MiB)
2025-04-30T12:29:06,168 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on 172.18.0.5:42235 (size: 8.5 KiB, free: 434.4 MiB)
2025-04-30T12:29:06,174 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 1 took 113 ms
2025-04-30T12:29:06,230 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 20.6 KiB, free 434.4 MiB)
2025-04-30T12:29:06,926 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 306.836809 ms
2025-04-30T12:29:06,930 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY%20copy.jsonl, range: 0-35354, partition values: [empty row]
2025-04-30T12:29:07,052 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 31.983393 ms
2025-04-30T12:29:07,073 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:29:07,084 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.3 MiB)
2025-04-30T12:29:07,087 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on 172.18.0.5:42235 (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T12:29:07,090 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 0 took 16 ms
2025-04-30T12:29:07,177 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T12:29:07,370 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 0.0 (TID 0). 2704 bytes result sent to driver
2025-04-30T12:29:07,386 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 0.0 (TID 0) in 1490 ms on 172.18.0.5 (executor 0) (1/1)
2025-04-30T12:29:07,389 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-04-30T12:29:07,394 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 0 (showString at <unknown>:0) finished in 1.609 s
2025-04-30T12:29:07,397 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T12:29:07,399 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 0: Stage finished
2025-04-30T12:29:07,401 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 finished: showString at <unknown>:0, took 1.661807 s
2025-04-30T12:29:07,701 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on 172.18.0.5:42235 in memory (size: 8.5 KiB, free: 434.4 MiB)
2025-04-30T12:29:07,703 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on bb9210fe839d:37749 in memory (size: 8.5 KiB, free: 434.4 MiB)
2025-04-30T12:29:08,516 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 36.38174 ms
2025-04-30T12:29:08,615 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T12:29:08,615 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T12:29:08,726 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 12.235387 ms
2025-04-30T12:29:08,733 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 200.1 KiB, free 434.0 MiB)
2025-04-30T12:29:08,746 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.9 MiB)
2025-04-30T12:29:08,748 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on bb9210fe839d:37749 (size: 34.4 KiB, free: 434.3 MiB)
2025-04-30T12:29:08,750 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 2 from count at <unknown>:0
2025-04-30T12:29:08,754 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T12:29:08,785 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Registering RDD 7 (count at <unknown>:0) as input to shuffle 0
2025-04-30T12:29:08,791 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got map stage job 1 (count at <unknown>:0) with 2 output partitions
2025-04-30T12:29:08,793 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ShuffleMapStage 1 (count at <unknown>:0)
2025-04-30T12:29:08,794 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T12:29:08,796 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T12:29:08,798 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0), which has no missing parents
2025-04-30T12:29:08,820 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 433.9 MiB)
2025-04-30T12:29:08,828 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 433.9 MiB)
2025-04-30T12:29:08,829 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on bb9210fe839d:37749 (size: 8.3 KiB, free: 434.3 MiB)
2025-04-30T12:29:08,830 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 3 from broadcast at DAGScheduler.scala:1585
2025-04-30T12:29:08,833 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
2025-04-30T12:29:08,834 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 1.0 with 2 tasks resource profile 0
2025-04-30T12:29:08,838 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 9646 bytes) 
2025-04-30T12:29:08,841 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 1.0 in stage 1.0 (TID 2) (172.18.0.4, executor 2, partition 1, PROCESS_LOCAL, 9639 bytes) 
2025-04-30T12:29:08,846 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 1
2025-04-30T12:29:08,848 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 1.0 (TID 1)
2025-04-30T12:29:08,857 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 2
2025-04-30T12:29:08,869 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Running task 1.0 in stage 1.0 (TID 2)
2025-04-30T12:29:08,880 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:29:08,892 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 434.0 MiB)
2025-04-30T12:29:08,896 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on 172.18.0.5:42235 (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T12:29:08,899 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 3 took 18 ms
2025-04-30T12:29:08,902 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 434.0 MiB)
2025-04-30T12:29:08,983 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 18.372331 ms
2025-04-30T12:29:08,995 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY%20copy.jsonl, range: 0-35354, partition values: [empty row]
2025-04-30T12:29:09,010 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 11.553848 ms
2025-04-30T12:29:09,012 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:29:09,012 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:29:09,024 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.9 MiB)
2025-04-30T12:29:09,030 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on 172.18.0.5:42235 (size: 34.4 KiB, free: 434.3 MiB)
2025-04-30T12:29:09,033 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 2 took 20 ms
2025-04-30T12:29:09,058 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 377.3 KiB, free 433.6 MiB)
2025-04-30T12:29:09,082 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:37749 after 2 ms (0 ms spent in bootstraps)
2025-04-30T12:29:09,133 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 434.4 MiB)
2025-04-30T12:29:09,139 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on 172.18.0.4:40499 (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T12:29:09,145 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 3 took 131 ms
2025-04-30T12:29:09,160 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 1.0 (TID 1). 1964 bytes result sent to driver
2025-04-30T12:29:09,169 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 1.0 (TID 1) in 333 ms on 172.18.0.5 (executor 0) (1/2)
2025-04-30T12:29:09,212 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 434.4 MiB)
2025-04-30T12:29:09,793 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 205.765741 ms
2025-04-30T12:29:09,804 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl, range: 0-35354, partition values: [empty row]
2025-04-30T12:29:09,847 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 15.578373 ms
2025-04-30T12:29:09,865 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:29:09,872 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.5:42235 after 2 ms (0 ms spent in bootstraps)
2025-04-30T12:29:09,886 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.3 MiB)
2025-04-30T12:29:09,890 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on 172.18.0.4:40499 (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T12:29:09,892 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 2 took 27 ms
2025-04-30T12:29:09,970 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T12:29:10,164 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Finished task 1.0 in stage 1.0 (TID 2). 2007 bytes result sent to driver
2025-04-30T12:29:10,171 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 1.0 in stage 1.0 (TID 2) in 1331 ms on 172.18.0.4 (executor 2) (2/2)
2025-04-30T12:29:10,172 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-04-30T12:29:10,173 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ShuffleMapStage 1 (count at <unknown>:0) finished in 1.370 s
2025-04-30T12:29:10,175 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - looking for newly runnable stages
2025-04-30T12:29:10,176 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - running: Set()
2025-04-30T12:29:10,177 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - waiting: Set()
2025-04-30T12:29:10,178 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - failed: Set()
2025-04-30T12:29:10,224 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 13.919881 ms
2025-04-30T12:29:10,258 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: count at <unknown>:0
2025-04-30T12:29:10,261 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 2 (count at <unknown>:0) with 1 output partitions
2025-04-30T12:29:10,262 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 3 (count at <unknown>:0)
2025-04-30T12:29:10,262 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List(ShuffleMapStage 2)
2025-04-30T12:29:10,263 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T12:29:10,264 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0), which has no missing parents
2025-04-30T12:29:10,276 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 433.9 MiB)
2025-04-30T12:29:10,293 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 433.9 MiB)
2025-04-30T12:29:10,296 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_4_piece0 in memory on bb9210fe839d:37749 (size: 6.0 KiB, free: 434.3 MiB)
2025-04-30T12:29:10,297 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_3_piece0 on bb9210fe839d:37749 in memory (size: 8.3 KiB, free: 434.3 MiB)
2025-04-30T12:29:10,298 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 4 from broadcast at DAGScheduler.scala:1585
2025-04-30T12:29:10,301 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T12:29:10,305 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_3_piece0 on 172.18.0.5:42235 in memory (size: 8.3 KiB, free: 434.3 MiB)
2025-04-30T12:29:10,309 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 3.0 with 1 tasks resource profile 0
2025-04-30T12:29:10,317 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_3_piece0 on 172.18.0.4:40499 in memory (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T12:29:10,318 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 3.0 (TID 3) (172.18.0.4, executor 2, partition 0, NODE_LOCAL, 9003 bytes) 
2025-04-30T12:29:10,322 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 3
2025-04-30T12:29:10,324 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 3.0 (TID 3)
2025-04-30T12:29:10,334 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2025-04-30T12:29:10,354 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:29:10,364 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.0 MiB)
2025-04-30T12:29:10,367 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_4_piece0 in memory on 172.18.0.4:40499 (size: 6.0 KiB, free: 434.4 MiB)
2025-04-30T12:29:10,369 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 4 took 14 ms
2025-04-30T12:29:10,372 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 434.0 MiB)
2025-04-30T12:29:10,446 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Don't have map outputs for shuffle 0, fetching them
2025-04-30T12:29:10,448 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@bb9210fe839d:32893)
2025-04-30T12:29:10,454 [dispatcher-event-loop-5] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - Asked to send map output locations for shuffle 0 to 172.18.0.4:58132
2025-04-30T12:29:10,490 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Got the map output locations
2025-04-30T12:29:10,523 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Getting 2 (120.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 1 (60.0 B) remote blocks
2025-04-30T12:29:10,531 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Started 1 remote fetches in 23 ms
2025-04-30T12:29:10,560 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 20.256136 ms
2025-04-30T12:29:10,582 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 3.0 (TID 3). 4081 bytes result sent to driver
2025-04-30T12:29:10,589 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 3.0 (TID 3) in 274 ms on 172.18.0.4 (executor 2) (1/1)
2025-04-30T12:29:10,590 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2025-04-30T12:29:10,592 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 3 (count at <unknown>:0) finished in 0.319 s
2025-04-30T12:29:10,593 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T12:29:10,594 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 3: Stage finished
2025-04-30T12:29:10,595 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 2 finished: count at <unknown>:0, took 0.336219 s
2025-04-30T12:29:10,602 [Thread-4] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-30T12:29:10,615 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@413b8567{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T12:29:10,621 [Thread-4] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://bb9210fe839d:4040
2025-04-30T12:29:10,628 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-30T12:29:10,629 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-30T12:29:10,636 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:29:10,636 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:29:10,638 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:29:10,640 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250430122859-0017
2025-04-30T12:29:10,642 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250430122859-0017
2025-04-30T12:29:10,648 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430122859-0017/0
2025-04-30T12:29:10,649 [ExecutorRunner for app-20250430122859-0017/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430122859-0017/0 interrupted
2025-04-30T12:29:10,650 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430122859-0017/1
2025-04-30T12:29:10,651 [ExecutorRunner for app-20250430122859-0017/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430122859-0017/1 interrupted
2025-04-30T12:29:10,650 [ExecutorRunner for app-20250430122859-0017/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:29:10,652 [ExecutorRunner for app-20250430122859-0017/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:29:10,653 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430122859-0017/2
2025-04-30T12:29:10,660 [ExecutorRunner for app-20250430122859-0017/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430122859-0017/2 interrupted
2025-04-30T12:29:10,662 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:29:10,662 [ExecutorRunner for app-20250430122859-0017/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:29:10,669 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:29:10,670 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:29:10,675 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:29:10,674 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:29:10,678 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:29:10,681 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:29:10,683 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:29:10,688 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:29:10,691 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:29:10,693 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:29:10,697 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:29:10,711 [dispatcher-event-loop-9] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-30T12:29:10,736 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:29:10,737 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:29:10,745 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-30T12:29:10,751 [dispatcher-event-loop-11] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-30T12:29:10,751 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430122859-0017/0 finished with state KILLED exitStatus 143
2025-04-30T12:29:10,754 [dispatcher-event-loop-8] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430122859-0017/0
2025-04-30T12:29:10,753 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-30T12:29:10,760 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:43904 got disassociated, removing it.
2025-04-30T12:29:10,760 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430122859-0017, execId=0)
2025-04-30T12:29:10,761 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - bb9210fe839d:32893 got disassociated, removing it.
2025-04-30T12:29:10,761 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430122859-0017 removed, cleanupLocalDirs = true
2025-04-30T12:29:10,761 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430122859-0017
2025-04-30T12:29:10,766 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430122859-0017/1 finished with state KILLED exitStatus 143
2025-04-30T12:29:10,766 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-30T12:29:10,767 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430122859-0017, execId=1)
2025-04-30T12:29:10,767 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430122859-0017/2 finished with state KILLED exitStatus 143
2025-04-30T12:29:10,767 [dispatcher-event-loop-6] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430122859-0017/1
2025-04-30T12:29:10,768 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430122859-0017 removed, cleanupLocalDirs = true
2025-04-30T12:29:10,768 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-30T12:29:10,768 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430122859-0017
2025-04-30T12:29:10,769 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430122859-0017, execId=2)
2025-04-30T12:29:10,769 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430122859-0017
2025-04-30T12:29:10,769 [dispatcher-event-loop-11] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430122859-0017/2
2025-04-30T12:29:10,769 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430122859-0017 removed, cleanupLocalDirs = true
2025-04-30T12:29:10,771 [Thread-4] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-30T12:29:11,119 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:29:11,120 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-9e0028c1-6dc4-49b5-986d-1fd3feb77038/pyspark-9fe5e4c9-3e43-45ae-8218-853b74d3e13b
2025-04-30T12:29:11,124 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-02a47edf-6d36-4f83-9090-2f5979d275ce
2025-04-30T12:29:11,128 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-9e0028c1-6dc4-49b5-986d-1fd3feb77038
2025-04-30T12:58:52,988 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:58:52,995 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-c6d2bf56-4850-41aa-8bb4-cb4ecdd10db0
2025-04-30T12:59:30,142 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-30T12:59:30,147 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:59:30,148 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-30T12:59:30,178 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:59:30,178 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-30T12:59:30,180 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:59:30,181 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: mod-2-pr-4-data-ingestion.py
2025-04-30T12:59:30,202 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-30T12:59:30,214 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-30T12:59:30,215 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-30T12:59:30,266 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-30T12:59:30,267 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-30T12:59:30,268 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:59:30,270 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:59:30,271 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-30T12:59:30,336 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:59:30,626 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 34063.
2025-04-30T12:59:30,655 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-30T12:59:30,689 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-30T12:59:30,709 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-30T12:59:30,711 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-30T12:59:30,716 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-30T12:59:30,744 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-a501f292-b4cb-4bf7-b0a7-bc2dc7c62e4d
2025-04-30T12:59:30,761 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:59:30,781 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-30T12:59:30,832 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3310ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-30T12:59:30,930 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-30T12:59:30,943 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-30T12:59:30,965 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @3443ms
2025-04-30T12:59:30,998 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@413b8567{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T12:59:30,999 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-30T12:59:31,025 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1320cce0{/,null,AVAILABLE,@Spark}
2025-04-30T12:59:31,133 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-30T12:59:31,187 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 31 ms (0 ms spent in bootstraps)
2025-04-30T12:59:31,296 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Registering app mod-2-pr-4-data-ingestion.py
2025-04-30T12:59:31,313 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Registered app mod-2-pr-4-data-ingestion.py with ID app-20250430125931-0018
2025-04-30T12:59:31,316 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430125931-0018 with rpId: 0
2025-04-30T12:59:31,320 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430125931-0018/0 on worker worker-20250429114342-172.18.0.5-38821
2025-04-30T12:59:31,321 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430125931-0018/1 on worker worker-20250429114342-172.18.0.3-34443
2025-04-30T12:59:31,321 [dispatcher-event-loop-10] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250430125931-0018
2025-04-30T12:59:31,323 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430125931-0018/2 on worker worker-20250429114342-172.18.0.4-36391
2025-04-30T12:59:31,332 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430125931-0018/0 on worker-20250429114342-172.18.0.5-38821 (172.18.0.5:38821) with 2 core(s)
2025-04-30T12:59:31,336 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430125931-0018/0 on hostPort 172.18.0.5:38821 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:59:31,337 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430125931-0018/1 on worker-20250429114342-172.18.0.3-34443 (172.18.0.3:34443) with 2 core(s)
2025-04-30T12:59:31,339 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430125931-0018/1 on hostPort 172.18.0.3:34443 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:59:31,340 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430125931-0018/2 on worker-20250429114342-172.18.0.4-36391 (172.18.0.4:36391) with 2 core(s)
2025-04-30T12:59:31,342 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430125931-0018/2 on hostPort 172.18.0.4:36391 with 2 core(s), 1024.0 MiB RAM
2025-04-30T12:59:31,342 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42533.
2025-04-30T12:59:31,343 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on bb9210fe839d:42533
2025-04-30T12:59:31,347 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:59:31,352 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430125931-0018/0 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:59:31,354 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430125931-0018/2 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:59:31,355 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430125931-0018/1 for mod-2-pr-4-data-ingestion.py
2025-04-30T12:59:31,363 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, bb9210fe839d, 42533, None)
2025-04-30T12:59:31,369 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager bb9210fe839d:42533 with 434.4 MiB RAM, BlockManagerId(driver, bb9210fe839d, 42533, None)
2025-04-30T12:59:31,374 [ExecutorRunner for app-20250430125931-0018/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:59:31,376 [ExecutorRunner for app-20250430125931-0018/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:59:31,376 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, bb9210fe839d, 42533, None)
2025-04-30T12:59:31,377 [ExecutorRunner for app-20250430125931-0018/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:59:31,378 [ExecutorRunner for app-20250430125931-0018/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:59:31,378 [ExecutorRunner for app-20250430125931-0018/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:59:31,379 [ExecutorRunner for app-20250430125931-0018/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:59:31,379 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, bb9210fe839d, 42533, None)
2025-04-30T12:59:31,379 [ExecutorRunner for app-20250430125931-0018/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:59:31,379 [ExecutorRunner for app-20250430125931-0018/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:59:31,380 [ExecutorRunner for app-20250430125931-0018/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:59:31,381 [ExecutorRunner for app-20250430125931-0018/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:59:31,381 [ExecutorRunner for app-20250430125931-0018/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:59:31,381 [ExecutorRunner for app-20250430125931-0018/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:59:31,381 [ExecutorRunner for app-20250430125931-0018/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:59:31,382 [ExecutorRunner for app-20250430125931-0018/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:59:31,383 [ExecutorRunner for app-20250430125931-0018/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:59:31,437 [ExecutorRunner for app-20250430125931-0018/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=34063" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:34063" "--executor-id" "1" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250430125931-0018" "--worker-url" "spark://Worker@172.18.0.3:34443" "--resourceProfileId" "0"
2025-04-30T12:59:31,440 [ExecutorRunner for app-20250430125931-0018/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=34063" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:34063" "--executor-id" "0" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250430125931-0018" "--worker-url" "spark://Worker@172.18.0.5:38821" "--resourceProfileId" "0"
2025-04-30T12:59:31,444 [ExecutorRunner for app-20250430125931-0018/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=34063" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:34063" "--executor-id" "2" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250430125931-0018" "--worker-url" "spark://Worker@172.18.0.4:36391" "--resourceProfileId" "0"
2025-04-30T12:59:31,452 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430125931-0018 with rpId: 0
2025-04-30T12:59:31,458 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430125931-0018 with rpId: 0
2025-04-30T12:59:31,461 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430125931-0018 with rpId: 0
2025-04-30T12:59:31,483 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430125931-0018/1 is now RUNNING
2025-04-30T12:59:31,489 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430125931-0018/0 is now RUNNING
2025-04-30T12:59:31,496 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430125931-0018/2 is now RUNNING
2025-04-30T12:59:31,829 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250430125931-0018.inprogress
2025-04-30T12:59:32,054 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@1320cce0{/,null,STOPPED,@Spark}
2025-04-30T12:59:32,057 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40857667{/jobs,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,059 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7fa04fb2{/jobs/json,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,062 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@35083451{/jobs/job,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,064 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@354b22b6{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,067 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@484d933e{/stages,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,070 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7274d8f2{/stages/json,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,074 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2dab95f3{/stages/stage,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,077 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4622b9f0{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,079 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@611e343f{/stages/pool,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,082 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@56578f8c{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,085 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@fa43f57{/storage,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,088 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2a9cdf28{/storage/json,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,091 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@41d8187c{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,098 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@251ea385{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,101 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@d678b60{/environment,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,104 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@ef17b66{/environment/json,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,108 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@58ad57bd{/executors,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,111 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1cadc900{/executors/json,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,114 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40aadd59{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,118 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3eeb6954{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,121 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1964fe85{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,124 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@52124300{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,151 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@76c527e1{/static,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,155 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@548a25dc{/,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,160 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@28825700{/api,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,163 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@611d70f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,166 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@21e7c069{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,175 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@64ddd77d{/metrics/json,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,178 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-30T12:59:32,574 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-04-30T12:59:32,581 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
2025-04-30T12:59:32,611 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@739d96e{/SQL,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,618 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@113354b6{/SQL/json,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,622 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7bb072e9{/SQL/execution,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,626 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2dd2a8f2{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-04-30T12:59:32,633 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@9867130{/static/sql,null,AVAILABLE,@Spark}
2025-04-30T12:59:33,548 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 1208@c27de723f0d4
2025-04-30T12:59:33,568 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:59:33,570 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:59:33,571 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:59:33,594 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 1280@066f03cf7e53
2025-04-30T12:59:33,617 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:59:33,619 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:59:33,620 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:59:33,657 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 1256@6cca10b5c892
2025-04-30T12:59:33,671 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T12:59:33,673 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T12:59:33,674 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T12:59:34,313 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:59:34,359 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:59:34,361 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T12:59:34,532 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:59:34,536 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:59:34,543 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 160 ms to list leaf files for 2 paths.
2025-04-30T12:59:34,545 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:59:34,546 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:59:34,548 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:59:34,575 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:59:34,586 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:59:34,589 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:59:34,592 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:59:34,596 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:59:34,614 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:59:34,617 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:59:34,619 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:59:34,621 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:59:34,630 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:59:35,113 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:34063 after 96 ms (0 ms spent in bootstraps)
2025-04-30T12:59:35,115 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:34063 after 102 ms (0 ms spent in bootstraps)
2025-04-30T12:59:35,155 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:34063 after 108 ms (0 ms spent in bootstraps)
2025-04-30T12:59:35,300 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:59:35,301 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:59:35,302 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:59:35,303 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:59:35,304 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:59:35,305 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:59:35,306 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:59:35,308 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:59:35,308 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:59:35,309 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:59:35,327 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T12:59:35,328 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T12:59:35,330 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T12:59:35,331 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T12:59:35,331 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T12:59:35,407 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:34063 after 4 ms (0 ms spent in bootstraps)
2025-04-30T12:59:35,416 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:34063 after 12 ms (0 ms spent in bootstraps)
2025-04-30T12:59:35,416 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:34063 after 3 ms (0 ms spent in bootstraps)
2025-04-30T12:59:35,517 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-00d4d2fa-4667-4dcc-a4a7-1a25b94c3d06/executor-d9c1b204-11cb-40db-95c7-705232527c51/blockmgr-1f85961b-3cf7-463d-a56b-d31dec4311c6
2025-04-30T12:59:35,538 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-c5ae99e1-4d9a-42b7-9555-210f29b2c4bb/executor-c28edc5d-6217-48a9-937d-bd5231395d14/blockmgr-880d9240-2cc3-4e65-be5d-4dd9cce78387
2025-04-30T12:59:35,547 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-8bb716ea-3eee-4da7-a70b-3a99059b4da9/executor-dc1a9730-323f-4528-a980-7b377eef40d3/blockmgr-9fb9a02e-d22e-49f6-9b1e-2fd7fb182d8e
2025-04-30T12:59:35,577 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:59:35,594 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:59:35,602 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T12:59:35,904 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:34063
2025-04-30T12:59:35,915 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:34063
2025-04-30T12:59:35,918 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.3:34443
2025-04-30T12:59:35,919 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.5:38821
2025-04-30T12:59:35,925 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:59:35,927 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:59:35,940 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:59:35,945 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:59:35,955 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.5:38821 after 26 ms (0 ms spent in bootstraps)
2025-04-30T12:59:35,960 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:34063
2025-04-30T12:59:35,966 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.4:36391
2025-04-30T12:59:35,976 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:59:35,986 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.5:38821
2025-04-30T12:59:36,001 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.3:34443 after 75 ms (0 ms spent in bootstraps)
2025-04-30T12:59:36,010 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:59:36,005 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.3:34443
2025-04-30T12:59:36,011 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T12:59:36,005 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:59:36,016 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T12:59:36,048 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.4:36391 after 17 ms (0 ms spent in bootstraps)
2025-04-30T12:59:36,051 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:37766) with ID 0,  ResourceProfileId 0
2025-04-30T12:59:36,058 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.4:36391
2025-04-30T12:59:36,065 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:59:36,069 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:60566) with ID 1,  ResourceProfileId 0
2025-04-30T12:59:36,074 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 0 on host 172.18.0.5
2025-04-30T12:59:36,080 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:59:36,083 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:59:36,085 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:59:36,086 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:33278) with ID 2,  ResourceProfileId 0
2025-04-30T12:59:36,096 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 1 on host 172.18.0.3
2025-04-30T12:59:36,098 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T12:59:36,099 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:59:36,100 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:59:36,110 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 2 on host 172.18.0.4
2025-04-30T12:59:36,112 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T12:59:36,113 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T12:59:36,132 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 29 ms to list leaf files for 1 paths.
2025-04-30T12:59:36,164 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39619.
2025-04-30T12:59:36,165 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.5:39619
2025-04-30T12:59:36,170 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:59:36,183 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(0, 172.18.0.5, 39619, None)
2025-04-30T12:59:36,186 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43025.
2025-04-30T12:59:36,190 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33397.
2025-04-30T12:59:36,191 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.4:33397
2025-04-30T12:59:36,191 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.3:43025
2025-04-30T12:59:36,198 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:59:36,199 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T12:59:36,210 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(2, 172.18.0.4, 33397, None)
2025-04-30T12:59:36,212 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.5:39619 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.5, 39619, None)
2025-04-30T12:59:36,216 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(1, 172.18.0.3, 43025, None)
2025-04-30T12:59:36,221 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.4:33397 with 434.4 MiB RAM, BlockManagerId(2, 172.18.0.4, 33397, None)
2025-04-30T12:59:36,223 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(0, 172.18.0.5, 39619, None)
2025-04-30T12:59:36,227 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(0, 172.18.0.5, 39619, None)
2025-04-30T12:59:36,233 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(2, 172.18.0.4, 33397, None)
2025-04-30T12:59:36,236 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(2, 172.18.0.4, 33397, None)
2025-04-30T12:59:36,237 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.3:43025 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.3, 43025, None)
2025-04-30T12:59:36,248 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:59:36,250 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(1, 172.18.0.3, 43025, None)
2025-04-30T12:59:36,252 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1fb9f72d for default.
2025-04-30T12:59:36,254 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(1, 172.18.0.3, 43025, None)
2025-04-30T12:59:36,261 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:59:36,263 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 39 ms to list leaf files for 1 paths.
2025-04-30T12:59:36,267 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@51b1068 for default.
2025-04-30T12:59:36,277 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T12:59:36,281 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@150e95fd for default.
2025-04-30T12:59:37,483 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T12:59:37,486 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T12:59:37,630 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 200.3 KiB, free 434.2 MiB)
2025-04-30T12:59:37,676 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
2025-04-30T12:59:37,680 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on bb9210fe839d:42533 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T12:59:37,686 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 0 from json at <unknown>:0
2025-04-30T12:59:37,700 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T12:59:37,832 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: json at <unknown>:0
2025-04-30T12:59:37,850 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 0 (json at <unknown>:0) with 1 output partitions
2025-04-30T12:59:37,851 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 0 (json at <unknown>:0)
2025-04-30T12:59:37,853 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T12:59:37,855 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T12:59:37,860 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0), which has no missing parents
2025-04-30T12:59:37,945 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.5 KiB, free 434.2 MiB)
2025-04-30T12:59:37,955 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
2025-04-30T12:59:37,957 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on bb9210fe839d:42533 (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T12:59:37,959 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 1 from broadcast at DAGScheduler.scala:1585
2025-04-30T12:59:37,977 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T12:59:37,978 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 0.0 with 1 tasks resource profile 0
2025-04-30T12:59:38,007 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T12:59:38,040 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 0
2025-04-30T12:59:38,054 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 0.0 (TID 0)
2025-04-30T12:59:38,156 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:59:38,198 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:42533 after 2 ms (0 ms spent in bootstraps)
2025-04-30T12:59:38,248 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.4 MiB)
2025-04-30T12:59:38,256 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on 172.18.0.5:39619 (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T12:59:38,262 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 1 took 105 ms
2025-04-30T12:59:38,310 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.5 KiB, free 434.4 MiB)
2025-04-30T12:59:38,613 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl, range: 0-32688, partition values: [empty row]
2025-04-30T12:59:38,985 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 186.929961 ms
2025-04-30T12:59:38,987 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:59:38,996 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.3 MiB)
2025-04-30T12:59:38,999 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on 172.18.0.5:39619 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T12:59:39,001 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 0 took 13 ms
2025-04-30T12:59:39,081 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T12:59:39,239 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 0.0 (TID 0). 2450 bytes result sent to driver
2025-04-30T12:59:39,251 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 0.0 (TID 0) in 1254 ms on 172.18.0.5 (executor 0) (1/1)
2025-04-30T12:59:39,253 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-04-30T12:59:39,259 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 0 (json at <unknown>:0) finished in 1.380 s
2025-04-30T12:59:39,263 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T12:59:39,264 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 0: Stage finished
2025-04-30T12:59:39,268 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 finished: json at <unknown>:0, took 1.435012 s
2025-04-30T12:59:39,422 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T12:59:39,423 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T12:59:39,938 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 268.869115 ms
2025-04-30T12:59:39,946 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 200.2 KiB, free 434.0 MiB)
2025-04-30T12:59:39,968 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
2025-04-30T12:59:39,978 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on bb9210fe839d:42533 (size: 34.5 KiB, free: 434.3 MiB)
2025-04-30T12:59:39,981 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 2 from count at <unknown>:0
2025-04-30T12:59:39,991 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T12:59:39,994 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on bb9210fe839d:42533 in memory (size: 7.7 KiB, free: 434.3 MiB)
2025-04-30T12:59:40,015 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on 172.18.0.5:39619 in memory (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T12:59:40,059 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Registering RDD 7 (count at <unknown>:0) as input to shuffle 0
2025-04-30T12:59:40,066 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got map stage job 1 (count at <unknown>:0) with 1 output partitions
2025-04-30T12:59:40,067 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ShuffleMapStage 1 (count at <unknown>:0)
2025-04-30T12:59:40,069 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T12:59:40,071 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T12:59:40,074 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0), which has no missing parents
2025-04-30T12:59:40,107 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 433.9 MiB)
2025-04-30T12:59:40,121 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 433.9 MiB)
2025-04-30T12:59:40,123 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on bb9210fe839d:42533 (size: 8.3 KiB, free: 434.3 MiB)
2025-04-30T12:59:40,125 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 3 from broadcast at DAGScheduler.scala:1585
2025-04-30T12:59:40,130 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T12:59:40,131 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 1.0 with 1 tasks resource profile 0
2025-04-30T12:59:40,138 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.4, executor 2, partition 0, PROCESS_LOCAL, 9638 bytes) 
2025-04-30T12:59:40,153 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 1
2025-04-30T12:59:40,165 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 1.0 (TID 1)
2025-04-30T12:59:40,286 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:59:40,328 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:42533 after 2 ms (0 ms spent in bootstraps)
2025-04-30T12:59:40,371 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 434.4 MiB)
2025-04-30T12:59:40,377 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on 172.18.0.4:33397 (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T12:59:40,382 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 3 took 95 ms
2025-04-30T12:59:40,439 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 434.4 MiB)
2025-04-30T12:59:40,965 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 180.381468 ms
2025-04-30T12:59:40,976 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl, range: 0-32688, partition values: [empty row]
2025-04-30T12:59:41,009 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 11.13445 ms
2025-04-30T12:59:41,026 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:59:41,034 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.3 MiB)
2025-04-30T12:59:41,038 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on 172.18.0.4:33397 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T12:59:41,040 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 2 took 13 ms
2025-04-30T12:59:41,110 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T12:59:41,310 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 1.0 (TID 1). 2007 bytes result sent to driver
2025-04-30T12:59:41,318 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 1.0 (TID 1) in 1183 ms on 172.18.0.4 (executor 2) (1/1)
2025-04-30T12:59:41,319 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-04-30T12:59:41,322 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ShuffleMapStage 1 (count at <unknown>:0) finished in 1.238 s
2025-04-30T12:59:41,323 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - looking for newly runnable stages
2025-04-30T12:59:41,325 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - running: Set()
2025-04-30T12:59:41,326 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - waiting: Set()
2025-04-30T12:59:41,328 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - failed: Set()
2025-04-30T12:59:41,378 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 17.921586 ms
2025-04-30T12:59:41,409 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: count at <unknown>:0
2025-04-30T12:59:41,413 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 2 (count at <unknown>:0) with 1 output partitions
2025-04-30T12:59:41,414 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 3 (count at <unknown>:0)
2025-04-30T12:59:41,414 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List(ShuffleMapStage 2)
2025-04-30T12:59:41,415 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T12:59:41,417 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0), which has no missing parents
2025-04-30T12:59:41,430 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 433.9 MiB)
2025-04-30T12:59:41,439 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 433.9 MiB)
2025-04-30T12:59:41,442 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_3_piece0 on bb9210fe839d:42533 in memory (size: 8.3 KiB, free: 434.3 MiB)
2025-04-30T12:59:41,447 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_4_piece0 in memory on bb9210fe839d:42533 (size: 6.0 KiB, free: 434.3 MiB)
2025-04-30T12:59:41,449 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 4 from broadcast at DAGScheduler.scala:1585
2025-04-30T12:59:41,463 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_3_piece0 on 172.18.0.4:33397 in memory (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T12:59:41,464 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T12:59:41,466 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 3.0 with 1 tasks resource profile 0
2025-04-30T12:59:41,477 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 3.0 (TID 2) (172.18.0.4, executor 2, partition 0, NODE_LOCAL, 9003 bytes) 
2025-04-30T12:59:41,481 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 2
2025-04-30T12:59:41,483 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 3.0 (TID 2)
2025-04-30T12:59:41,492 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2025-04-30T12:59:41,531 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:59:41,540 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.0 MiB)
2025-04-30T12:59:41,543 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_4_piece0 in memory on 172.18.0.4:33397 (size: 6.0 KiB, free: 434.4 MiB)
2025-04-30T12:59:41,546 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 4 took 13 ms
2025-04-30T12:59:41,547 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 434.0 MiB)
2025-04-30T12:59:41,600 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Don't have map outputs for shuffle 0, fetching them
2025-04-30T12:59:41,602 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@bb9210fe839d:34063)
2025-04-30T12:59:41,607 [dispatcher-event-loop-8] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - Asked to send map output locations for shuffle 0 to 172.18.0.4:33278
2025-04-30T12:59:41,663 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Got the map output locations
2025-04-30T12:59:41,690 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-04-30T12:59:41,693 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Started 0 remote fetches in 14 ms
2025-04-30T12:59:41,714 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 14.863735 ms
2025-04-30T12:59:41,729 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 3.0 (TID 2). 4081 bytes result sent to driver
2025-04-30T12:59:41,737 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 3.0 (TID 2) in 262 ms on 172.18.0.4 (executor 2) (1/1)
2025-04-30T12:59:41,738 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2025-04-30T12:59:41,740 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 3 (count at <unknown>:0) finished in 0.314 s
2025-04-30T12:59:41,741 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T12:59:41,742 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 3: Stage finished
2025-04-30T12:59:41,743 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 2 finished: count at <unknown>:0, took 0.333389 s
2025-04-30T12:59:41,853 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T12:59:41,853 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T12:59:41,905 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils [] - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T12:59:41,927 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T12:59:41,927 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T12:59:41,929 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T12:59:41,930 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T12:59:41,931 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T12:59:41,932 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T12:59:42,003 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5 stored as values in memory (estimated size 200.2 KiB, free 433.7 MiB)
2025-04-30T12:59:42,024 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
2025-04-30T12:59:42,028 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_2_piece0 on bb9210fe839d:42533 in memory (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T12:59:42,031 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_5_piece0 in memory on bb9210fe839d:42533 (size: 34.5 KiB, free: 434.3 MiB)
2025-04-30T12:59:42,034 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 5 from parquet at <unknown>:0
2025-04-30T12:59:42,037 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T12:59:42,038 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_2_piece0 on 172.18.0.4:33397 in memory (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T12:59:42,051 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_4_piece0 on bb9210fe839d:42533 in memory (size: 6.0 KiB, free: 434.3 MiB)
2025-04-30T12:59:42,055 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_4_piece0 on 172.18.0.4:33397 in memory (size: 6.0 KiB, free: 434.4 MiB)
2025-04-30T12:59:42,059 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: parquet at <unknown>:0
2025-04-30T12:59:42,062 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 3 (parquet at <unknown>:0) with 1 output partitions
2025-04-30T12:59:42,062 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 4 (parquet at <unknown>:0)
2025-04-30T12:59:42,063 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T12:59:42,064 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T12:59:42,065 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 4 (MapPartitionsRDD[13] at parquet at <unknown>:0), which has no missing parents
2025-04-30T12:59:42,099 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6 stored as values in memory (estimated size 215.4 KiB, free 433.7 MiB)
2025-04-30T12:59:42,111 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 77.5 KiB, free 433.7 MiB)
2025-04-30T12:59:42,114 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_6_piece0 in memory on bb9210fe839d:42533 (size: 77.5 KiB, free: 434.3 MiB)
2025-04-30T12:59:42,115 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 6 from broadcast at DAGScheduler.scala:1585
2025-04-30T12:59:42,117 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at parquet at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T12:59:42,117 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 4.0 with 1 tasks resource profile 0
2025-04-30T12:59:42,120 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 4.0 (TID 3) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T12:59:42,125 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 3
2025-04-30T12:59:42,127 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 4.0 (TID 3)
2025-04-30T12:59:42,131 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2025-04-30T12:59:42,159 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_0_piece0 on bb9210fe839d:42533 in memory (size: 34.5 KiB, free: 434.3 MiB)
2025-04-30T12:59:42,166 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_0_piece0 on 172.18.0.5:39619 in memory (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T12:59:42,172 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:59:42,185 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 77.5 KiB, free 434.3 MiB)
2025-04-30T12:59:42,188 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_6_piece0 in memory on 172.18.0.5:39619 (size: 77.5 KiB, free: 434.3 MiB)
2025-04-30T12:59:42,191 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 6 took 18 ms
2025-04-30T12:59:42,195 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6 stored as values in memory (estimated size 215.4 KiB, free 434.1 MiB)
2025-04-30T12:59:42,273 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T12:59:42,274 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T12:59:42,276 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T12:59:42,277 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T12:59:42,278 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T12:59:42,279 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T12:59:42,289 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T12:59:42,294 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T12:59:42,338 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat [] - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-04-30T12:59:42,419 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport [] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "date_birth",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "driver_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt_current_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "first_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "last_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "license_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "phone_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_make",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_model",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_year",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary city (STRING);
  optional binary country (STRING);
  optional binary date_birth (STRING);
  optional int64 driver_id;
  optional binary dt_current_timestamp (STRING);
  optional binary first_name (STRING);
  optional binary last_name (STRING);
  optional binary license_number (STRING);
  optional binary phone_number (STRING);
  optional binary uuid (STRING);
  optional binary vehicle_make (STRING);
  optional binary vehicle_model (STRING);
  optional binary vehicle_type (STRING);
  optional int64 vehicle_year;
}

       
2025-04-30T12:59:42,454 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.0 in stage 4.0 (TID 3)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301259424794493061479331362_0004_m_000000_3 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430125931-0018/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
2025-04-30T12:59:42,491 [task-result-getter-3] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.0 in stage 4.0 (TID 3) (172.18.0.5 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301259424794493061479331362_0004_m_000000_3 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430125931-0018/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

2025-04-30T12:59:42,495 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.1 in stage 4.0 (TID 4) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T12:59:42,499 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 4
2025-04-30T12:59:42,500 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.executor.Executor [] - Running task 0.1 in stage 4.0 (TID 4)
2025-04-30T12:59:42,529 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T12:59:42,530 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T12:59:42,531 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T12:59:42,532 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T12:59:42,532 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T12:59:42,533 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T12:59:42,534 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T12:59:42,535 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T12:59:42,538 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat [] - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-04-30T12:59:42,546 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport [] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "date_birth",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "driver_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt_current_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "first_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "last_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "license_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "phone_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_make",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_model",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_year",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary city (STRING);
  optional binary country (STRING);
  optional binary date_birth (STRING);
  optional int64 driver_id;
  optional binary dt_current_timestamp (STRING);
  optional binary first_name (STRING);
  optional binary last_name (STRING);
  optional binary license_number (STRING);
  optional binary phone_number (STRING);
  optional binary uuid (STRING);
  optional binary vehicle_make (STRING);
  optional binary vehicle_model (STRING);
  optional binary vehicle_type (STRING);
  optional int64 vehicle_year;
}

       
2025-04-30T12:59:42,575 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.1 in stage 4.0 (TID 4)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301259424794493061479331362_0004_m_000000_4 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430125931-0018/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
2025-04-30T12:59:42,584 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.1 in stage 4.0 (TID 4) (172.18.0.5 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301259424794493061479331362_0004_m_000000_4 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430125931-0018/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

2025-04-30T12:59:42,587 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.2 in stage 4.0 (TID 5) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T12:59:42,599 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 5
2025-04-30T12:59:42,607 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.executor.Executor [] - Running task 0.2 in stage 4.0 (TID 5)
2025-04-30T12:59:42,663 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2025-04-30T12:59:42,715 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T12:59:42,749 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.5:39619 after 1 ms (0 ms spent in bootstraps)
2025-04-30T12:59:42,796 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 77.5 KiB, free 434.3 MiB)
2025-04-30T12:59:42,800 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_6_piece0 in memory on 172.18.0.3:43025 (size: 77.5 KiB, free: 434.3 MiB)
2025-04-30T12:59:42,805 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 6 took 89 ms
2025-04-30T12:59:42,854 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6 stored as values in memory (estimated size 215.4 KiB, free 434.1 MiB)
2025-04-30T12:59:43,102 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T12:59:43,103 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T12:59:43,207 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T12:59:43,208 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T12:59:43,209 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T12:59:43,211 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T12:59:43,218 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T12:59:43,224 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T12:59:43,255 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat [] - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-04-30T12:59:43,331 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport [] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "date_birth",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "driver_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt_current_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "first_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "last_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "license_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "phone_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_make",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_model",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_year",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary city (STRING);
  optional binary country (STRING);
  optional binary date_birth (STRING);
  optional int64 driver_id;
  optional binary dt_current_timestamp (STRING);
  optional binary first_name (STRING);
  optional binary last_name (STRING);
  optional binary license_number (STRING);
  optional binary phone_number (STRING);
  optional binary uuid (STRING);
  optional binary vehicle_make (STRING);
  optional binary vehicle_model (STRING);
  optional binary vehicle_type (STRING);
  optional int64 vehicle_year;
}

       
2025-04-30T12:59:43,376 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.2 in stage 4.0 (TID 5)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301259424794493061479331362_0004_m_000000_5 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430125931-0018/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
2025-04-30T12:59:43,409 [task-result-getter-1] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.2 in stage 4.0 (TID 5) (172.18.0.3 executor 1): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301259424794493061479331362_0004_m_000000_5 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430125931-0018/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

2025-04-30T12:59:43,411 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.3 in stage 4.0 (TID 6) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T12:59:43,415 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 6
2025-04-30T12:59:43,416 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.executor.Executor [] - Running task 0.3 in stage 4.0 (TID 6)
2025-04-30T12:59:43,457 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T12:59:43,458 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T12:59:43,460 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T12:59:43,461 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T12:59:43,462 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T12:59:43,463 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T12:59:43,464 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T12:59:43,465 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T12:59:43,468 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat [] - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-04-30T12:59:43,475 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport [] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "date_birth",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "driver_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt_current_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "first_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "last_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "license_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "phone_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_make",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_model",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_year",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary city (STRING);
  optional binary country (STRING);
  optional binary date_birth (STRING);
  optional int64 driver_id;
  optional binary dt_current_timestamp (STRING);
  optional binary first_name (STRING);
  optional binary last_name (STRING);
  optional binary license_number (STRING);
  optional binary phone_number (STRING);
  optional binary uuid (STRING);
  optional binary vehicle_make (STRING);
  optional binary vehicle_model (STRING);
  optional binary vehicle_type (STRING);
  optional int64 vehicle_year;
}

       
2025-04-30T12:59:43,513 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.3 in stage 4.0 (TID 6)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301259424794493061479331362_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430125931-0018/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
2025-04-30T12:59:43,521 [task-result-getter-2] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.3 in stage 4.0 (TID 6) (172.18.0.3 executor 1): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301259424794493061479331362_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430125931-0018/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

2025-04-30T12:59:43,522 [task-result-getter-2] ERROR org.apache.spark.scheduler.TaskSetManager [] - Task 0 in stage 4.0 failed 4 times; aborting job
2025-04-30T12:59:43,524 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2025-04-30T12:59:43,527 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Cancelling stage 4
2025-04-30T12:59:43,527 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 4: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 6) (172.18.0.3 executor 1): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301259424794493061479331362_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430125931-0018/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

Driver stacktrace:
2025-04-30T12:59:43,530 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 4 (parquet at <unknown>:0) failed in 1.462 s due to Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 6) (172.18.0.3 executor 1): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301259424794493061479331362_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430125931-0018/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

Driver stacktrace:
2025-04-30T12:59:43,533 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 3 failed: parquet at <unknown>:0, took 1.473427 s
2025-04-30T12:59:43,534 [Thread-4] ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter [] - Aborting job 9d20d424-d598-4658-bc25-d1e79015f815.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 6) (172.18.0.3 executor 1): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301259424794493061479331362_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430125931-0018/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[scala-library-2.12.18.jar:?]
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[scala-library-2.12.18.jar:?]
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at scala.Option.foreach(Option.scala:407) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461) ~[spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76) [spark-sql-api_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
	at java.lang.reflect.Method.invoke(Unknown Source) ~[?:?]
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) [py4j-0.10.9.7.jar:?]
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374) [py4j-0.10.9.7.jar:?]
	at py4j.Gateway.invoke(Gateway.java:282) [py4j-0.10.9.7.jar:?]
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) [py4j-0.10.9.7.jar:?]
	at py4j.commands.CallCommand.execute(CallCommand.java:79) [py4j-0.10.9.7.jar:?]
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182) [py4j-0.10.9.7.jar:?]
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106) [py4j-0.10.9.7.jar:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
Caused by: java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301259424794493061479331362_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430125931-0018/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]
	... 1 more
2025-04-30T12:59:43,958 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Invoking stop() from shutdown hook
2025-04-30T12:59:43,959 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-30T12:59:43,967 [shutdown-hook-0] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@413b8567{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T12:59:43,970 [shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://bb9210fe839d:4040
2025-04-30T12:59:43,975 [shutdown-hook-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-30T12:59:43,977 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-30T12:59:43,984 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:59:43,984 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:59:43,984 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T12:59:43,988 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250430125931-0018
2025-04-30T12:59:43,988 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250430125931-0018
2025-04-30T12:59:43,993 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430125931-0018/2
2025-04-30T12:59:43,993 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430125931-0018/0
2025-04-30T12:59:43,994 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430125931-0018/1
2025-04-30T12:59:43,994 [ExecutorRunner for app-20250430125931-0018/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430125931-0018/2 interrupted
2025-04-30T12:59:43,996 [ExecutorRunner for app-20250430125931-0018/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430125931-0018/1 interrupted
2025-04-30T12:59:43,996 [ExecutorRunner for app-20250430125931-0018/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430125931-0018/0 interrupted
2025-04-30T12:59:43,997 [ExecutorRunner for app-20250430125931-0018/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:59:43,997 [ExecutorRunner for app-20250430125931-0018/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:59:43,997 [ExecutorRunner for app-20250430125931-0018/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T12:59:44,000 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:59:44,000 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:59:44,001 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T12:59:44,005 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:59:44,005 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:59:44,007 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:59:44,016 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:59:44,017 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:59:44,018 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:59:44,018 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:59:44,018 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:59:44,020 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:59:44,032 [dispatcher-event-loop-6] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-30T12:59:44,048 [shutdown-hook-0] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T12:59:44,049 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T12:59:44,053 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-30T12:59:44,060 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-30T12:59:44,063 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430125931-0018/1 finished with state KILLED exitStatus 143
2025-04-30T12:59:44,065 [dispatcher-event-loop-11] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430125931-0018/1
2025-04-30T12:59:44,066 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-30T12:59:44,067 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430125931-0018, execId=1)
2025-04-30T12:59:44,067 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430125931-0018/0 finished with state KILLED exitStatus 143
2025-04-30T12:59:44,068 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:50524 got disassociated, removing it.
2025-04-30T12:59:44,068 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430125931-0018 removed, cleanupLocalDirs = true
2025-04-30T12:59:44,068 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430125931-0018
2025-04-30T12:59:44,069 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - bb9210fe839d:34063 got disassociated, removing it.
2025-04-30T12:59:44,070 [dispatcher-event-loop-7] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430125931-0018/0
2025-04-30T12:59:44,070 [dispatcher-event-loop-0] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-30T12:59:44,071 [dispatcher-event-loop-0] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430125931-0018, execId=0)
2025-04-30T12:59:44,072 [dispatcher-event-loop-0] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430125931-0018 removed, cleanupLocalDirs = true
2025-04-30T12:59:44,073 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430125931-0018
2025-04-30T12:59:44,077 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430125931-0018/2 finished with state KILLED exitStatus 143
2025-04-30T12:59:44,078 [dispatcher-event-loop-1] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430125931-0018/2
2025-04-30T12:59:44,080 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-30T12:59:44,080 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-30T12:59:44,081 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430125931-0018, execId=2)
2025-04-30T12:59:44,081 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T12:59:44,082 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430125931-0018 removed, cleanupLocalDirs = true
2025-04-30T12:59:44,083 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430125931-0018
2025-04-30T12:59:44,084 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-ba148bd6-24aa-405b-8b2a-fb8c33a36e08/pyspark-67225870-1cb5-4716-917a-087fe4259e71
2025-04-30T12:59:44,089 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-ba148bd6-24aa-405b-8b2a-fb8c33a36e08
2025-04-30T12:59:44,096 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-ced8cb83-8756-4119-89b0-66e9adc136e7
2025-04-30T13:11:13,913 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-30T13:11:13,917 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T13:11:13,918 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-30T13:11:13,944 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:11:13,945 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-30T13:11:13,946 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:11:13,947 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: mod-2-pr-4-data-ingestion.py
2025-04-30T13:11:13,967 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-30T13:11:13,977 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-30T13:11:13,979 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-30T13:11:14,027 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-30T13:11:14,028 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-30T13:11:14,029 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:11:14,030 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:11:14,031 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-30T13:11:14,119 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:11:14,419 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 33415.
2025-04-30T13:11:14,449 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-30T13:11:14,486 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-30T13:11:14,507 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-30T13:11:14,509 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-30T13:11:14,514 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-30T13:11:14,535 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-e31c3534-b687-43cf-83bc-ba2b806e12a3
2025-04-30T13:11:14,552 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T13:11:14,570 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-30T13:11:14,612 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3127ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-30T13:11:14,706 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-30T13:11:14,718 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-30T13:11:14,739 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @3256ms
2025-04-30T13:11:14,773 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@413b8567{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T13:11:14,773 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-30T13:11:14,799 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1320cce0{/,null,AVAILABLE,@Spark}
2025-04-30T13:11:14,918 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-30T13:11:14,970 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 28 ms (0 ms spent in bootstraps)
2025-04-30T13:11:15,050 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Registering app mod-2-pr-4-data-ingestion.py
2025-04-30T13:11:15,053 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Registered app mod-2-pr-4-data-ingestion.py with ID app-20250430131115-0019
2025-04-30T13:11:15,055 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430131115-0019 with rpId: 0
2025-04-30T13:11:15,058 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430131115-0019/0 on worker worker-20250429114342-172.18.0.5-38821
2025-04-30T13:11:15,060 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430131115-0019/1 on worker worker-20250429114342-172.18.0.3-34443
2025-04-30T13:11:15,061 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430131115-0019/2 on worker worker-20250429114342-172.18.0.4-36391
2025-04-30T13:11:15,062 [dispatcher-event-loop-10] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250430131115-0019
2025-04-30T13:11:15,066 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430131115-0019/0 on worker-20250429114342-172.18.0.5-38821 (172.18.0.5:38821) with 2 core(s)
2025-04-30T13:11:15,068 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430131115-0019/1 for mod-2-pr-4-data-ingestion.py
2025-04-30T13:11:15,069 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430131115-0019/0 for mod-2-pr-4-data-ingestion.py
2025-04-30T13:11:15,071 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430131115-0019/0 on hostPort 172.18.0.5:38821 with 2 core(s), 1024.0 MiB RAM
2025-04-30T13:11:15,073 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430131115-0019/1 on worker-20250429114342-172.18.0.3-34443 (172.18.0.3:34443) with 2 core(s)
2025-04-30T13:11:15,074 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430131115-0019/1 on hostPort 172.18.0.3:34443 with 2 core(s), 1024.0 MiB RAM
2025-04-30T13:11:15,073 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430131115-0019/2 for mod-2-pr-4-data-ingestion.py
2025-04-30T13:11:15,076 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430131115-0019/2 on worker-20250429114342-172.18.0.4-36391 (172.18.0.4:36391) with 2 core(s)
2025-04-30T13:11:15,076 [ExecutorRunner for app-20250430131115-0019/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:11:15,077 [ExecutorRunner for app-20250430131115-0019/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:11:15,077 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34701.
2025-04-30T13:11:15,078 [ExecutorRunner for app-20250430131115-0019/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:11:15,078 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430131115-0019/2 on hostPort 172.18.0.4:36391 with 2 core(s), 1024.0 MiB RAM
2025-04-30T13:11:15,079 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on bb9210fe839d:34701
2025-04-30T13:11:15,079 [ExecutorRunner for app-20250430131115-0019/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:11:15,080 [ExecutorRunner for app-20250430131115-0019/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:11:15,080 [ExecutorRunner for app-20250430131115-0019/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:11:15,082 [ExecutorRunner for app-20250430131115-0019/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:11:15,082 [ExecutorRunner for app-20250430131115-0019/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:11:15,083 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T13:11:15,083 [ExecutorRunner for app-20250430131115-0019/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:11:15,083 [ExecutorRunner for app-20250430131115-0019/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:11:15,084 [ExecutorRunner for app-20250430131115-0019/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:11:15,084 [ExecutorRunner for app-20250430131115-0019/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:11:15,087 [ExecutorRunner for app-20250430131115-0019/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:11:15,089 [ExecutorRunner for app-20250430131115-0019/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:11:15,090 [ExecutorRunner for app-20250430131115-0019/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:11:15,100 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, bb9210fe839d, 34701, None)
2025-04-30T13:11:15,107 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager bb9210fe839d:34701 with 434.4 MiB RAM, BlockManagerId(driver, bb9210fe839d, 34701, None)
2025-04-30T13:11:15,113 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, bb9210fe839d, 34701, None)
2025-04-30T13:11:15,116 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, bb9210fe839d, 34701, None)
2025-04-30T13:11:15,124 [ExecutorRunner for app-20250430131115-0019/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=33415" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:33415" "--executor-id" "1" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250430131115-0019" "--worker-url" "spark://Worker@172.18.0.3:34443" "--resourceProfileId" "0"
2025-04-30T13:11:15,129 [ExecutorRunner for app-20250430131115-0019/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=33415" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:33415" "--executor-id" "0" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250430131115-0019" "--worker-url" "spark://Worker@172.18.0.5:38821" "--resourceProfileId" "0"
2025-04-30T13:11:15,130 [ExecutorRunner for app-20250430131115-0019/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=33415" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:33415" "--executor-id" "2" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250430131115-0019" "--worker-url" "spark://Worker@172.18.0.4:36391" "--resourceProfileId" "0"
2025-04-30T13:11:15,139 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430131115-0019 with rpId: 0
2025-04-30T13:11:15,148 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430131115-0019 with rpId: 0
2025-04-30T13:11:15,151 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430131115-0019 with rpId: 0
2025-04-30T13:11:15,172 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430131115-0019/1 is now RUNNING
2025-04-30T13:11:15,180 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430131115-0019/0 is now RUNNING
2025-04-30T13:11:15,185 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430131115-0019/2 is now RUNNING
2025-04-30T13:11:15,574 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250430131115-0019.inprogress
2025-04-30T13:11:15,788 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@1320cce0{/,null,STOPPED,@Spark}
2025-04-30T13:11:15,792 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40857667{/jobs,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,795 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7fa04fb2{/jobs/json,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,798 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@35083451{/jobs/job,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,801 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@354b22b6{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,804 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@484d933e{/stages,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,807 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7274d8f2{/stages/json,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,810 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2dab95f3{/stages/stage,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,814 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4622b9f0{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,817 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@611e343f{/stages/pool,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,819 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@56578f8c{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,824 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@fa43f57{/storage,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,828 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2a9cdf28{/storage/json,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,831 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@41d8187c{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,834 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@251ea385{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,836 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@d678b60{/environment,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,839 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@ef17b66{/environment/json,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,842 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@58ad57bd{/executors,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,845 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1cadc900{/executors/json,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,848 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40aadd59{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,851 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3eeb6954{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,854 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1964fe85{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,858 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@52124300{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,882 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@76c527e1{/static,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,885 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@548a25dc{/,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,891 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@28825700{/api,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,897 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@611d70f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,902 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@21e7c069{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,917 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@64ddd77d{/metrics/json,null,AVAILABLE,@Spark}
2025-04-30T13:11:15,919 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-30T13:11:16,327 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-04-30T13:11:16,332 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
2025-04-30T13:11:16,358 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@739d96e{/SQL,null,AVAILABLE,@Spark}
2025-04-30T13:11:16,363 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@113354b6{/SQL/json,null,AVAILABLE,@Spark}
2025-04-30T13:11:16,367 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7bb072e9{/SQL/execution,null,AVAILABLE,@Spark}
2025-04-30T13:11:16,369 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2dd2a8f2{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-04-30T13:11:16,377 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@9867130{/static/sql,null,AVAILABLE,@Spark}
2025-04-30T13:11:17,175 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 1381@066f03cf7e53
2025-04-30T13:11:17,187 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T13:11:17,189 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T13:11:17,190 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T13:11:17,238 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 1305@c27de723f0d4
2025-04-30T13:11:17,251 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T13:11:17,253 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T13:11:17,254 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T13:11:17,257 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 1354@6cca10b5c892
2025-04-30T13:11:17,270 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T13:11:17,272 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T13:11:17,272 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T13:11:17,889 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:11:17,976 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:11:18,055 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:11:18,176 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:11:18,178 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:11:18,181 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:11:18,185 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:11:18,190 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:11:18,209 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:11:18,215 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:11:18,217 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:11:18,220 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:11:18,224 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:11:18,281 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:11:18,283 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:11:18,284 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 140 ms to list leaf files for 2 paths.
2025-04-30T13:11:18,294 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:11:18,296 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:11:18,298 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:11:18,738 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:33415 after 95 ms (0 ms spent in bootstraps)
2025-04-30T13:11:18,777 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:33415 after 108 ms (0 ms spent in bootstraps)
2025-04-30T13:11:18,848 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:33415 after 107 ms (0 ms spent in bootstraps)
2025-04-30T13:11:18,917 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:11:18,918 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:11:18,919 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:11:18,920 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:11:18,921 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:11:18,951 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:11:18,955 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:11:18,957 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:11:18,960 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:11:18,962 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:11:19,044 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:11:19,045 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:11:19,047 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:11:19,049 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:11:19,050 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:11:19,071 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:33415 after 4 ms (0 ms spent in bootstraps)
2025-04-30T13:11:19,088 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:33415 after 8 ms (0 ms spent in bootstraps)
2025-04-30T13:11:19,175 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:33415 after 18 ms (0 ms spent in bootstraps)
2025-04-30T13:11:19,198 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-c5ae99e1-4d9a-42b7-9555-210f29b2c4bb/executor-d40d5197-4fdb-47ad-8bc6-00f32a2a233b/blockmgr-d183c7dc-623b-496d-ae8c-c767c589e4b3
2025-04-30T13:11:19,232 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-00d4d2fa-4667-4dcc-a4a7-1a25b94c3d06/executor-438608ab-5af8-4796-b77b-674ad7cf4b05/blockmgr-5e407a3c-8af5-478b-812f-88e5bf6cedf5
2025-04-30T13:11:19,249 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T13:11:19,281 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T13:11:19,304 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-8bb716ea-3eee-4da7-a70b-3a99059b4da9/executor-b01c5438-09ca-4315-95a6-c7ac7816422f/blockmgr-4f0cf557-a23d-4709-82db-e1fd8bda2291
2025-04-30T13:11:19,361 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T13:11:19,584 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:33415
2025-04-30T13:11:19,588 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.5:38821
2025-04-30T13:11:19,607 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:11:19,608 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T13:11:19,610 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:11:19,616 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.5:38821 after 20 ms (0 ms spent in bootstraps)
2025-04-30T13:11:19,618 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.3:34443
2025-04-30T13:11:19,618 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:33415
2025-04-30T13:11:19,619 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.5:38821
2025-04-30T13:11:19,632 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.3:34443 after 7 ms (0 ms spent in bootstraps)
2025-04-30T13:11:19,636 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:11:19,639 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T13:11:19,641 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:11:19,642 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.3:34443
2025-04-30T13:11:19,649 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 28 ms to list leaf files for 1 paths.
2025-04-30T13:11:19,658 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:44714) with ID 0,  ResourceProfileId 0
2025-04-30T13:11:19,680 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T13:11:19,685 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 0 on host 172.18.0.5
2025-04-30T13:11:19,687 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T13:11:19,688 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T13:11:19,689 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:40308) with ID 1,  ResourceProfileId 0
2025-04-30T13:11:19,700 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T13:11:19,709 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 1 on host 172.18.0.3
2025-04-30T13:11:19,710 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T13:11:19,712 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T13:11:19,717 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:33415
2025-04-30T13:11:19,719 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.4:36391
2025-04-30T13:11:19,732 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.4:36391 after 5 ms (0 ms spent in bootstraps)
2025-04-30T13:11:19,735 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.4:36391
2025-04-30T13:11:19,743 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:11:19,745 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T13:11:19,746 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:11:19,757 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 36 ms to list leaf files for 1 paths.
2025-04-30T13:11:19,780 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42377.
2025-04-30T13:11:19,781 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.5:42377
2025-04-30T13:11:19,785 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T13:11:19,797 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:40914) with ID 2,  ResourceProfileId 0
2025-04-30T13:11:19,799 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(0, 172.18.0.5, 42377, None)
2025-04-30T13:11:19,803 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45001.
2025-04-30T13:11:19,804 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.3:45001
2025-04-30T13:11:19,805 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T13:11:19,808 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T13:11:19,812 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 2 on host 172.18.0.4
2025-04-30T13:11:19,815 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T13:11:19,817 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T13:11:19,822 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(1, 172.18.0.3, 45001, None)
2025-04-30T13:11:19,827 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.5:42377 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.5, 42377, None)
2025-04-30T13:11:19,834 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(0, 172.18.0.5, 42377, None)
2025-04-30T13:11:19,835 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.3:45001 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.3, 45001, None)
2025-04-30T13:11:19,840 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(0, 172.18.0.5, 42377, None)
2025-04-30T13:11:19,857 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T13:11:19,853 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(1, 172.18.0.3, 45001, None)
2025-04-30T13:11:19,859 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@51b1068 for default.
2025-04-30T13:11:19,870 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(1, 172.18.0.3, 45001, None)
2025-04-30T13:11:19,887 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T13:11:19,890 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@7a6deec0 for default.
2025-04-30T13:11:19,901 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44753.
2025-04-30T13:11:19,907 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.4:44753
2025-04-30T13:11:19,912 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T13:11:19,925 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(2, 172.18.0.4, 44753, None)
2025-04-30T13:11:19,937 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.4:44753 with 434.4 MiB RAM, BlockManagerId(2, 172.18.0.4, 44753, None)
2025-04-30T13:11:19,945 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(2, 172.18.0.4, 44753, None)
2025-04-30T13:11:19,947 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(2, 172.18.0.4, 44753, None)
2025-04-30T13:11:19,960 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T13:11:19,963 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@51b1068 for default.
2025-04-30T13:11:21,090 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T13:11:21,091 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T13:11:21,240 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 200.3 KiB, free 434.2 MiB)
2025-04-30T13:11:21,289 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
2025-04-30T13:11:21,292 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on bb9210fe839d:34701 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T13:11:21,297 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 0 from json at <unknown>:0
2025-04-30T13:11:21,307 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T13:11:21,428 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: json at <unknown>:0
2025-04-30T13:11:21,445 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 0 (json at <unknown>:0) with 1 output partitions
2025-04-30T13:11:21,446 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 0 (json at <unknown>:0)
2025-04-30T13:11:21,447 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T13:11:21,449 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T13:11:21,456 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0), which has no missing parents
2025-04-30T13:11:21,532 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.5 KiB, free 434.2 MiB)
2025-04-30T13:11:21,534 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
2025-04-30T13:11:21,535 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on bb9210fe839d:34701 (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T13:11:21,537 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 1 from broadcast at DAGScheduler.scala:1585
2025-04-30T13:11:21,555 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T13:11:21,557 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 0.0 with 1 tasks resource profile 0
2025-04-30T13:11:21,591 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T13:11:21,638 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 0
2025-04-30T13:11:21,659 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 0.0 (TID 0)
2025-04-30T13:11:21,747 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:11:21,792 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:34701 after 10 ms (0 ms spent in bootstraps)
2025-04-30T13:11:21,832 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.4 MiB)
2025-04-30T13:11:21,839 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on 172.18.0.3:45001 (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T13:11:21,844 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 1 took 96 ms
2025-04-30T13:11:21,892 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.5 KiB, free 434.4 MiB)
2025-04-30T13:11:22,169 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl, range: 0-32688, partition values: [empty row]
2025-04-30T13:11:22,453 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 163.712664 ms
2025-04-30T13:11:22,455 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:11:22,463 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.3 MiB)
2025-04-30T13:11:22,467 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on 172.18.0.3:45001 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T13:11:22,469 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 0 took 13 ms
2025-04-30T13:11:22,540 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T13:11:22,684 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 0.0 (TID 0). 2450 bytes result sent to driver
2025-04-30T13:11:22,697 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 0.0 (TID 0) in 1123 ms on 172.18.0.3 (executor 1) (1/1)
2025-04-30T13:11:22,699 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-04-30T13:11:22,703 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 0 (json at <unknown>:0) finished in 1.231 s
2025-04-30T13:11:22,706 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T13:11:22,707 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 0: Stage finished
2025-04-30T13:11:22,711 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 finished: json at <unknown>:0, took 1.281672 s
2025-04-30T13:11:22,824 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T13:11:22,825 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T13:11:23,159 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 185.954937 ms
2025-04-30T13:11:23,165 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 200.2 KiB, free 434.0 MiB)
2025-04-30T13:11:23,177 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.9 MiB)
2025-04-30T13:11:23,179 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on bb9210fe839d:34701 (size: 34.4 KiB, free: 434.3 MiB)
2025-04-30T13:11:23,181 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 2 from count at <unknown>:0
2025-04-30T13:11:23,186 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T13:11:23,229 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Registering RDD 7 (count at <unknown>:0) as input to shuffle 0
2025-04-30T13:11:23,233 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got map stage job 1 (count at <unknown>:0) with 1 output partitions
2025-04-30T13:11:23,234 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ShuffleMapStage 1 (count at <unknown>:0)
2025-04-30T13:11:23,234 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T13:11:23,237 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T13:11:23,240 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0), which has no missing parents
2025-04-30T13:11:23,257 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 433.9 MiB)
2025-04-30T13:11:23,267 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 433.9 MiB)
2025-04-30T13:11:23,269 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on bb9210fe839d:34701 (size: 8.3 KiB, free: 434.3 MiB)
2025-04-30T13:11:23,271 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 3 from broadcast at DAGScheduler.scala:1585
2025-04-30T13:11:23,276 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T13:11:23,277 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 1.0 with 1 tasks resource profile 0
2025-04-30T13:11:23,281 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 9638 bytes) 
2025-04-30T13:11:23,304 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 1
2025-04-30T13:11:23,308 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on bb9210fe839d:34701 in memory (size: 7.7 KiB, free: 434.3 MiB)
2025-04-30T13:11:23,327 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 1.0 (TID 1)
2025-04-30T13:11:23,326 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on 172.18.0.3:45001 in memory (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T13:11:23,478 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:11:23,517 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:34701 after 2 ms (0 ms spent in bootstraps)
2025-04-30T13:11:23,560 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 434.4 MiB)
2025-04-30T13:11:23,566 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on 172.18.0.5:42377 (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T13:11:23,570 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 3 took 92 ms
2025-04-30T13:11:23,632 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 434.4 MiB)
2025-04-30T13:11:24,204 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 196.241987 ms
2025-04-30T13:11:24,214 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl, range: 0-32688, partition values: [empty row]
2025-04-30T13:11:24,244 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 9.866332 ms
2025-04-30T13:11:24,260 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:11:24,267 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.3 MiB)
2025-04-30T13:11:24,270 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on 172.18.0.5:42377 (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T13:11:24,272 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 2 took 11 ms
2025-04-30T13:11:24,331 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T13:11:24,533 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 1.0 (TID 1). 2007 bytes result sent to driver
2025-04-30T13:11:24,541 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 1.0 (TID 1) in 1262 ms on 172.18.0.5 (executor 0) (1/1)
2025-04-30T13:11:24,541 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-04-30T13:11:24,544 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ShuffleMapStage 1 (count at <unknown>:0) finished in 1.300 s
2025-04-30T13:11:24,545 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - looking for newly runnable stages
2025-04-30T13:11:24,546 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - running: Set()
2025-04-30T13:11:24,547 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - waiting: Set()
2025-04-30T13:11:24,547 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - failed: Set()
2025-04-30T13:11:24,589 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 14.138819 ms
2025-04-30T13:11:24,614 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: count at <unknown>:0
2025-04-30T13:11:24,617 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 2 (count at <unknown>:0) with 1 output partitions
2025-04-30T13:11:24,617 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 3 (count at <unknown>:0)
2025-04-30T13:11:24,618 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List(ShuffleMapStage 2)
2025-04-30T13:11:24,619 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T13:11:24,620 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0), which has no missing parents
2025-04-30T13:11:24,631 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 433.9 MiB)
2025-04-30T13:11:24,641 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 433.9 MiB)
2025-04-30T13:11:24,645 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_4_piece0 in memory on bb9210fe839d:34701 (size: 6.0 KiB, free: 434.3 MiB)
2025-04-30T13:11:24,648 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 4 from broadcast at DAGScheduler.scala:1585
2025-04-30T13:11:24,648 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_3_piece0 on bb9210fe839d:34701 in memory (size: 8.3 KiB, free: 434.3 MiB)
2025-04-30T13:11:24,650 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T13:11:24,651 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 3.0 with 1 tasks resource profile 0
2025-04-30T13:11:24,657 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_3_piece0 on 172.18.0.5:42377 in memory (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T13:11:24,660 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 3.0 (TID 2) (172.18.0.5, executor 0, partition 0, NODE_LOCAL, 9003 bytes) 
2025-04-30T13:11:24,668 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 2
2025-04-30T13:11:24,670 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 3.0 (TID 2)
2025-04-30T13:11:24,678 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2025-04-30T13:11:24,692 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:11:24,700 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.0 MiB)
2025-04-30T13:11:24,702 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_4_piece0 in memory on 172.18.0.5:42377 (size: 6.0 KiB, free: 434.4 MiB)
2025-04-30T13:11:24,705 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 4 took 12 ms
2025-04-30T13:11:24,708 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 434.0 MiB)
2025-04-30T13:11:24,771 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Don't have map outputs for shuffle 0, fetching them
2025-04-30T13:11:24,773 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@bb9210fe839d:33415)
2025-04-30T13:11:24,778 [dispatcher-event-loop-10] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - Asked to send map output locations for shuffle 0 to 172.18.0.5:44714
2025-04-30T13:11:24,826 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Got the map output locations
2025-04-30T13:11:24,864 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-04-30T13:11:24,868 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Started 0 remote fetches in 21 ms
2025-04-30T13:11:24,892 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 17.694474 ms
2025-04-30T13:11:24,913 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 3.0 (TID 2). 4081 bytes result sent to driver
2025-04-30T13:11:24,923 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 3.0 (TID 2) in 265 ms on 172.18.0.5 (executor 0) (1/1)
2025-04-30T13:11:24,924 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2025-04-30T13:11:24,926 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 3 (count at <unknown>:0) finished in 0.297 s
2025-04-30T13:11:24,927 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T13:11:24,928 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 3: Stage finished
2025-04-30T13:11:24,929 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 2 finished: count at <unknown>:0, took 0.314374 s
2025-04-30T13:11:25,036 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T13:11:25,037 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T13:11:25,102 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils [] - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:11:25,118 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:11:25,119 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:11:25,122 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:11:25,123 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:11:25,124 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:11:25,125 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:11:25,187 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5 stored as values in memory (estimated size 200.2 KiB, free 433.7 MiB)
2025-04-30T13:11:25,201 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_2_piece0 on bb9210fe839d:34701 in memory (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T13:11:25,207 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_2_piece0 on 172.18.0.5:42377 in memory (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T13:11:25,210 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.9 MiB)
2025-04-30T13:11:25,212 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_5_piece0 in memory on bb9210fe839d:34701 (size: 34.4 KiB, free: 434.3 MiB)
2025-04-30T13:11:25,214 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 5 from parquet at <unknown>:0
2025-04-30T13:11:25,216 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T13:11:25,220 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_4_piece0 on bb9210fe839d:34701 in memory (size: 6.0 KiB, free: 434.3 MiB)
2025-04-30T13:11:25,225 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_4_piece0 on 172.18.0.5:42377 in memory (size: 6.0 KiB, free: 434.4 MiB)
2025-04-30T13:11:25,234 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: parquet at <unknown>:0
2025-04-30T13:11:25,236 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 3 (parquet at <unknown>:0) with 1 output partitions
2025-04-30T13:11:25,237 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 4 (parquet at <unknown>:0)
2025-04-30T13:11:25,237 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T13:11:25,238 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T13:11:25,241 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 4 (MapPartitionsRDD[13] at parquet at <unknown>:0), which has no missing parents
2025-04-30T13:11:25,280 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6 stored as values in memory (estimated size 215.4 KiB, free 433.7 MiB)
2025-04-30T13:11:25,290 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 77.5 KiB, free 433.7 MiB)
2025-04-30T13:11:25,292 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_6_piece0 in memory on bb9210fe839d:34701 (size: 77.5 KiB, free: 434.3 MiB)
2025-04-30T13:11:25,294 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 6 from broadcast at DAGScheduler.scala:1585
2025-04-30T13:11:25,296 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at parquet at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T13:11:25,297 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 4.0 with 1 tasks resource profile 0
2025-04-30T13:11:25,300 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 4.0 (TID 3) (172.18.0.4, executor 2, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T13:11:25,315 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 3
2025-04-30T13:11:25,337 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_0_piece0 on bb9210fe839d:34701 in memory (size: 34.5 KiB, free: 434.3 MiB)
2025-04-30T13:11:25,337 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 4.0 (TID 3)
2025-04-30T13:11:25,353 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_0_piece0 on 172.18.0.3:45001 in memory (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T13:11:25,415 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2025-04-30T13:11:25,465 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:11:25,501 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:34701 after 2 ms (0 ms spent in bootstraps)
2025-04-30T13:11:25,544 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 77.5 KiB, free 434.3 MiB)
2025-04-30T13:11:25,549 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_6_piece0 in memory on 172.18.0.4:44753 (size: 77.5 KiB, free: 434.3 MiB)
2025-04-30T13:11:25,554 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 6 took 88 ms
2025-04-30T13:11:25,611 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6 stored as values in memory (estimated size 215.4 KiB, free 434.1 MiB)
2025-04-30T13:11:25,888 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:11:25,889 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:11:25,988 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:11:25,989 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:11:25,989 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:11:25,991 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:11:25,998 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:11:26,005 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:11:26,033 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat [] - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-04-30T13:11:26,110 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport [] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "date_birth",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "driver_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt_current_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "first_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "last_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "license_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "phone_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_make",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_model",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_year",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary city (STRING);
  optional binary country (STRING);
  optional binary date_birth (STRING);
  optional int64 driver_id;
  optional binary dt_current_timestamp (STRING);
  optional binary first_name (STRING);
  optional binary last_name (STRING);
  optional binary license_number (STRING);
  optional binary phone_number (STRING);
  optional binary uuid (STRING);
  optional binary vehicle_make (STRING);
  optional binary vehicle_model (STRING);
  optional binary vehicle_type (STRING);
  optional int64 vehicle_year;
}

       
2025-04-30T13:11:26,149 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.0 in stage 4.0 (TID 3)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301311254940688957817399304_0004_m_000000_3 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430131115-0019/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
2025-04-30T13:11:26,195 [task-result-getter-3] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.0 in stage 4.0 (TID 3) (172.18.0.4 executor 2): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301311254940688957817399304_0004_m_000000_3 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430131115-0019/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

2025-04-30T13:11:26,197 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.1 in stage 4.0 (TID 4) (172.18.0.4, executor 2, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T13:11:26,202 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 4
2025-04-30T13:11:26,204 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.executor.Executor [] - Running task 0.1 in stage 4.0 (TID 4)
2025-04-30T13:11:26,242 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:11:26,243 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:11:26,244 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:11:26,245 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:11:26,245 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:11:26,247 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:11:26,248 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:11:26,249 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:11:26,251 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat [] - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-04-30T13:11:26,258 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport [] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "date_birth",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "driver_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt_current_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "first_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "last_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "license_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "phone_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_make",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_model",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_year",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary city (STRING);
  optional binary country (STRING);
  optional binary date_birth (STRING);
  optional int64 driver_id;
  optional binary dt_current_timestamp (STRING);
  optional binary first_name (STRING);
  optional binary last_name (STRING);
  optional binary license_number (STRING);
  optional binary phone_number (STRING);
  optional binary uuid (STRING);
  optional binary vehicle_make (STRING);
  optional binary vehicle_model (STRING);
  optional binary vehicle_type (STRING);
  optional int64 vehicle_year;
}

       
2025-04-30T13:11:26,287 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.1 in stage 4.0 (TID 4)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301311254940688957817399304_0004_m_000000_4 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430131115-0019/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
2025-04-30T13:11:26,295 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.1 in stage 4.0 (TID 4) (172.18.0.4 executor 2): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301311254940688957817399304_0004_m_000000_4 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430131115-0019/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

2025-04-30T13:11:26,296 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.2 in stage 4.0 (TID 5) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T13:11:26,300 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 5
2025-04-30T13:11:26,301 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.executor.Executor [] - Running task 0.2 in stage 4.0 (TID 5)
2025-04-30T13:11:26,306 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:11:26,313 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 77.5 KiB, free 434.3 MiB)
2025-04-30T13:11:26,316 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_6_piece0 in memory on 172.18.0.5:42377 (size: 77.5 KiB, free: 434.3 MiB)
2025-04-30T13:11:26,318 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 6 took 12 ms
2025-04-30T13:11:26,320 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6 stored as values in memory (estimated size 215.4 KiB, free 434.1 MiB)
2025-04-30T13:11:26,379 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:11:26,379 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:11:26,382 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:11:26,382 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:11:26,383 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:11:26,385 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:11:26,397 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:11:26,400 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:11:26,428 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat [] - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-04-30T13:11:26,506 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport [] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "date_birth",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "driver_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt_current_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "first_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "last_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "license_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "phone_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_make",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_model",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_year",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary city (STRING);
  optional binary country (STRING);
  optional binary date_birth (STRING);
  optional int64 driver_id;
  optional binary dt_current_timestamp (STRING);
  optional binary first_name (STRING);
  optional binary last_name (STRING);
  optional binary license_number (STRING);
  optional binary phone_number (STRING);
  optional binary uuid (STRING);
  optional binary vehicle_make (STRING);
  optional binary vehicle_model (STRING);
  optional binary vehicle_type (STRING);
  optional int64 vehicle_year;
}

       
2025-04-30T13:11:26,543 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.2 in stage 4.0 (TID 5)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301311254940688957817399304_0004_m_000000_5 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430131115-0019/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
2025-04-30T13:11:26,566 [task-result-getter-1] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.2 in stage 4.0 (TID 5) (172.18.0.5 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301311254940688957817399304_0004_m_000000_5 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430131115-0019/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

2025-04-30T13:11:26,568 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.3 in stage 4.0 (TID 6) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T13:11:26,572 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 6
2025-04-30T13:11:26,573 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.executor.Executor [] - Running task 0.3 in stage 4.0 (TID 6)
2025-04-30T13:11:26,577 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2025-04-30T13:11:26,591 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:11:26,598 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.4:44753 after 2 ms (0 ms spent in bootstraps)
2025-04-30T13:11:26,619 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 77.5 KiB, free 434.3 MiB)
2025-04-30T13:11:26,623 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_6_piece0 in memory on 172.18.0.3:45001 (size: 77.5 KiB, free: 434.3 MiB)
2025-04-30T13:11:26,625 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 6 took 33 ms
2025-04-30T13:11:26,634 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6 stored as values in memory (estimated size 215.4 KiB, free 434.1 MiB)
2025-04-30T13:11:26,720 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:11:26,720 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:11:26,723 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:11:26,724 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:11:26,724 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:11:26,726 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:11:26,735 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:11:26,739 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:11:26,776 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat [] - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-04-30T13:11:26,832 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport [] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "date_birth",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "driver_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt_current_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "first_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "last_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "license_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "phone_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_make",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_model",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_year",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary city (STRING);
  optional binary country (STRING);
  optional binary date_birth (STRING);
  optional int64 driver_id;
  optional binary dt_current_timestamp (STRING);
  optional binary first_name (STRING);
  optional binary last_name (STRING);
  optional binary license_number (STRING);
  optional binary phone_number (STRING);
  optional binary uuid (STRING);
  optional binary vehicle_make (STRING);
  optional binary vehicle_model (STRING);
  optional binary vehicle_type (STRING);
  optional int64 vehicle_year;
}

       
2025-04-30T13:11:26,861 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.3 in stage 4.0 (TID 6)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301311254940688957817399304_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430131115-0019/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
2025-04-30T13:11:26,888 [task-result-getter-2] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.3 in stage 4.0 (TID 6) (172.18.0.3 executor 1): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301311254940688957817399304_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430131115-0019/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

2025-04-30T13:11:26,890 [task-result-getter-2] ERROR org.apache.spark.scheduler.TaskSetManager [] - Task 0 in stage 4.0 failed 4 times; aborting job
2025-04-30T13:11:26,892 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2025-04-30T13:11:26,894 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Cancelling stage 4
2025-04-30T13:11:26,895 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 4: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 6) (172.18.0.3 executor 1): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301311254940688957817399304_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430131115-0019/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

Driver stacktrace:
2025-04-30T13:11:26,897 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 4 (parquet at <unknown>:0) failed in 1.652 s due to Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 6) (172.18.0.3 executor 1): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301311254940688957817399304_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430131115-0019/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

Driver stacktrace:
2025-04-30T13:11:26,901 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 3 failed: parquet at <unknown>:0, took 1.666106 s
2025-04-30T13:11:26,903 [Thread-4] ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter [] - Aborting job 2695d194-c6e4-4538-ac4c-2ff1feeb2a5a.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 6) (172.18.0.3 executor 1): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301311254940688957817399304_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430131115-0019/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[scala-library-2.12.18.jar:?]
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[scala-library-2.12.18.jar:?]
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at scala.Option.foreach(Option.scala:407) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461) ~[spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76) [spark-sql-api_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
	at java.lang.reflect.Method.invoke(Unknown Source) ~[?:?]
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) [py4j-0.10.9.7.jar:?]
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374) [py4j-0.10.9.7.jar:?]
	at py4j.Gateway.invoke(Gateway.java:282) [py4j-0.10.9.7.jar:?]
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) [py4j-0.10.9.7.jar:?]
	at py4j.commands.CallCommand.execute(CallCommand.java:79) [py4j-0.10.9.7.jar:?]
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182) [py4j-0.10.9.7.jar:?]
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106) [py4j-0.10.9.7.jar:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
Caused by: java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301311254940688957817399304_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430131115-0019/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]
	... 1 more
2025-04-30T13:11:27,293 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Invoking stop() from shutdown hook
2025-04-30T13:11:27,294 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-30T13:11:27,301 [shutdown-hook-0] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@413b8567{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T13:11:27,305 [shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://bb9210fe839d:4040
2025-04-30T13:11:27,308 [shutdown-hook-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-30T13:11:27,309 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-30T13:11:27,314 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T13:11:27,314 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T13:11:27,315 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T13:11:27,318 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250430131115-0019
2025-04-30T13:11:27,319 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250430131115-0019
2025-04-30T13:11:27,322 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430131115-0019/0
2025-04-30T13:11:27,325 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430131115-0019/2
2025-04-30T13:11:27,325 [ExecutorRunner for app-20250430131115-0019/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430131115-0019/0 interrupted
2025-04-30T13:11:27,325 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430131115-0019/1
2025-04-30T13:11:27,327 [ExecutorRunner for app-20250430131115-0019/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430131115-0019/2 interrupted
2025-04-30T13:11:27,327 [ExecutorRunner for app-20250430131115-0019/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T13:11:27,328 [ExecutorRunner for app-20250430131115-0019/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T13:11:27,330 [ExecutorRunner for app-20250430131115-0019/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430131115-0019/1 interrupted
2025-04-30T13:11:27,330 [ExecutorRunner for app-20250430131115-0019/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T13:11:27,330 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T13:11:27,331 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T13:11:27,333 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T13:11:27,336 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T13:11:27,338 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T13:11:27,340 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T13:11:27,343 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T13:11:27,345 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T13:11:27,346 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T13:11:27,347 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T13:11:27,346 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T13:11:27,348 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T13:11:27,366 [dispatcher-event-loop-6] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-30T13:11:27,382 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430131115-0019/2 finished with state KILLED exitStatus 143
2025-04-30T13:11:27,383 [dispatcher-event-loop-6] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-30T13:11:27,383 [dispatcher-event-loop-10] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430131115-0019/2
2025-04-30T13:11:27,384 [dispatcher-event-loop-6] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430131115-0019, execId=2)
2025-04-30T13:11:27,385 [dispatcher-event-loop-6] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430131115-0019 removed, cleanupLocalDirs = true
2025-04-30T13:11:27,385 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430131115-0019
2025-04-30T13:11:27,385 [shutdown-hook-0] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T13:11:27,386 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T13:11:27,392 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-30T13:11:27,397 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-30T13:11:27,400 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430131115-0019/0 finished with state KILLED exitStatus 143
2025-04-30T13:11:27,402 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-30T13:11:27,402 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430131115-0019/1 finished with state KILLED exitStatus 143
2025-04-30T13:11:27,403 [dispatcher-event-loop-8] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430131115-0019/0
2025-04-30T13:11:27,404 [dispatcher-event-loop-11] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-30T13:11:27,404 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430131115-0019, execId=0)
2025-04-30T13:11:27,405 [dispatcher-event-loop-8] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430131115-0019/1
2025-04-30T13:11:27,405 [dispatcher-event-loop-11] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430131115-0019, execId=1)
2025-04-30T13:11:27,406 [dispatcher-event-loop-11] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430131115-0019 removed, cleanupLocalDirs = true
2025-04-30T13:11:27,406 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:36810 got disassociated, removing it.
2025-04-30T13:11:27,406 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430131115-0019 removed, cleanupLocalDirs = true
2025-04-30T13:11:27,407 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430131115-0019
2025-04-30T13:11:27,407 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430131115-0019
2025-04-30T13:11:27,408 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - bb9210fe839d:33415 got disassociated, removing it.
2025-04-30T13:11:27,415 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-30T13:11:27,416 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T13:11:27,417 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-834d595e-f7fe-4612-8e01-bab679ebfb6f/pyspark-58503451-62aa-46b5-b13b-cb4708b1268b
2025-04-30T13:11:27,422 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-b4261f85-1951-4505-a279-7f3e9d50592e
2025-04-30T13:11:27,426 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-834d595e-f7fe-4612-8e01-bab679ebfb6f
2025-04-30T13:13:15,943 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-30T13:13:15,947 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T13:13:15,949 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-30T13:13:15,972 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:13:15,972 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-30T13:13:15,973 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:13:15,974 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: mod-2-pr-4-data-ingestion.py
2025-04-30T13:13:15,995 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-30T13:13:16,002 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-30T13:13:16,004 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-30T13:13:16,052 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-30T13:13:16,052 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-30T13:13:16,053 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:13:16,054 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:13:16,055 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-30T13:13:16,111 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:13:16,362 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 44967.
2025-04-30T13:13:16,389 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-30T13:13:16,423 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-30T13:13:16,442 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-30T13:13:16,444 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-30T13:13:16,449 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-30T13:13:16,468 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-41c324f1-1c14-445c-a78f-65031a5b7161
2025-04-30T13:13:16,483 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T13:13:16,500 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-30T13:13:16,539 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3150ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-30T13:13:16,617 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-30T13:13:16,627 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-30T13:13:16,643 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @3256ms
2025-04-30T13:13:16,670 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@413b8567{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T13:13:16,671 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-30T13:13:16,697 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1320cce0{/,null,AVAILABLE,@Spark}
2025-04-30T13:13:16,802 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-30T13:13:16,849 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 27 ms (0 ms spent in bootstraps)
2025-04-30T13:13:16,928 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Registering app mod-2-pr-4-data-ingestion.py
2025-04-30T13:13:16,930 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Registered app mod-2-pr-4-data-ingestion.py with ID app-20250430131316-0020
2025-04-30T13:13:16,931 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430131316-0020 with rpId: 0
2025-04-30T13:13:16,932 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430131316-0020/0 on worker worker-20250429114342-172.18.0.5-38821
2025-04-30T13:13:16,933 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430131316-0020/1 on worker worker-20250429114342-172.18.0.3-34443
2025-04-30T13:13:16,935 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430131316-0020/2 on worker worker-20250429114342-172.18.0.4-36391
2025-04-30T13:13:16,937 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430131316-0020/0 for mod-2-pr-4-data-ingestion.py
2025-04-30T13:13:16,938 [dispatcher-event-loop-10] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250430131316-0020
2025-04-30T13:13:16,939 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430131316-0020/1 for mod-2-pr-4-data-ingestion.py
2025-04-30T13:13:16,940 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430131316-0020/2 for mod-2-pr-4-data-ingestion.py
2025-04-30T13:13:16,945 [ExecutorRunner for app-20250430131316-0020/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:13:16,946 [ExecutorRunner for app-20250430131316-0020/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:13:16,946 [ExecutorRunner for app-20250430131316-0020/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:13:16,947 [ExecutorRunner for app-20250430131316-0020/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:13:16,947 [ExecutorRunner for app-20250430131316-0020/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:13:16,947 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430131316-0020/0 on worker-20250429114342-172.18.0.5-38821 (172.18.0.5:38821) with 2 core(s)
2025-04-30T13:13:16,947 [ExecutorRunner for app-20250430131316-0020/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:13:16,947 [ExecutorRunner for app-20250430131316-0020/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:13:16,947 [ExecutorRunner for app-20250430131316-0020/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:13:16,948 [ExecutorRunner for app-20250430131316-0020/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:13:16,948 [ExecutorRunner for app-20250430131316-0020/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:13:16,949 [ExecutorRunner for app-20250430131316-0020/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:13:16,949 [ExecutorRunner for app-20250430131316-0020/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:13:16,950 [ExecutorRunner for app-20250430131316-0020/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:13:16,950 [ExecutorRunner for app-20250430131316-0020/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:13:16,951 [ExecutorRunner for app-20250430131316-0020/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:13:16,951 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430131316-0020/0 on hostPort 172.18.0.5:38821 with 2 core(s), 1024.0 MiB RAM
2025-04-30T13:13:16,952 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430131316-0020/1 on worker-20250429114342-172.18.0.3-34443 (172.18.0.3:34443) with 2 core(s)
2025-04-30T13:13:16,953 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430131316-0020/1 on hostPort 172.18.0.3:34443 with 2 core(s), 1024.0 MiB RAM
2025-04-30T13:13:16,954 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46295.
2025-04-30T13:13:16,954 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430131316-0020/2 on worker-20250429114342-172.18.0.4-36391 (172.18.0.4:36391) with 2 core(s)
2025-04-30T13:13:16,955 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on bb9210fe839d:46295
2025-04-30T13:13:16,956 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430131316-0020/2 on hostPort 172.18.0.4:36391 with 2 core(s), 1024.0 MiB RAM
2025-04-30T13:13:16,958 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T13:13:16,967 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, bb9210fe839d, 46295, None)
2025-04-30T13:13:16,972 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager bb9210fe839d:46295 with 434.4 MiB RAM, BlockManagerId(driver, bb9210fe839d, 46295, None)
2025-04-30T13:13:16,976 [ExecutorRunner for app-20250430131316-0020/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=44967" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:44967" "--executor-id" "2" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250430131316-0020" "--worker-url" "spark://Worker@172.18.0.4:36391" "--resourceProfileId" "0"
2025-04-30T13:13:16,977 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, bb9210fe839d, 46295, None)
2025-04-30T13:13:16,979 [ExecutorRunner for app-20250430131316-0020/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=44967" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:44967" "--executor-id" "1" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250430131316-0020" "--worker-url" "spark://Worker@172.18.0.3:34443" "--resourceProfileId" "0"
2025-04-30T13:13:16,979 [ExecutorRunner for app-20250430131316-0020/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=44967" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@bb9210fe839d:44967" "--executor-id" "0" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250430131316-0020" "--worker-url" "spark://Worker@172.18.0.5:38821" "--resourceProfileId" "0"
2025-04-30T13:13:16,979 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, bb9210fe839d, 46295, None)
2025-04-30T13:13:16,987 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430131316-0020 with rpId: 0
2025-04-30T13:13:16,990 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430131316-0020 with rpId: 0
2025-04-30T13:13:16,997 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430131316-0020 with rpId: 0
2025-04-30T13:13:17,013 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430131316-0020/2 is now RUNNING
2025-04-30T13:13:17,015 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430131316-0020/1 is now RUNNING
2025-04-30T13:13:17,020 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430131316-0020/0 is now RUNNING
2025-04-30T13:13:17,366 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250430131316-0020.inprogress
2025-04-30T13:13:17,566 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@1320cce0{/,null,STOPPED,@Spark}
2025-04-30T13:13:17,568 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40857667{/jobs,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,571 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7fa04fb2{/jobs/json,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,574 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@35083451{/jobs/job,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,577 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@354b22b6{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,579 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@484d933e{/stages,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,581 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7274d8f2{/stages/json,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,584 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2dab95f3{/stages/stage,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,587 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4622b9f0{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,590 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@611e343f{/stages/pool,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,594 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@56578f8c{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,597 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@fa43f57{/storage,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,599 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2a9cdf28{/storage/json,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,603 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@41d8187c{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,605 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@251ea385{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,609 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@d678b60{/environment,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,612 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@ef17b66{/environment/json,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,615 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@58ad57bd{/executors,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,620 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1cadc900{/executors/json,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,623 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40aadd59{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,626 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3eeb6954{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,631 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1964fe85{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,637 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@52124300{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,660 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@76c527e1{/static,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,672 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@548a25dc{/,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,676 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@28825700{/api,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,680 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@611d70f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,682 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@21e7c069{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,697 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@64ddd77d{/metrics/json,null,AVAILABLE,@Spark}
2025-04-30T13:13:17,700 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-30T13:13:18,105 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-04-30T13:13:18,111 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
2025-04-30T13:13:18,138 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@739d96e{/SQL,null,AVAILABLE,@Spark}
2025-04-30T13:13:18,142 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@113354b6{/SQL/json,null,AVAILABLE,@Spark}
2025-04-30T13:13:18,146 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7bb072e9{/SQL/execution,null,AVAILABLE,@Spark}
2025-04-30T13:13:18,153 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2dd2a8f2{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-04-30T13:13:18,161 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@9867130{/static/sql,null,AVAILABLE,@Spark}
2025-04-30T13:13:18,885 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 1452@6cca10b5c892
2025-04-30T13:13:18,898 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T13:13:18,900 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T13:13:18,901 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T13:13:18,912 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 1480@066f03cf7e53
2025-04-30T13:13:18,924 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T13:13:18,926 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T13:13:18,927 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T13:13:18,946 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 1405@c27de723f0d4
2025-04-30T13:13:18,958 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T13:13:18,960 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T13:13:18,961 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T13:13:19,417 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:13:19,493 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:13:19,569 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:13:19,623 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:13:19,625 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:13:19,628 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:13:19,634 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:13:19,637 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:13:19,686 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:13:19,692 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:13:19,696 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:13:19,697 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:13:19,699 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:13:19,728 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 128 ms to list leaf files for 2 paths.
2025-04-30T13:13:19,751 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:13:19,754 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:13:19,763 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:13:19,766 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:13:19,770 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:13:20,314 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:44967 after 132 ms (0 ms spent in bootstraps)
2025-04-30T13:13:20,407 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:44967 after 178 ms (0 ms spent in bootstraps)
2025-04-30T13:13:20,480 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:44967 after 189 ms (0 ms spent in bootstraps)
2025-04-30T13:13:20,575 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:13:20,576 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:13:20,577 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:13:20,580 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:13:20,583 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:13:20,631 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:13:20,633 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:13:20,636 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:13:20,637 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:13:20,638 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:13:20,696 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:44967 after 3 ms (0 ms spent in bootstraps)
2025-04-30T13:13:20,704 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:13:20,705 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:13:20,707 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:13:20,708 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:13:20,710 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:13:20,743 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:44967 after 12 ms (0 ms spent in bootstraps)
2025-04-30T13:13:20,796 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:44967 after 4 ms (0 ms spent in bootstraps)
2025-04-30T13:13:20,816 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-8bb716ea-3eee-4da7-a70b-3a99059b4da9/executor-5ea699f1-28d0-4440-a525-d6f75f8057e7/blockmgr-79536ce1-5542-4120-acf0-647f9e913eed
2025-04-30T13:13:20,860 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-c5ae99e1-4d9a-42b7-9555-210f29b2c4bb/executor-f63286da-85a5-47a7-9369-c3ebbf8fb673/blockmgr-b8a145d6-68e7-49ce-8b38-650d977c1119
2025-04-30T13:13:20,867 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T13:13:20,903 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-00d4d2fa-4667-4dcc-a4a7-1a25b94c3d06/executor-292e4737-bac3-489e-aeae-bc8c1ad2e54f/blockmgr-ac8c9feb-749d-477c-a79b-83fdd3eda310
2025-04-30T13:13:20,917 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T13:13:20,948 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T13:13:21,142 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:44967
2025-04-30T13:13:21,144 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.4:36391
2025-04-30T13:13:21,156 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.4:36391 after 8 ms (0 ms spent in bootstraps)
2025-04-30T13:13:21,156 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.4:36391
2025-04-30T13:13:21,160 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:13:21,161 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T13:13:21,163 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:13:21,191 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.5:38821
2025-04-30T13:13:21,191 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:44967
2025-04-30T13:13:21,208 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:32928) with ID 2,  ResourceProfileId 0
2025-04-30T13:13:21,235 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:13:21,237 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T13:13:21,237 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T13:13:21,239 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:13:21,241 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.5:38821 after 28 ms (0 ms spent in bootstraps)
2025-04-30T13:13:21,245 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.5:38821
2025-04-30T13:13:21,246 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 2 on host 172.18.0.4
2025-04-30T13:13:21,249 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T13:13:21,249 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@bb9210fe839d:44967
2025-04-30T13:13:21,250 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T13:13:21,251 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.3:34443
2025-04-30T13:13:21,278 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.3:34443
2025-04-30T13:13:21,279 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:13:21,275 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.3:34443 after 11 ms (0 ms spent in bootstraps)
2025-04-30T13:13:21,280 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T13:13:21,283 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:13:21,283 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:34996) with ID 0,  ResourceProfileId 0
2025-04-30T13:13:21,298 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T13:13:21,311 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 0 on host 172.18.0.5
2025-04-30T13:13:21,313 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T13:13:21,315 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T13:13:21,335 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 33 ms to list leaf files for 1 paths.
2025-04-30T13:13:21,342 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:45498) with ID 1,  ResourceProfileId 0
2025-04-30T13:13:21,356 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T13:13:21,363 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33103.
2025-04-30T13:13:21,365 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.4:33103
2025-04-30T13:13:21,367 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 1 on host 172.18.0.3
2025-04-30T13:13:21,368 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T13:13:21,378 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T13:13:21,378 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T13:13:21,392 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(2, 172.18.0.4, 33103, None)
2025-04-30T13:13:21,407 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40453.
2025-04-30T13:13:21,410 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.5:40453
2025-04-30T13:13:21,414 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.4:33103 with 434.4 MiB RAM, BlockManagerId(2, 172.18.0.4, 33103, None)
2025-04-30T13:13:21,416 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T13:13:21,423 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(2, 172.18.0.4, 33103, None)
2025-04-30T13:13:21,427 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(2, 172.18.0.4, 33103, None)
2025-04-30T13:13:21,430 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(0, 172.18.0.5, 40453, None)
2025-04-30T13:13:21,440 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T13:13:21,446 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@540144a3 for default.
2025-04-30T13:13:21,447 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.5:40453 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.5, 40453, None)
2025-04-30T13:13:21,456 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(0, 172.18.0.5, 40453, None)
2025-04-30T13:13:21,461 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(0, 172.18.0.5, 40453, None)
2025-04-30T13:13:21,466 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 30 ms to list leaf files for 1 paths.
2025-04-30T13:13:21,481 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43007.
2025-04-30T13:13:21,483 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.3:43007
2025-04-30T13:13:21,484 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T13:13:21,486 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@25c7034b for default.
2025-04-30T13:13:21,492 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T13:13:21,509 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(1, 172.18.0.3, 43007, None)
2025-04-30T13:13:21,524 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.3:43007 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.3, 43007, None)
2025-04-30T13:13:21,535 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(1, 172.18.0.3, 43007, None)
2025-04-30T13:13:21,538 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(1, 172.18.0.3, 43007, None)
2025-04-30T13:13:21,550 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T13:13:21,553 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@51427552 for default.
2025-04-30T13:13:22,906 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T13:13:22,908 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T13:13:23,083 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 200.3 KiB, free 434.2 MiB)
2025-04-30T13:13:23,129 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
2025-04-30T13:13:23,132 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on bb9210fe839d:46295 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T13:13:23,136 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 0 from json at <unknown>:0
2025-04-30T13:13:23,147 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T13:13:23,265 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: json at <unknown>:0
2025-04-30T13:13:23,279 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 0 (json at <unknown>:0) with 1 output partitions
2025-04-30T13:13:23,280 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 0 (json at <unknown>:0)
2025-04-30T13:13:23,281 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T13:13:23,283 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T13:13:23,288 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0), which has no missing parents
2025-04-30T13:13:23,363 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.5 KiB, free 434.2 MiB)
2025-04-30T13:13:23,367 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
2025-04-30T13:13:23,369 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on bb9210fe839d:46295 (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T13:13:23,371 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 1 from broadcast at DAGScheduler.scala:1585
2025-04-30T13:13:23,388 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T13:13:23,390 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 0.0 with 1 tasks resource profile 0
2025-04-30T13:13:23,429 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T13:13:23,458 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 0
2025-04-30T13:13:23,470 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 0.0 (TID 0)
2025-04-30T13:13:23,614 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:13:23,660 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:46295 after 2 ms (0 ms spent in bootstraps)
2025-04-30T13:13:23,702 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.4 MiB)
2025-04-30T13:13:23,709 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on 172.18.0.5:40453 (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T13:13:23,715 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 1 took 100 ms
2025-04-30T13:13:23,786 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.5 KiB, free 434.4 MiB)
2025-04-30T13:13:24,086 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl, range: 0-32688, partition values: [empty row]
2025-04-30T13:13:24,385 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 172.817421 ms
2025-04-30T13:13:24,387 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:13:24,395 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.3 MiB)
2025-04-30T13:13:24,397 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on 172.18.0.5:40453 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T13:13:24,400 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 0 took 12 ms
2025-04-30T13:13:24,470 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T13:13:24,615 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 0.0 (TID 0). 2450 bytes result sent to driver
2025-04-30T13:13:24,628 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 0.0 (TID 0) in 1219 ms on 172.18.0.5 (executor 0) (1/1)
2025-04-30T13:13:24,630 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-04-30T13:13:24,634 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 0 (json at <unknown>:0) finished in 1.331 s
2025-04-30T13:13:24,637 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T13:13:24,639 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 0: Stage finished
2025-04-30T13:13:24,642 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 finished: json at <unknown>:0, took 1.376893 s
2025-04-30T13:13:24,816 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T13:13:24,817 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T13:13:25,201 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 226.116611 ms
2025-04-30T13:13:25,208 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 200.2 KiB, free 434.0 MiB)
2025-04-30T13:13:25,225 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.9 MiB)
2025-04-30T13:13:25,227 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on bb9210fe839d:46295 (size: 34.4 KiB, free: 434.3 MiB)
2025-04-30T13:13:25,229 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 2 from count at <unknown>:0
2025-04-30T13:13:25,234 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T13:13:25,274 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Registering RDD 7 (count at <unknown>:0) as input to shuffle 0
2025-04-30T13:13:25,280 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got map stage job 1 (count at <unknown>:0) with 1 output partitions
2025-04-30T13:13:25,281 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ShuffleMapStage 1 (count at <unknown>:0)
2025-04-30T13:13:25,282 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T13:13:25,284 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T13:13:25,287 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0), which has no missing parents
2025-04-30T13:13:25,311 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 433.9 MiB)
2025-04-30T13:13:25,315 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 433.9 MiB)
2025-04-30T13:13:25,317 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on bb9210fe839d:46295 (size: 8.3 KiB, free: 434.3 MiB)
2025-04-30T13:13:25,319 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 3 from broadcast at DAGScheduler.scala:1585
2025-04-30T13:13:25,322 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T13:13:25,323 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 1.0 with 1 tasks resource profile 0
2025-04-30T13:13:25,327 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 9638 bytes) 
2025-04-30T13:13:25,332 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 1
2025-04-30T13:13:25,333 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 1.0 (TID 1)
2025-04-30T13:13:25,356 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:13:25,368 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 434.0 MiB)
2025-04-30T13:13:25,372 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on 172.18.0.5:40453 (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T13:13:25,375 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 3 took 17 ms
2025-04-30T13:13:25,379 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 433.9 MiB)
2025-04-30T13:13:25,490 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 27.876207 ms
2025-04-30T13:13:25,504 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl, range: 0-32688, partition values: [empty row]
2025-04-30T13:13:25,517 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 9.477378 ms
2025-04-30T13:13:25,532 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:13:25,539 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.9 MiB)
2025-04-30T13:13:25,543 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on 172.18.0.5:40453 (size: 34.4 KiB, free: 434.3 MiB)
2025-04-30T13:13:25,545 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 2 took 12 ms
2025-04-30T13:13:25,563 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 377.3 KiB, free 433.5 MiB)
2025-04-30T13:13:25,671 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 1.0 (TID 1). 1964 bytes result sent to driver
2025-04-30T13:13:25,679 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 1.0 (TID 1) in 354 ms on 172.18.0.5 (executor 0) (1/1)
2025-04-30T13:13:25,680 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-04-30T13:13:25,683 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ShuffleMapStage 1 (count at <unknown>:0) finished in 0.389 s
2025-04-30T13:13:25,684 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - looking for newly runnable stages
2025-04-30T13:13:25,686 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - running: Set()
2025-04-30T13:13:25,686 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - waiting: Set()
2025-04-30T13:13:25,688 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - failed: Set()
2025-04-30T13:13:25,769 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 28.13492 ms
2025-04-30T13:13:25,818 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: count at <unknown>:0
2025-04-30T13:13:25,822 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 2 (count at <unknown>:0) with 1 output partitions
2025-04-30T13:13:25,823 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 3 (count at <unknown>:0)
2025-04-30T13:13:25,824 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List(ShuffleMapStage 2)
2025-04-30T13:13:25,825 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T13:13:25,828 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0), which has no missing parents
2025-04-30T13:13:25,842 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 433.9 MiB)
2025-04-30T13:13:25,860 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 433.9 MiB)
2025-04-30T13:13:25,863 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_4_piece0 in memory on bb9210fe839d:46295 (size: 6.0 KiB, free: 434.3 MiB)
2025-04-30T13:13:25,864 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 4 from broadcast at DAGScheduler.scala:1585
2025-04-30T13:13:25,867 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T13:13:25,868 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 3.0 with 1 tasks resource profile 0
2025-04-30T13:13:25,878 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 3.0 (TID 2) (172.18.0.5, executor 0, partition 0, NODE_LOCAL, 9003 bytes) 
2025-04-30T13:13:25,883 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 2
2025-04-30T13:13:25,885 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 3.0 (TID 2)
2025-04-30T13:13:25,896 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2025-04-30T13:13:25,906 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:13:25,916 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_0_piece0 on bb9210fe839d:46295 in memory (size: 34.5 KiB, free: 434.3 MiB)
2025-04-30T13:13:25,928 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_0_piece0 on 172.18.0.5:40453 in memory (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T13:13:25,929 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 433.6 MiB)
2025-04-30T13:13:25,941 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_4_piece0 in memory on 172.18.0.5:40453 (size: 6.0 KiB, free: 434.3 MiB)
2025-04-30T13:13:25,947 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 4 took 39 ms
2025-04-30T13:13:25,949 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 433.9 MiB)
2025-04-30T13:13:25,965 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on bb9210fe839d:46295 in memory (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T13:13:25,973 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on 172.18.0.5:40453 in memory (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T13:13:25,990 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_3_piece0 on bb9210fe839d:46295 in memory (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T13:13:25,994 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Don't have map outputs for shuffle 0, fetching them
2025-04-30T13:13:25,997 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_3_piece0 on 172.18.0.5:40453 in memory (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T13:13:26,001 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@bb9210fe839d:44967)
2025-04-30T13:13:26,014 [dispatcher-event-loop-7] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - Asked to send map output locations for shuffle 0 to 172.18.0.5:34996
2025-04-30T13:13:26,098 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Got the map output locations
2025-04-30T13:13:26,136 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-04-30T13:13:26,139 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Started 0 remote fetches in 16 ms
2025-04-30T13:13:26,168 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 20.687356 ms
2025-04-30T13:13:26,185 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 3.0 (TID 2). 4038 bytes result sent to driver
2025-04-30T13:13:26,195 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 3.0 (TID 2) in 318 ms on 172.18.0.5 (executor 0) (1/1)
2025-04-30T13:13:26,195 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2025-04-30T13:13:26,198 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 3 (count at <unknown>:0) finished in 0.358 s
2025-04-30T13:13:26,199 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T13:13:26,200 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 3: Stage finished
2025-04-30T13:13:26,202 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 2 finished: count at <unknown>:0, took 0.382064 s
2025-04-30T13:13:26,327 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T13:13:26,328 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T13:13:26,418 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils [] - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:13:26,453 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:13:26,454 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:13:26,457 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:13:26,458 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:13:26,462 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:13:26,465 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:13:26,503 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5 stored as values in memory (estimated size 200.2 KiB, free 434.0 MiB)
2025-04-30T13:13:26,522 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_4_piece0 on bb9210fe839d:46295 in memory (size: 6.0 KiB, free: 434.4 MiB)
2025-04-30T13:13:26,532 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_4_piece0 on 172.18.0.5:40453 in memory (size: 6.0 KiB, free: 434.4 MiB)
2025-04-30T13:13:26,538 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.9 MiB)
2025-04-30T13:13:26,541 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_5_piece0 in memory on bb9210fe839d:46295 (size: 34.4 KiB, free: 434.3 MiB)
2025-04-30T13:13:26,543 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 5 from parquet at <unknown>:0
2025-04-30T13:13:26,547 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T13:13:26,573 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_2_piece0 on 172.18.0.5:40453 in memory (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T13:13:26,576 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_2_piece0 on bb9210fe839d:46295 in memory (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T13:13:26,580 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: parquet at <unknown>:0
2025-04-30T13:13:26,583 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 3 (parquet at <unknown>:0) with 1 output partitions
2025-04-30T13:13:26,585 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 4 (parquet at <unknown>:0)
2025-04-30T13:13:26,587 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T13:13:26,590 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T13:13:26,597 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 4 (MapPartitionsRDD[13] at parquet at <unknown>:0), which has no missing parents
2025-04-30T13:13:26,706 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6 stored as values in memory (estimated size 215.3 KiB, free 434.0 MiB)
2025-04-30T13:13:26,728 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 77.4 KiB, free 433.9 MiB)
2025-04-30T13:13:26,734 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_6_piece0 in memory on bb9210fe839d:46295 (size: 77.4 KiB, free: 434.3 MiB)
2025-04-30T13:13:26,736 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 6 from broadcast at DAGScheduler.scala:1585
2025-04-30T13:13:26,738 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at parquet at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T13:13:26,738 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 4.0 with 1 tasks resource profile 0
2025-04-30T13:13:26,741 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 4.0 (TID 3) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T13:13:26,748 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 3
2025-04-30T13:13:26,750 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 4.0 (TID 3)
2025-04-30T13:13:26,756 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:13:26,766 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 77.4 KiB, free 434.3 MiB)
2025-04-30T13:13:26,770 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_6_piece0 in memory on 172.18.0.5:40453 (size: 77.4 KiB, free: 434.3 MiB)
2025-04-30T13:13:26,773 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 6 took 16 ms
2025-04-30T13:13:26,776 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6 stored as values in memory (estimated size 215.3 KiB, free 434.1 MiB)
2025-04-30T13:13:26,844 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:13:26,845 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:13:26,848 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:13:26,848 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:13:26,848 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:13:26,849 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:13:26,860 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:13:26,863 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:13:26,896 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat [] - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-04-30T13:13:26,975 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport [] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "date_birth",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "driver_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt_current_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "first_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "last_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "license_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "phone_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_make",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_model",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_year",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary city (STRING);
  optional binary country (STRING);
  optional binary date_birth (STRING);
  optional int64 driver_id;
  optional binary dt_current_timestamp (STRING);
  optional binary first_name (STRING);
  optional binary last_name (STRING);
  optional binary license_number (STRING);
  optional binary phone_number (STRING);
  optional binary uuid (STRING);
  optional binary vehicle_make (STRING);
  optional binary vehicle_model (STRING);
  optional binary vehicle_type (STRING);
  optional int64 vehicle_year;
}

       
2025-04-30T13:13:27,046 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.io.compress.CodecPool [] - Got brand-new compressor [.snappy]
2025-04-30T13:13:27,284 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl, range: 0-32688, partition values: [empty row]
2025-04-30T13:13:27,410 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 63.810558 ms
2025-04-30T13:13:27,416 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 5 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:13:27,427 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.1 MiB)
2025-04-30T13:13:27,431 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_5_piece0 in memory on 172.18.0.5:40453 (size: 34.4 KiB, free: 434.3 MiB)
2025-04-30T13:13:27,435 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 5 took 17 ms
2025-04-30T13:13:27,454 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5 stored as values in memory (estimated size 377.3 KiB, free 433.7 MiB)
2025-04-30T13:13:27,816 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - Saved output of task 'attempt_202504301313264354538036669692179_0004_m_000000_3' to file:/tmp/spark-output/drivers/_temporary/0/task_202504301313264354538036669692179_0004_m_000000
2025-04-30T13:13:27,818 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil [] - attempt_202504301313264354538036669692179_0004_m_000000_3: Committed. Elapsed time: 2 ms.
2025-04-30T13:13:27,825 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 4.0 (TID 3). 2545 bytes result sent to driver
2025-04-30T13:13:27,832 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 4.0 (TID 3) in 1091 ms on 172.18.0.5 (executor 0) (1/1)
2025-04-30T13:13:27,833 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2025-04-30T13:13:27,835 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 4 (parquet at <unknown>:0) finished in 1.220 s
2025-04-30T13:13:27,836 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T13:13:27,837 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 4: Stage finished
2025-04-30T13:13:27,839 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 3 finished: parquet at <unknown>:0, took 1.256545 s
2025-04-30T13:13:27,842 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter [] - Start to commit write Job ede527f5-0402-4739-b086-444227349a2b.
2025-04-30T13:13:27,870 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter [] - Write Job ede527f5-0402-4739-b086-444227349a2b committed. Elapsed time: 26 ms.
2025-04-30T13:13:27,878 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter [] - Finished processing stats for write job ede527f5-0402-4739-b086-444227349a2b.
2025-04-30T13:13:27,928 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T13:13:27,929 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T13:13:27,958 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_7 stored as values in memory (estimated size 200.1 KiB, free 433.7 MiB)
2025-04-30T13:13:27,976 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_7_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.7 MiB)
2025-04-30T13:13:27,979 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_7_piece0 in memory on bb9210fe839d:46295 (size: 34.4 KiB, free: 434.3 MiB)
2025-04-30T13:13:27,981 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 7 from count at <unknown>:0
2025-04-30T13:13:27,984 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T13:13:28,003 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Registering RDD 17 (count at <unknown>:0) as input to shuffle 1
2025-04-30T13:13:28,005 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got map stage job 4 (count at <unknown>:0) with 2 output partitions
2025-04-30T13:13:28,006 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ShuffleMapStage 5 (count at <unknown>:0)
2025-04-30T13:13:28,007 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T13:13:28,009 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T13:13:28,014 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ShuffleMapStage 5 (MapPartitionsRDD[17] at count at <unknown>:0), which has no missing parents
2025-04-30T13:13:28,019 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_8 stored as values in memory (estimated size 17.0 KiB, free 433.6 MiB)
2025-04-30T13:13:28,023 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 433.6 MiB)
2025-04-30T13:13:28,025 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_8_piece0 in memory on bb9210fe839d:46295 (size: 8.3 KiB, free: 434.2 MiB)
2025-04-30T13:13:28,027 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 8 from broadcast at DAGScheduler.scala:1585
2025-04-30T13:13:28,029 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[17] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
2025-04-30T13:13:28,031 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 5.0 with 2 tasks resource profile 0
2025-04-30T13:13:28,035 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 5.0 (TID 4) (172.18.0.4, executor 2, partition 0, PROCESS_LOCAL, 9646 bytes) 
2025-04-30T13:13:28,038 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 1.0 in stage 5.0 (TID 5) (172.18.0.5, executor 0, partition 1, PROCESS_LOCAL, 9639 bytes) 
2025-04-30T13:13:28,045 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 5
2025-04-30T13:13:28,047 [Executor task launch worker for task 1.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.executor.Executor [] - Running task 1.0 in stage 5.0 (TID 5)
2025-04-30T13:13:28,058 [Executor task launch worker for task 1.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 8 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:13:28,066 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 4
2025-04-30T13:13:28,070 [Executor task launch worker for task 1.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 433.7 MiB)
2025-04-30T13:13:28,075 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_8_piece0 in memory on 172.18.0.5:40453 (size: 8.3 KiB, free: 434.3 MiB)
2025-04-30T13:13:28,082 [Executor task launch worker for task 1.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 8 took 23 ms
2025-04-30T13:13:28,085 [Executor task launch worker for task 1.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_8 stored as values in memory (estimated size 17.0 KiB, free 433.7 MiB)
2025-04-30T13:13:28,095 [Executor task launch worker for task 0.0 in stage 5.0 (TID 4)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 5.0 (TID 4)
2025-04-30T13:13:28,095 [Executor task launch worker for task 1.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl, range: 0-35354, partition values: [empty row]
2025-04-30T13:13:28,102 [Executor task launch worker for task 1.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 7 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:13:28,115 [Executor task launch worker for task 1.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_7_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.7 MiB)
2025-04-30T13:13:28,119 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_7_piece0 in memory on 172.18.0.5:40453 (size: 34.4 KiB, free: 434.2 MiB)
2025-04-30T13:13:28,121 [Executor task launch worker for task 1.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 7 took 18 ms
2025-04-30T13:13:28,155 [Executor task launch worker for task 1.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_7 stored as values in memory (estimated size 377.3 KiB, free 433.3 MiB)
2025-04-30T13:13:28,207 [Executor task launch worker for task 0.0 in stage 5.0 (TID 4)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2025-04-30T13:13:28,224 [Executor task launch worker for task 1.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.executor.Executor [] - Finished task 1.0 in stage 5.0 (TID 5). 1921 bytes result sent to driver
2025-04-30T13:13:28,231 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 1.0 in stage 5.0 (TID 5) in 194 ms on 172.18.0.5 (executor 0) (1/2)
2025-04-30T13:13:28,283 [Executor task launch worker for task 0.0 in stage 5.0 (TID 4)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 8 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:13:28,322 [Executor task launch worker for task 0.0 in stage 5.0 (TID 4)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.5:40453 after 2 ms (0 ms spent in bootstraps)
2025-04-30T13:13:28,368 [Executor task launch worker for task 0.0 in stage 5.0 (TID 4)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 434.4 MiB)
2025-04-30T13:13:28,373 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_8_piece0 in memory on 172.18.0.4:33103 (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T13:13:28,378 [Executor task launch worker for task 0.0 in stage 5.0 (TID 4)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 8 took 94 ms
2025-04-30T13:13:28,431 [Executor task launch worker for task 0.0 in stage 5.0 (TID 4)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_8 stored as values in memory (estimated size 17.0 KiB, free 434.4 MiB)
2025-04-30T13:13:29,138 [Executor task launch worker for task 0.0 in stage 5.0 (TID 4)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 229.599321 ms
2025-04-30T13:13:29,151 [Executor task launch worker for task 0.0 in stage 5.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY%20copy.jsonl, range: 0-35354, partition values: [empty row]
2025-04-30T13:13:29,186 [Executor task launch worker for task 0.0 in stage 5.0 (TID 4)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 12.124019 ms
2025-04-30T13:13:29,205 [Executor task launch worker for task 0.0 in stage 5.0 (TID 4)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 7 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:13:29,216 [Executor task launch worker for task 0.0 in stage 5.0 (TID 4)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_7_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.3 MiB)
2025-04-30T13:13:29,219 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_7_piece0 in memory on 172.18.0.4:33103 (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T13:13:29,222 [Executor task launch worker for task 0.0 in stage 5.0 (TID 4)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 7 took 16 ms
2025-04-30T13:13:29,322 [Executor task launch worker for task 0.0 in stage 5.0 (TID 4)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_7 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T13:13:29,623 [Executor task launch worker for task 0.0 in stage 5.0 (TID 4)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 5.0 (TID 4). 2007 bytes result sent to driver
2025-04-30T13:13:29,630 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 5.0 (TID 4) in 1596 ms on 172.18.0.4 (executor 2) (2/2)
2025-04-30T13:13:29,631 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2025-04-30T13:13:29,633 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ShuffleMapStage 5 (count at <unknown>:0) finished in 1.615 s
2025-04-30T13:13:29,634 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - looking for newly runnable stages
2025-04-30T13:13:29,635 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - running: Set()
2025-04-30T13:13:29,636 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - waiting: Set()
2025-04-30T13:13:29,637 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - failed: Set()
2025-04-30T13:13:29,684 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: count at <unknown>:0
2025-04-30T13:13:29,686 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 5 (count at <unknown>:0) with 1 output partitions
2025-04-30T13:13:29,687 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 7 (count at <unknown>:0)
2025-04-30T13:13:29,687 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List(ShuffleMapStage 6)
2025-04-30T13:13:29,688 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T13:13:29,689 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 7 (MapPartitionsRDD[20] at count at <unknown>:0), which has no missing parents
2025-04-30T13:13:29,697 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_9 stored as values in memory (estimated size 12.5 KiB, free 433.6 MiB)
2025-04-30T13:13:29,700 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 433.6 MiB)
2025-04-30T13:13:29,702 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_9_piece0 in memory on bb9210fe839d:46295 (size: 6.0 KiB, free: 434.2 MiB)
2025-04-30T13:13:29,705 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 9 from broadcast at DAGScheduler.scala:1585
2025-04-30T13:13:29,708 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[20] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T13:13:29,709 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 7.0 with 1 tasks resource profile 0
2025-04-30T13:13:29,712 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 7.0 (TID 6) (172.18.0.4, executor 2, partition 0, NODE_LOCAL, 9003 bytes) 
2025-04-30T13:13:29,717 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 6
2025-04-30T13:13:29,719 [Executor task launch worker for task 0.0 in stage 7.0 (TID 6)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 7.0 (TID 6)
2025-04-30T13:13:29,729 [Executor task launch worker for task 0.0 in stage 7.0 (TID 6)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 2 and clearing cache
2025-04-30T13:13:29,749 [Executor task launch worker for task 0.0 in stage 7.0 (TID 6)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 9 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:13:29,760 [Executor task launch worker for task 0.0 in stage 7.0 (TID 6)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:46295 after 3 ms (0 ms spent in bootstraps)
2025-04-30T13:13:29,773 [Executor task launch worker for task 0.0 in stage 7.0 (TID 6)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.0 MiB)
2025-04-30T13:13:29,777 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_9_piece0 in memory on 172.18.0.4:33103 (size: 6.0 KiB, free: 434.4 MiB)
2025-04-30T13:13:29,779 [Executor task launch worker for task 0.0 in stage 7.0 (TID 6)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 9 took 29 ms
2025-04-30T13:13:29,781 [Executor task launch worker for task 0.0 in stage 7.0 (TID 6)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_9 stored as values in memory (estimated size 12.5 KiB, free 434.0 MiB)
2025-04-30T13:13:29,871 [Executor task launch worker for task 0.0 in stage 7.0 (TID 6)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Don't have map outputs for shuffle 1, fetching them
2025-04-30T13:13:29,874 [Executor task launch worker for task 0.0 in stage 7.0 (TID 6)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@bb9210fe839d:44967)
2025-04-30T13:13:29,878 [dispatcher-event-loop-4] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - Asked to send map output locations for shuffle 1 to 172.18.0.4:32928
2025-04-30T13:13:29,902 [Executor task launch worker for task 0.0 in stage 7.0 (TID 6)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Got the map output locations
2025-04-30T13:13:29,954 [Executor task launch worker for task 0.0 in stage 7.0 (TID 6)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Getting 2 (120.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 1 (60.0 B) remote blocks
2025-04-30T13:13:29,966 [Executor task launch worker for task 0.0 in stage 7.0 (TID 6)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Started 1 remote fetches in 36 ms
2025-04-30T13:13:30,007 [Executor task launch worker for task 0.0 in stage 7.0 (TID 6)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 25.649409 ms
2025-04-30T13:13:30,034 [Executor task launch worker for task 0.0 in stage 7.0 (TID 6)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 7.0 (TID 6). 4081 bytes result sent to driver
2025-04-30T13:13:30,041 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 7.0 (TID 6) in 328 ms on 172.18.0.4 (executor 2) (1/1)
2025-04-30T13:13:30,042 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2025-04-30T13:13:30,044 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 7 (count at <unknown>:0) finished in 0.350 s
2025-04-30T13:13:30,045 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T13:13:30,046 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 7: Stage finished
2025-04-30T13:13:30,047 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 5 finished: count at <unknown>:0, took 0.362744 s
2025-04-30T13:13:30,100 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T13:13:30,101 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T13:13:30,142 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_10 stored as values in memory (estimated size 200.2 KiB, free 433.4 MiB)
2025-04-30T13:13:30,159 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.4 MiB)
2025-04-30T13:13:30,162 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_10_piece0 in memory on bb9210fe839d:46295 (size: 34.4 KiB, free: 434.2 MiB)
2025-04-30T13:13:30,164 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 10 from count at <unknown>:0
2025-04-30T13:13:30,167 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T13:13:30,181 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Registering RDD 24 (count at <unknown>:0) as input to shuffle 2
2025-04-30T13:13:30,182 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got map stage job 6 (count at <unknown>:0) with 1 output partitions
2025-04-30T13:13:30,183 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ShuffleMapStage 8 (count at <unknown>:0)
2025-04-30T13:13:30,183 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T13:13:30,184 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T13:13:30,186 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ShuffleMapStage 8 (MapPartitionsRDD[24] at count at <unknown>:0), which has no missing parents
2025-04-30T13:13:30,192 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_11 stored as values in memory (estimated size 17.0 KiB, free 433.4 MiB)
2025-04-30T13:13:30,196 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_11_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 433.4 MiB)
2025-04-30T13:13:30,198 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_11_piece0 in memory on bb9210fe839d:46295 (size: 8.3 KiB, free: 434.2 MiB)
2025-04-30T13:13:30,199 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 11 from broadcast at DAGScheduler.scala:1585
2025-04-30T13:13:30,201 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[24] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T13:13:30,202 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 8.0 with 1 tasks resource profile 0
2025-04-30T13:13:30,207 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 8.0 (TID 7) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 9638 bytes) 
2025-04-30T13:13:30,226 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 7
2025-04-30T13:13:30,247 [Executor task launch worker for task 0.0 in stage 8.0 (TID 7)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 8.0 (TID 7)
2025-04-30T13:13:30,322 [Executor task launch worker for task 0.0 in stage 8.0 (TID 7)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 2 and clearing cache
2025-04-30T13:13:30,378 [Executor task launch worker for task 0.0 in stage 8.0 (TID 7)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 11 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:13:30,417 [Executor task launch worker for task 0.0 in stage 8.0 (TID 7)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to bb9210fe839d/172.18.0.2:46295 after 3 ms (0 ms spent in bootstraps)
2025-04-30T13:13:30,444 [Executor task launch worker for task 0.0 in stage 8.0 (TID 7)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_11_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 434.4 MiB)
2025-04-30T13:13:30,450 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_11_piece0 in memory on 172.18.0.3:43007 (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T13:13:30,455 [Executor task launch worker for task 0.0 in stage 8.0 (TID 7)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 11 took 76 ms
2025-04-30T13:13:30,527 [Executor task launch worker for task 0.0 in stage 8.0 (TID 7)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_11 stored as values in memory (estimated size 17.0 KiB, free 434.4 MiB)
2025-04-30T13:13:31,132 [Executor task launch worker for task 0.0 in stage 8.0 (TID 7)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 198.150916 ms
2025-04-30T13:13:31,143 [Executor task launch worker for task 0.0 in stage 8.0 (TID 7)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl, range: 0-32688, partition values: [empty row]
2025-04-30T13:13:31,174 [Executor task launch worker for task 0.0 in stage 8.0 (TID 7)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 10.502837 ms
2025-04-30T13:13:31,191 [Executor task launch worker for task 0.0 in stage 8.0 (TID 7)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 10 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:13:31,200 [Executor task launch worker for task 0.0 in stage 8.0 (TID 7)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.3 MiB)
2025-04-30T13:13:31,203 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_10_piece0 in memory on 172.18.0.3:43007 (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T13:13:31,206 [Executor task launch worker for task 0.0 in stage 8.0 (TID 7)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 10 took 13 ms
2025-04-30T13:13:31,282 [Executor task launch worker for task 0.0 in stage 8.0 (TID 7)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_10 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T13:13:31,491 [Executor task launch worker for task 0.0 in stage 8.0 (TID 7)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 8.0 (TID 7). 2007 bytes result sent to driver
2025-04-30T13:13:31,498 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 8.0 (TID 7) in 1292 ms on 172.18.0.3 (executor 1) (1/1)
2025-04-30T13:13:31,499 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 8.0, whose tasks have all completed, from pool 
2025-04-30T13:13:31,500 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ShuffleMapStage 8 (count at <unknown>:0) finished in 1.312 s
2025-04-30T13:13:31,501 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - looking for newly runnable stages
2025-04-30T13:13:31,501 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - running: Set()
2025-04-30T13:13:31,503 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - waiting: Set()
2025-04-30T13:13:31,503 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - failed: Set()
2025-04-30T13:13:31,540 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: count at <unknown>:0
2025-04-30T13:13:31,542 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 7 (count at <unknown>:0) with 1 output partitions
2025-04-30T13:13:31,545 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 10 (count at <unknown>:0)
2025-04-30T13:13:31,546 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List(ShuffleMapStage 9)
2025-04-30T13:13:31,547 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T13:13:31,549 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 10 (MapPartitionsRDD[27] at count at <unknown>:0), which has no missing parents
2025-04-30T13:13:31,555 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_12 stored as values in memory (estimated size 12.5 KiB, free 433.3 MiB)
2025-04-30T13:13:31,559 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_12_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 433.3 MiB)
2025-04-30T13:13:31,562 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_12_piece0 in memory on bb9210fe839d:46295 (size: 6.0 KiB, free: 434.2 MiB)
2025-04-30T13:13:31,564 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 12 from broadcast at DAGScheduler.scala:1585
2025-04-30T13:13:31,567 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[27] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T13:13:31,568 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 10.0 with 1 tasks resource profile 0
2025-04-30T13:13:31,571 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 10.0 (TID 8) (172.18.0.3, executor 1, partition 0, NODE_LOCAL, 9003 bytes) 
2025-04-30T13:13:31,577 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 8
2025-04-30T13:13:31,579 [Executor task launch worker for task 0.0 in stage 10.0 (TID 8)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 10.0 (TID 8)
2025-04-30T13:13:31,587 [Executor task launch worker for task 0.0 in stage 10.0 (TID 8)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 3 and clearing cache
2025-04-30T13:13:31,607 [Executor task launch worker for task 0.0 in stage 10.0 (TID 8)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 12 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:13:31,618 [Executor task launch worker for task 0.0 in stage 10.0 (TID 8)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_12_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.0 MiB)
2025-04-30T13:13:31,621 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_12_piece0 in memory on 172.18.0.3:43007 (size: 6.0 KiB, free: 434.4 MiB)
2025-04-30T13:13:31,625 [Executor task launch worker for task 0.0 in stage 10.0 (TID 8)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 12 took 16 ms
2025-04-30T13:13:31,628 [Executor task launch worker for task 0.0 in stage 10.0 (TID 8)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_12 stored as values in memory (estimated size 12.5 KiB, free 434.0 MiB)
2025-04-30T13:13:31,689 [Executor task launch worker for task 0.0 in stage 10.0 (TID 8)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Don't have map outputs for shuffle 2, fetching them
2025-04-30T13:13:31,692 [Executor task launch worker for task 0.0 in stage 10.0 (TID 8)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@bb9210fe839d:44967)
2025-04-30T13:13:31,708 [dispatcher-event-loop-6] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - Asked to send map output locations for shuffle 2 to 172.18.0.3:45498
2025-04-30T13:13:31,728 [Executor task launch worker for task 0.0 in stage 10.0 (TID 8)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Got the map output locations
2025-04-30T13:13:31,757 [Executor task launch worker for task 0.0 in stage 10.0 (TID 8)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-04-30T13:13:31,762 [Executor task launch worker for task 0.0 in stage 10.0 (TID 8)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Started 0 remote fetches in 16 ms
2025-04-30T13:13:31,790 [Executor task launch worker for task 0.0 in stage 10.0 (TID 8)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 21.102477 ms
2025-04-30T13:13:31,807 [Executor task launch worker for task 0.0 in stage 10.0 (TID 8)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 10.0 (TID 8). 4081 bytes result sent to driver
2025-04-30T13:13:31,812 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 10.0 (TID 8) in 242 ms on 172.18.0.3 (executor 1) (1/1)
2025-04-30T13:13:31,813 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 10.0, whose tasks have all completed, from pool 
2025-04-30T13:13:31,814 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 10 (count at <unknown>:0) finished in 0.263 s
2025-04-30T13:13:31,815 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T13:13:31,816 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 10: Stage finished
2025-04-30T13:13:31,817 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 7 finished: count at <unknown>:0, took 0.275788 s
2025-04-30T13:13:31,822 [Thread-4] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-30T13:13:31,836 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@413b8567{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T13:13:31,841 [Thread-4] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://bb9210fe839d:4040
2025-04-30T13:13:31,849 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-30T13:13:31,850 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-30T13:13:31,856 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T13:13:31,857 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T13:13:31,859 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T13:13:31,866 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250430131316-0020
2025-04-30T13:13:31,868 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250430131316-0020
2025-04-30T13:13:31,878 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430131316-0020/1
2025-04-30T13:13:31,879 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430131316-0020/2
2025-04-30T13:13:31,881 [ExecutorRunner for app-20250430131316-0020/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430131316-0020/1 interrupted
2025-04-30T13:13:31,881 [ExecutorRunner for app-20250430131316-0020/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430131316-0020/2 interrupted
2025-04-30T13:13:31,881 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430131316-0020/0
2025-04-30T13:13:31,882 [ExecutorRunner for app-20250430131316-0020/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T13:13:31,883 [ExecutorRunner for app-20250430131316-0020/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430131316-0020/0 interrupted
2025-04-30T13:13:31,884 [ExecutorRunner for app-20250430131316-0020/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T13:13:31,886 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T13:13:31,889 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T13:13:31,881 [ExecutorRunner for app-20250430131316-0020/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T13:13:31,893 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T13:13:31,898 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T13:13:31,899 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T13:13:31,900 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T13:13:31,902 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T13:13:31,902 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T13:13:31,912 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T13:13:31,916 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T13:13:31,931 [dispatcher-event-loop-4] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-30T13:13:31,932 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T13:13:31,933 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T13:13:31,953 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T13:13:31,954 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T13:13:31,963 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-30T13:13:31,969 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-30T13:13:31,970 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430131316-0020/2 finished with state KILLED exitStatus 143
2025-04-30T13:13:31,972 [dispatcher-event-loop-1] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430131316-0020/2
2025-04-30T13:13:31,973 [dispatcher-event-loop-11] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-30T13:13:31,974 [dispatcher-event-loop-11] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430131316-0020, execId=2)
2025-04-30T13:13:31,975 [dispatcher-event-loop-11] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430131316-0020 removed, cleanupLocalDirs = true
2025-04-30T13:13:31,975 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430131316-0020
2025-04-30T13:13:31,983 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:39274 got disassociated, removing it.
2025-04-30T13:13:31,983 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.master.Master [] - bb9210fe839d:44967 got disassociated, removing it.
2025-04-30T13:13:31,984 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430131316-0020/0 finished with state KILLED exitStatus 143
2025-04-30T13:13:31,985 [dispatcher-event-loop-11] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430131316-0020/0
2025-04-30T13:13:31,986 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-30T13:13:31,986 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430131316-0020, execId=0)
2025-04-30T13:13:31,987 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430131316-0020 removed, cleanupLocalDirs = true
2025-04-30T13:13:31,987 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430131316-0020
2025-04-30T13:13:31,992 [dispatcher-event-loop-5] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430131316-0020/1
2025-04-30T13:13:31,991 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430131316-0020/1 finished with state KILLED exitStatus 143
2025-04-30T13:13:31,995 [Thread-4] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-30T13:13:31,995 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-30T13:13:31,996 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430131316-0020, execId=1)
2025-04-30T13:13:31,997 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430131316-0020
2025-04-30T13:13:31,997 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430131316-0020 removed, cleanupLocalDirs = true
2025-04-30T13:13:32,329 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T13:13:32,330 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-4474c065-2325-4cbd-a585-9ff8cbd60411
2025-04-30T13:13:32,333 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-a6a3a75b-ee18-431f-be22-abed81c8a3a8/pyspark-0f009abc-86fe-40d6-bdf0-783be3c1217d
2025-04-30T13:13:32,336 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-a6a3a75b-ee18-431f-be22-abed81c8a3a8
2025-04-30T13:25:49,864 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.3:43452 got disassociated, removing it.
2025-04-30T13:25:49,865 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.3:34443 got disassociated, removing it.
2025-04-30T13:25:49,868 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.master.Master [] - Removing worker worker-20250429114342-172.18.0.3-34443 on 172.18.0.3:34443
2025-04-30T13:25:49,869 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.master.Master [] - Telling app of lost worker: worker-20250429114342-172.18.0.3-34443
2025-04-30T13:25:49,870 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.5:53732 got disassociated, removing it.
2025-04-30T13:25:49,872 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.5:38821 got disassociated, removing it.
2025-04-30T13:25:49,879 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.master.Master [] - Removing worker worker-20250429114342-172.18.0.5-38821 on 172.18.0.5:38821
2025-04-30T13:25:49,879 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.master.Master [] - Telling app of lost worker: worker-20250429114342-172.18.0.5-38821
2025-04-30T13:25:49,921 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.4:33758 got disassociated, removing it.
2025-04-30T13:25:49,922 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.4:36391 got disassociated, removing it.
2025-04-30T13:25:49,923 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - Removing worker worker-20250429114342-172.18.0.4-36391 on 172.18.0.4:36391
2025-04-30T13:25:49,923 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - Telling app of lost worker: worker-20250429114342-172.18.0.4-36391
2025-04-30T13:39:31,745 [main] INFO  org.apache.spark.deploy.master.Master [] - Started daemon with process name: 62@663dbd2abcb8
2025-04-30T13:39:31,778 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T13:39:31,781 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T13:39:31,782 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T13:39:32,250 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Started daemon with process name: 60@d5e10145fa97
2025-04-30T13:39:32,270 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T13:39:32,275 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T13:39:32,276 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T13:39:32,377 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:39:32,379 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:39:32,381 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:39:32,383 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:39:32,385 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:39:32,698 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Started daemon with process name: 60@167310a79d22
2025-04-30T13:39:32,706 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Started daemon with process name: 60@a5c865c0d920
2025-04-30T13:39:32,727 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T13:39:32,721 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T13:39:32,731 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T13:39:32,735 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T13:39:32,747 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T13:39:32,751 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T13:39:32,759 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:39:32,865 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:39:32,866 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:39:32,868 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:39:32,870 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:39:32,873 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:39:33,281 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:39:33,285 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:39:33,289 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:39:33,295 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:39:33,298 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:39:33,300 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:39:33,304 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:39:33,306 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:39:33,308 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:39:33,310 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:39:33,315 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:39:33,785 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:39:33,800 [main] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkMaster' on port 7077.
2025-04-30T13:39:33,801 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:39:33,900 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Starting Spark master at spark://663dbd2abcb8:7077
2025-04-30T13:39:33,972 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Running Spark version 3.5.5
2025-04-30T13:39:34,083 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @5166ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-30T13:39:34,175 [dispatcher-event-loop-1] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:8080 for MasterUI
2025-04-30T13:39:34,201 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-30T13:39:34,285 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - Started @5369ms
2025-04-30T13:39:34,368 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@61eb529f{HTTP/1.1, (http/1.1)}{0.0.0.0:8080}
2025-04-30T13:39:34,374 [dispatcher-event-loop-1] INFO  org.apache.spark.util.Utils [] - Successfully started service 'MasterUI' on port 8080.
2025-04-30T13:39:34,468 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6f742a8b{/app,null,AVAILABLE,@Spark}
2025-04-30T13:39:34,476 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6c9cf10{/app/json,null,AVAILABLE,@Spark}
2025-04-30T13:39:34,483 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@264bd6a{/,null,AVAILABLE,@Spark}
2025-04-30T13:39:34,490 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2dd6ac90{/json,null,AVAILABLE,@Spark}
2025-04-30T13:39:34,517 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2a47bcd5{/static,null,AVAILABLE,@Spark}
2025-04-30T13:39:34,522 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5b4d4288{/app/kill,null,AVAILABLE,@Spark}
2025-04-30T13:39:34,533 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5288b29d{/driver/kill,null,AVAILABLE,@Spark}
2025-04-30T13:39:34,552 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@67995065{/workers/kill,null,AVAILABLE,@Spark}
2025-04-30T13:39:34,580 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.ui.MasterWebUI [] - Bound MasterWebUI to 0.0.0.0, and started at http://663dbd2abcb8:8080
2025-04-30T13:39:34,582 [main] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkWorker' on port 37703.
2025-04-30T13:39:34,586 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Worker decommissioning not enabled.
2025-04-30T13:39:34,858 [main] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkWorker' on port 36861.
2025-04-30T13:39:34,865 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Worker decommissioning not enabled.
2025-04-30T13:39:34,940 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@511d0513{/metrics/master/json,null,AVAILABLE,@Spark}
2025-04-30T13:39:34,947 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4480211c{/metrics/applications/json,null,AVAILABLE,@Spark}
2025-04-30T13:39:34,984 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - I have been elected leader! New state: ALIVE
2025-04-30T13:39:35,046 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Starting Spark worker 172.18.0.3:37703 with 2 cores, 3.0 GiB RAM
2025-04-30T13:39:35,052 [main] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkWorker' on port 46325.
2025-04-30T13:39:35,062 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Running Spark version 3.5.5
2025-04-30T13:39:35,067 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Spark home: /opt/bitnami/spark
2025-04-30T13:39:35,067 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Worker decommissioning not enabled.
2025-04-30T13:39:35,126 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:39:35,128 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.worker.
2025-04-30T13:39:35,136 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:39:35,205 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @5936ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-30T13:39:35,265 [dispatcher-event-loop-1] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:8081 for WorkerUI
2025-04-30T13:39:35,289 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-30T13:39:35,290 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Starting Spark worker 172.18.0.4:36861 with 2 cores, 3.0 GiB RAM
2025-04-30T13:39:35,301 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Running Spark version 3.5.5
2025-04-30T13:39:35,307 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Spark home: /opt/bitnami/spark
2025-04-30T13:39:35,358 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:39:35,362 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.worker.
2025-04-30T13:39:35,363 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:39:35,365 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - Started @6098ms
2025-04-30T13:39:35,452 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @5719ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-30T13:39:35,467 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@2ba0731a{HTTP/1.1, (http/1.1)}{0.0.0.0:8081}
2025-04-30T13:39:35,473 [dispatcher-event-loop-1] INFO  org.apache.spark.util.Utils [] - Successfully started service 'WorkerUI' on port 8081.
2025-04-30T13:39:35,495 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Starting Spark worker 172.18.0.5:46325 with 2 cores, 3.0 GiB RAM
2025-04-30T13:39:35,518 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Running Spark version 3.5.5
2025-04-30T13:39:35,532 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Spark home: /opt/bitnami/spark
2025-04-30T13:39:35,550 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1ff28c25{/logPage,null,AVAILABLE,@Spark}
2025-04-30T13:39:35,557 [dispatcher-event-loop-1] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:8081 for WorkerUI
2025-04-30T13:39:35,581 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@364fd5f3{/logPage/json,null,AVAILABLE,@Spark}
2025-04-30T13:39:35,588 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4b43b187{/,null,AVAILABLE,@Spark}
2025-04-30T13:39:35,592 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@e30a721{/json,null,AVAILABLE,@Spark}
2025-04-30T13:39:35,602 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:39:35,605 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-30T13:39:35,609 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.worker.
2025-04-30T13:39:35,617 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@386833d{/static,null,AVAILABLE,@Spark}
2025-04-30T13:39:35,626 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:39:35,627 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@142e20df{/log,null,AVAILABLE,@Spark}
2025-04-30T13:39:35,635 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.ui.WorkerWebUI [] - Bound WorkerWebUI to 0.0.0.0, and started at http://d5e10145fa97:8081
2025-04-30T13:39:35,650 [worker-register-master-threadpool-0] INFO  org.apache.spark.deploy.worker.Worker [] - Connecting to master spark-master:7077...
2025-04-30T13:39:35,682 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - Started @5948ms
2025-04-30T13:39:35,740 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@22fe8396{/metrics/json,null,AVAILABLE,@Spark}
2025-04-30T13:39:35,758 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @6008ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-30T13:39:35,808 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@61fe7bc3{HTTP/1.1, (http/1.1)}{0.0.0.0:8081}
2025-04-30T13:39:35,810 [dispatcher-event-loop-1] INFO  org.apache.spark.util.Utils [] - Successfully started service 'WorkerUI' on port 8081.
2025-04-30T13:39:35,849 [dispatcher-event-loop-1] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:8081 for WorkerUI
2025-04-30T13:39:35,885 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 116 ms (0 ms spent in bootstraps)
2025-04-30T13:39:35,894 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@11053412{/logPage,null,AVAILABLE,@Spark}
2025-04-30T13:39:35,904 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3bdadb4b{/logPage/json,null,AVAILABLE,@Spark}
2025-04-30T13:39:35,918 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-30T13:39:35,926 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2d957fac{/,null,AVAILABLE,@Spark}
2025-04-30T13:39:35,937 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@9aa2004{/json,null,AVAILABLE,@Spark}
2025-04-30T13:39:35,988 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@8967c2c{/static,null,AVAILABLE,@Spark}
2025-04-30T13:39:35,994 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@67918a4b{/log,null,AVAILABLE,@Spark}
2025-04-30T13:39:36,010 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - Started @6261ms
2025-04-30T13:39:36,012 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.ui.WorkerWebUI [] - Bound WorkerWebUI to 0.0.0.0, and started at http://167310a79d22:8081
2025-04-30T13:39:36,018 [worker-register-master-threadpool-0] INFO  org.apache.spark.deploy.worker.Worker [] - Connecting to master spark-master:7077...
2025-04-30T13:39:36,086 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6ce009ff{/metrics/json,null,AVAILABLE,@Spark}
2025-04-30T13:39:36,126 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@65d81303{HTTP/1.1, (http/1.1)}{0.0.0.0:8081}
2025-04-30T13:39:36,127 [dispatcher-event-loop-1] INFO  org.apache.spark.util.Utils [] - Successfully started service 'WorkerUI' on port 8081.
2025-04-30T13:39:36,175 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 77 ms (0 ms spent in bootstraps)
2025-04-30T13:39:36,178 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@698fa38a{/logPage,null,AVAILABLE,@Spark}
2025-04-30T13:39:36,185 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@58f4edb2{/logPage/json,null,AVAILABLE,@Spark}
2025-04-30T13:39:36,189 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3b434d13{/,null,AVAILABLE,@Spark}
2025-04-30T13:39:36,192 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@749c353c{/json,null,AVAILABLE,@Spark}
2025-04-30T13:39:36,212 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3ccdb31e{/static,null,AVAILABLE,@Spark}
2025-04-30T13:39:36,220 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3dcc1894{/log,null,AVAILABLE,@Spark}
2025-04-30T13:39:36,226 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.ui.WorkerWebUI [] - Bound WorkerWebUI to 0.0.0.0, and started at http://a5c865c0d920:8081
2025-04-30T13:39:36,230 [worker-register-master-threadpool-0] INFO  org.apache.spark.deploy.worker.Worker [] - Connecting to master spark-master:7077...
2025-04-30T13:39:36,267 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.master.Master [] - Registering worker 172.18.0.3:37703 with 2 cores, 3.0 GiB RAM
2025-04-30T13:39:36,276 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@173a3242{/metrics/json,null,AVAILABLE,@Spark}
2025-04-30T13:39:36,316 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Successfully registered with master spark://663dbd2abcb8:7077
2025-04-30T13:39:36,357 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Registering worker 172.18.0.4:36861 with 2 cores, 3.0 GiB RAM
2025-04-30T13:39:36,376 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 85 ms (0 ms spent in bootstraps)
2025-04-30T13:39:36,382 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Successfully registered with master spark://663dbd2abcb8:7077
2025-04-30T13:39:36,484 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - Registering worker 172.18.0.5:46325 with 2 cores, 3.0 GiB RAM
2025-04-30T13:39:36,497 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.worker.Worker [] - Successfully registered with master spark://663dbd2abcb8:7077
2025-04-30T13:40:38,750 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-30T13:40:38,754 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T13:40:38,755 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-30T13:40:38,777 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:40:38,778 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-30T13:40:38,779 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:40:38,779 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: mod-2-pr-4-data-ingestion.py
2025-04-30T13:40:38,801 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-30T13:40:38,813 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-30T13:40:38,814 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-30T13:40:38,867 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-30T13:40:38,868 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-30T13:40:38,869 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:40:38,871 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:40:38,872 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-30T13:40:38,934 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:40:39,203 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 38729.
2025-04-30T13:40:39,232 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-30T13:40:39,265 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-30T13:40:39,283 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-30T13:40:39,284 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-30T13:40:39,291 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-30T13:40:39,311 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-2825709c-b1f8-4bc3-8eac-b904812de0ec
2025-04-30T13:40:39,328 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T13:40:39,346 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-30T13:40:39,386 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3290ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-30T13:40:39,471 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-30T13:40:39,480 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-30T13:40:39,495 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @3400ms
2025-04-30T13:40:39,538 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@1d6c4229{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T13:40:39,541 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-30T13:40:39,568 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4bf3b767{/,null,AVAILABLE,@Spark}
2025-04-30T13:40:39,814 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-30T13:40:39,952 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 79 ms (0 ms spent in bootstraps)
2025-04-30T13:40:40,125 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Registering app mod-2-pr-4-data-ingestion.py
2025-04-30T13:40:40,152 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Registered app mod-2-pr-4-data-ingestion.py with ID app-20250430134040-0000
2025-04-30T13:40:40,158 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430134040-0000 with rpId: 0
2025-04-30T13:40:40,164 [dispatcher-event-loop-10] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250430134040-0000
2025-04-30T13:40:40,176 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42961.
2025-04-30T13:40:40,177 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 663dbd2abcb8:42961
2025-04-30T13:40:40,180 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T13:40:40,189 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, 663dbd2abcb8, 42961, None)
2025-04-30T13:40:40,190 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430134040-0000/0 on worker worker-20250430133934-172.18.0.4-36861
2025-04-30T13:40:40,195 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 663dbd2abcb8:42961 with 434.4 MiB RAM, BlockManagerId(driver, 663dbd2abcb8, 42961, None)
2025-04-30T13:40:40,199 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, 663dbd2abcb8, 42961, None)
2025-04-30T13:40:40,202 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, 663dbd2abcb8, 42961, None)
2025-04-30T13:40:40,208 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430134040-0000/1 on worker worker-20250430133934-172.18.0.3-37703
2025-04-30T13:40:40,214 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430134040-0000/2 on worker worker-20250430133935-172.18.0.5-46325
2025-04-30T13:40:40,221 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430134040-0000/0 on worker-20250430133934-172.18.0.4-36861 (172.18.0.4:36861) with 2 core(s)
2025-04-30T13:40:40,227 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430134040-0000/0 on hostPort 172.18.0.4:36861 with 2 core(s), 1024.0 MiB RAM
2025-04-30T13:40:40,230 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430134040-0000/1 on worker-20250430133934-172.18.0.3-37703 (172.18.0.3:37703) with 2 core(s)
2025-04-30T13:40:40,231 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430134040-0000/1 on hostPort 172.18.0.3:37703 with 2 core(s), 1024.0 MiB RAM
2025-04-30T13:40:40,234 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430134040-0000/2 on worker-20250430133935-172.18.0.5-46325 (172.18.0.5:46325) with 2 core(s)
2025-04-30T13:40:40,237 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430134040-0000/2 on hostPort 172.18.0.5:46325 with 2 core(s), 1024.0 MiB RAM
2025-04-30T13:40:40,329 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430134040-0000/0 for mod-2-pr-4-data-ingestion.py
2025-04-30T13:40:40,353 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430134040-0000/2 for mod-2-pr-4-data-ingestion.py
2025-04-30T13:40:40,373 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430134040-0000/1 for mod-2-pr-4-data-ingestion.py
2025-04-30T13:40:40,428 [ExecutorRunner for app-20250430134040-0000/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:40:40,429 [ExecutorRunner for app-20250430134040-0000/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:40:40,429 [ExecutorRunner for app-20250430134040-0000/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:40:40,430 [ExecutorRunner for app-20250430134040-0000/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:40:40,430 [ExecutorRunner for app-20250430134040-0000/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:40:40,431 [ExecutorRunner for app-20250430134040-0000/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:40:40,432 [ExecutorRunner for app-20250430134040-0000/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:40:40,432 [ExecutorRunner for app-20250430134040-0000/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:40:40,433 [ExecutorRunner for app-20250430134040-0000/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:40:40,434 [ExecutorRunner for app-20250430134040-0000/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:40:40,445 [ExecutorRunner for app-20250430134040-0000/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:40:40,446 [ExecutorRunner for app-20250430134040-0000/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:40:40,447 [ExecutorRunner for app-20250430134040-0000/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:40:40,448 [ExecutorRunner for app-20250430134040-0000/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:40:40,449 [ExecutorRunner for app-20250430134040-0000/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:40:40,484 [ExecutorRunner for app-20250430134040-0000/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=38729" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@663dbd2abcb8:38729" "--executor-id" "0" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250430134040-0000" "--worker-url" "spark://Worker@172.18.0.4:36861" "--resourceProfileId" "0"
2025-04-30T13:40:40,489 [ExecutorRunner for app-20250430134040-0000/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=38729" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@663dbd2abcb8:38729" "--executor-id" "2" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250430134040-0000" "--worker-url" "spark://Worker@172.18.0.5:46325" "--resourceProfileId" "0"
2025-04-30T13:40:40,522 [ExecutorRunner for app-20250430134040-0000/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=38729" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@663dbd2abcb8:38729" "--executor-id" "1" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250430134040-0000" "--worker-url" "spark://Worker@172.18.0.3:37703" "--resourceProfileId" "0"
2025-04-30T13:40:40,576 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430134040-0000 with rpId: 0
2025-04-30T13:40:40,592 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430134040-0000 with rpId: 0
2025-04-30T13:40:40,598 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430134040-0000 with rpId: 0
2025-04-30T13:40:40,636 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430134040-0000/2 is now RUNNING
2025-04-30T13:40:40,641 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430134040-0000/0 is now RUNNING
2025-04-30T13:40:40,647 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430134040-0000/1 is now RUNNING
2025-04-30T13:40:40,906 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250430134040-0000.inprogress
2025-04-30T13:40:41,368 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@4bf3b767{/,null,STOPPED,@Spark}
2025-04-30T13:40:41,372 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3796c140{/jobs,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,382 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@491a2079{/jobs/json,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,388 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3d191138{/jobs/job,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,392 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@56ba0989{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,396 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@41855815{/stages,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,414 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@398c5c0a{/stages/json,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,428 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7bedbbb6{/stages/stage,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,431 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1b84225e{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,434 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7271393c{/stages/pool,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,457 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@12c539de{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,468 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@35fdb2e9{/storage,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,474 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6d0557d8{/storage/json,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,481 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@d673a98{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,489 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@302c93df{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,497 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7c073921{/environment,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,511 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5a48444b{/environment/json,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,517 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@54429999{/executors,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,521 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3605636e{/executors/json,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,536 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2cfd3f34{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,545 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@279878da{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,549 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@11721853{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,559 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@14252c3a{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,578 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5da404f8{/static,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,581 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@75e4e0dc{/,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,587 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@577cca47{/api,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,596 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@403a2c11{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,600 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@540dd0fd{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,616 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@32060621{/metrics/json,null,AVAILABLE,@Spark}
2025-04-30T13:40:41,619 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-30T13:40:42,231 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-04-30T13:40:42,245 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
2025-04-30T13:40:42,288 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@743d8150{/SQL,null,AVAILABLE,@Spark}
2025-04-30T13:40:42,298 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3d69679{/SQL/json,null,AVAILABLE,@Spark}
2025-04-30T13:40:42,326 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4b866c63{/SQL/execution,null,AVAILABLE,@Spark}
2025-04-30T13:40:42,341 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@79bc31af{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-04-30T13:40:42,360 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@58af3038{/static/sql,null,AVAILABLE,@Spark}
2025-04-30T13:40:43,450 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 147@a5c865c0d920
2025-04-30T13:40:43,458 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 149@167310a79d22
2025-04-30T13:40:43,470 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T13:40:43,474 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T13:40:43,475 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T13:40:43,477 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T13:40:43,479 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T13:40:43,480 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T13:40:43,510 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 150@d5e10145fa97
2025-04-30T13:40:43,522 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T13:40:43,524 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T13:40:43,524 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T13:40:44,075 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:40:44,105 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:40:44,188 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:40:44,196 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 109 ms to list leaf files for 2 paths.
2025-04-30T13:40:44,322 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:40:44,323 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:40:44,325 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:40:44,327 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:40:44,328 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:40:44,335 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:40:44,341 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:40:44,347 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:40:44,350 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:40:44,356 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:40:44,383 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:40:44,384 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:40:44,386 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:40:44,388 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:40:44,404 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:40:44,860 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 663dbd2abcb8/172.18.0.2:38729 after 95 ms (0 ms spent in bootstraps)
2025-04-30T13:40:44,941 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 663dbd2abcb8/172.18.0.2:38729 after 133 ms (0 ms spent in bootstraps)
2025-04-30T13:40:44,984 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 663dbd2abcb8/172.18.0.2:38729 after 135 ms (0 ms spent in bootstraps)
2025-04-30T13:40:45,145 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:40:45,147 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:40:45,150 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:40:45,152 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:40:45,153 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:40:45,184 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:40:45,185 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:40:45,186 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:40:45,187 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:40:45,188 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:40:45,227 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:40:45,228 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:40:45,229 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:40:45,230 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:40:45,231 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:40:45,266 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 663dbd2abcb8/172.18.0.2:38729 after 10 ms (0 ms spent in bootstraps)
2025-04-30T13:40:45,307 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 663dbd2abcb8/172.18.0.2:38729 after 4 ms (0 ms spent in bootstraps)
2025-04-30T13:40:45,326 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 663dbd2abcb8/172.18.0.2:38729 after 4 ms (0 ms spent in bootstraps)
2025-04-30T13:40:45,431 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-e9a5b7fd-89f0-4de8-b165-b7764e6d64d6/executor-d509401c-b8ba-4a23-9ee5-5070dd608f66/blockmgr-3a3403a1-b8a4-462c-bab0-82d9b4f0fdee
2025-04-30T13:40:45,452 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-6942eef4-1d13-4d8d-9005-12285cba0512/executor-3b0c0ace-5092-4a1a-b1d4-7094634b2033/blockmgr-ce72dd59-c381-4b50-bd81-1a0e3f0ec5bf
2025-04-30T13:40:45,476 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-0615d478-5c1d-46f8-8bd1-0f009ad1fd80/executor-a51c88a7-a21d-4638-a948-5ea9c4aab219/blockmgr-96169d28-4010-4d63-8fc2-018d13f55e75
2025-04-30T13:40:45,489 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T13:40:45,515 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T13:40:45,526 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T13:40:45,602 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 17 ms to list leaf files for 1 paths.
2025-04-30T13:40:45,666 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 17 ms to list leaf files for 1 paths.
2025-04-30T13:40:45,812 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@663dbd2abcb8:38729
2025-04-30T13:40:45,814 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.5:46325
2025-04-30T13:40:45,831 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:40:45,832 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.5:46325 after 14 ms (0 ms spent in bootstraps)
2025-04-30T13:40:45,833 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.5:46325
2025-04-30T13:40:45,833 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T13:40:45,835 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:40:45,841 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@663dbd2abcb8:38729
2025-04-30T13:40:45,847 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.3:37703
2025-04-30T13:40:45,859 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:40:45,860 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.3:37703 after 9 ms (0 ms spent in bootstraps)
2025-04-30T13:40:45,869 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@663dbd2abcb8:38729
2025-04-30T13:40:45,864 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.3:37703
2025-04-30T13:40:45,871 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.4:36861
2025-04-30T13:40:45,861 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T13:40:45,873 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:40:45,888 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.4:36861
2025-04-30T13:40:45,889 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.4:36861 after 7 ms (0 ms spent in bootstraps)
2025-04-30T13:40:45,890 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:40:45,892 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T13:40:45,893 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:40:45,913 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:40354) with ID 2,  ResourceProfileId 0
2025-04-30T13:40:45,924 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:58850) with ID 1,  ResourceProfileId 0
2025-04-30T13:40:45,931 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T13:40:45,939 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 2 on host 172.18.0.5
2025-04-30T13:40:45,941 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:60066) with ID 0,  ResourceProfileId 0
2025-04-30T13:40:45,937 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T13:40:45,943 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T13:40:45,945 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T13:40:45,949 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 1 on host 172.18.0.3
2025-04-30T13:40:45,951 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T13:40:45,952 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T13:40:45,958 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T13:40:45,965 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 0 on host 172.18.0.4
2025-04-30T13:40:45,968 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T13:40:45,970 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T13:40:46,015 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45935.
2025-04-30T13:40:46,016 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.5:45935
2025-04-30T13:40:46,019 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T13:40:46,026 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33733.
2025-04-30T13:40:46,027 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.3:33733
2025-04-30T13:40:46,029 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(2, 172.18.0.5, 45935, None)
2025-04-30T13:40:46,031 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T13:40:46,034 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37741.
2025-04-30T13:40:46,035 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.4:37741
2025-04-30T13:40:46,042 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(1, 172.18.0.3, 33733, None)
2025-04-30T13:40:46,042 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T13:40:46,051 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.5:45935 with 434.4 MiB RAM, BlockManagerId(2, 172.18.0.5, 45935, None)
2025-04-30T13:40:46,057 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.3:33733 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.3, 33733, None)
2025-04-30T13:40:46,060 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(2, 172.18.0.5, 45935, None)
2025-04-30T13:40:46,062 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(0, 172.18.0.4, 37741, None)
2025-04-30T13:40:46,064 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(1, 172.18.0.3, 33733, None)
2025-04-30T13:40:46,065 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(2, 172.18.0.5, 45935, None)
2025-04-30T13:40:46,067 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(1, 172.18.0.3, 33733, None)
2025-04-30T13:40:46,079 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T13:40:46,079 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T13:40:46,083 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2f9a6030 for default.
2025-04-30T13:40:46,083 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@38b07f3b for default.
2025-04-30T13:40:46,085 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.4:37741 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.4, 37741, None)
2025-04-30T13:40:46,091 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(0, 172.18.0.4, 37741, None)
2025-04-30T13:40:46,095 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(0, 172.18.0.4, 37741, None)
2025-04-30T13:40:46,115 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T13:40:46,117 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@33cbe95 for default.
2025-04-30T13:40:47,285 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T13:40:47,286 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T13:40:47,420 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 200.3 KiB, free 434.2 MiB)
2025-04-30T13:40:47,462 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
2025-04-30T13:40:47,465 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on 663dbd2abcb8:42961 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T13:40:47,470 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 0 from json at <unknown>:0
2025-04-30T13:40:47,482 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T13:40:47,674 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: json at <unknown>:0
2025-04-30T13:40:47,697 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 0 (json at <unknown>:0) with 1 output partitions
2025-04-30T13:40:47,699 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 0 (json at <unknown>:0)
2025-04-30T13:40:47,700 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T13:40:47,703 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T13:40:47,708 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0), which has no missing parents
2025-04-30T13:40:47,788 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.5 KiB, free 434.2 MiB)
2025-04-30T13:40:47,791 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
2025-04-30T13:40:47,792 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on 663dbd2abcb8:42961 (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T13:40:47,794 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 1 from broadcast at DAGScheduler.scala:1585
2025-04-30T13:40:47,812 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T13:40:47,813 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 0.0 with 1 tasks resource profile 0
2025-04-30T13:40:47,861 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.5, executor 2, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T13:40:47,883 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 0
2025-04-30T13:40:47,892 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 0.0 (TID 0)
2025-04-30T13:40:47,989 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:40:48,035 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 663dbd2abcb8/172.18.0.2:42961 after 2 ms (0 ms spent in bootstraps)
2025-04-30T13:40:48,073 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.4 MiB)
2025-04-30T13:40:48,080 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on 172.18.0.5:45935 (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T13:40:48,094 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 1 took 104 ms
2025-04-30T13:40:48,149 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.5 KiB, free 434.4 MiB)
2025-04-30T13:40:48,436 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl, range: 0-32688, partition values: [empty row]
2025-04-30T13:40:48,760 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 173.546321 ms
2025-04-30T13:40:48,762 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:40:48,772 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.3 MiB)
2025-04-30T13:40:48,774 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on 172.18.0.5:45935 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T13:40:48,776 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 0 took 13 ms
2025-04-30T13:40:48,847 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T13:40:49,002 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 0.0 (TID 0). 2450 bytes result sent to driver
2025-04-30T13:40:49,015 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 0.0 (TID 0) in 1172 ms on 172.18.0.5 (executor 2) (1/1)
2025-04-30T13:40:49,017 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-04-30T13:40:49,021 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 0 (json at <unknown>:0) finished in 1.296 s
2025-04-30T13:40:49,024 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T13:40:49,025 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 0: Stage finished
2025-04-30T13:40:49,029 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 finished: json at <unknown>:0, took 1.353690 s
2025-04-30T13:40:49,157 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T13:40:49,158 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T13:40:49,523 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 232.501565 ms
2025-04-30T13:40:49,529 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 200.2 KiB, free 434.0 MiB)
2025-04-30T13:40:49,543 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
2025-04-30T13:40:49,544 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on 663dbd2abcb8:42961 (size: 34.5 KiB, free: 434.3 MiB)
2025-04-30T13:40:49,546 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 2 from count at <unknown>:0
2025-04-30T13:40:49,551 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T13:40:49,593 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Registering RDD 7 (count at <unknown>:0) as input to shuffle 0
2025-04-30T13:40:49,603 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got map stage job 1 (count at <unknown>:0) with 1 output partitions
2025-04-30T13:40:49,606 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ShuffleMapStage 1 (count at <unknown>:0)
2025-04-30T13:40:49,608 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T13:40:49,610 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T13:40:49,613 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0), which has no missing parents
2025-04-30T13:40:49,651 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 433.9 MiB)
2025-04-30T13:40:49,668 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 433.9 MiB)
2025-04-30T13:40:49,670 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on 663dbd2abcb8:42961 (size: 8.3 KiB, free: 434.3 MiB)
2025-04-30T13:40:49,672 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 3 from broadcast at DAGScheduler.scala:1585
2025-04-30T13:40:49,678 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T13:40:49,678 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 1.0 with 1 tasks resource profile 0
2025-04-30T13:40:49,683 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.4, executor 0, partition 0, PROCESS_LOCAL, 9638 bytes) 
2025-04-30T13:40:49,702 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 1
2025-04-30T13:40:49,726 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on 663dbd2abcb8:42961 in memory (size: 7.7 KiB, free: 434.3 MiB)
2025-04-30T13:40:49,753 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on 172.18.0.5:45935 in memory (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T13:40:49,764 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 1.0 (TID 1)
2025-04-30T13:40:50,009 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:40:50,112 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 663dbd2abcb8/172.18.0.2:42961 after 6 ms (0 ms spent in bootstraps)
2025-04-30T13:40:50,212 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 434.4 MiB)
2025-04-30T13:40:50,218 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on 172.18.0.4:37741 (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T13:40:50,226 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 3 took 216 ms
2025-04-30T13:40:50,325 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 434.4 MiB)
2025-04-30T13:40:50,923 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 179.001766 ms
2025-04-30T13:40:50,934 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl, range: 0-32688, partition values: [empty row]
2025-04-30T13:40:50,969 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 11.126582 ms
2025-04-30T13:40:50,987 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:40:50,998 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.3 MiB)
2025-04-30T13:40:51,002 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on 172.18.0.4:37741 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T13:40:51,005 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 2 took 17 ms
2025-04-30T13:40:51,085 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T13:40:51,266 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 1.0 (TID 1). 2007 bytes result sent to driver
2025-04-30T13:40:51,275 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 1.0 (TID 1) in 1594 ms on 172.18.0.4 (executor 0) (1/1)
2025-04-30T13:40:51,276 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-04-30T13:40:51,279 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ShuffleMapStage 1 (count at <unknown>:0) finished in 1.660 s
2025-04-30T13:40:51,280 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - looking for newly runnable stages
2025-04-30T13:40:51,281 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - running: Set()
2025-04-30T13:40:51,282 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - waiting: Set()
2025-04-30T13:40:51,282 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - failed: Set()
2025-04-30T13:40:51,333 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 16.686573 ms
2025-04-30T13:40:51,364 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: count at <unknown>:0
2025-04-30T13:40:51,367 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 2 (count at <unknown>:0) with 1 output partitions
2025-04-30T13:40:51,367 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 3 (count at <unknown>:0)
2025-04-30T13:40:51,368 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List(ShuffleMapStage 2)
2025-04-30T13:40:51,369 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T13:40:51,370 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0), which has no missing parents
2025-04-30T13:40:51,382 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 433.9 MiB)
2025-04-30T13:40:51,393 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 433.9 MiB)
2025-04-30T13:40:51,397 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_4_piece0 in memory on 663dbd2abcb8:42961 (size: 6.0 KiB, free: 434.3 MiB)
2025-04-30T13:40:51,399 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_3_piece0 on 663dbd2abcb8:42961 in memory (size: 8.3 KiB, free: 434.3 MiB)
2025-04-30T13:40:51,401 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 4 from broadcast at DAGScheduler.scala:1585
2025-04-30T13:40:51,403 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T13:40:51,404 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 3.0 with 1 tasks resource profile 0
2025-04-30T13:40:51,410 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_3_piece0 on 172.18.0.4:37741 in memory (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T13:40:51,414 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 3.0 (TID 2) (172.18.0.4, executor 0, partition 0, NODE_LOCAL, 9003 bytes) 
2025-04-30T13:40:51,419 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 2
2025-04-30T13:40:51,421 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 3.0 (TID 2)
2025-04-30T13:40:51,432 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2025-04-30T13:40:51,452 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:40:51,465 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.0 MiB)
2025-04-30T13:40:51,468 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_4_piece0 in memory on 172.18.0.4:37741 (size: 6.0 KiB, free: 434.4 MiB)
2025-04-30T13:40:51,470 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 4 took 17 ms
2025-04-30T13:40:51,472 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 434.0 MiB)
2025-04-30T13:40:51,541 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Don't have map outputs for shuffle 0, fetching them
2025-04-30T13:40:51,544 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@663dbd2abcb8:38729)
2025-04-30T13:40:51,549 [dispatcher-event-loop-7] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - Asked to send map output locations for shuffle 0 to 172.18.0.4:60066
2025-04-30T13:40:51,592 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Got the map output locations
2025-04-30T13:40:51,623 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-04-30T13:40:51,627 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Started 0 remote fetches in 15 ms
2025-04-30T13:40:51,656 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 15.171894 ms
2025-04-30T13:40:51,672 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 3.0 (TID 2). 4081 bytes result sent to driver
2025-04-30T13:40:51,679 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 3.0 (TID 2) in 267 ms on 172.18.0.4 (executor 0) (1/1)
2025-04-30T13:40:51,680 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2025-04-30T13:40:51,682 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 3 (count at <unknown>:0) finished in 0.302 s
2025-04-30T13:40:51,683 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T13:40:51,683 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 3: Stage finished
2025-04-30T13:40:51,684 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 2 finished: count at <unknown>:0, took 0.319854 s
2025-04-30T13:40:51,781 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T13:40:51,781 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T13:40:51,853 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils [] - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:40:51,871 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:40:51,872 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:40:51,875 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:40:51,876 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:40:51,876 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:40:51,877 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:40:51,947 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5 stored as values in memory (estimated size 200.2 KiB, free 433.7 MiB)
2025-04-30T13:40:51,961 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_4_piece0 on 663dbd2abcb8:42961 in memory (size: 6.0 KiB, free: 434.3 MiB)
2025-04-30T13:40:51,966 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_4_piece0 on 172.18.0.4:37741 in memory (size: 6.0 KiB, free: 434.4 MiB)
2025-04-30T13:40:51,967 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
2025-04-30T13:40:51,970 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_5_piece0 in memory on 663dbd2abcb8:42961 (size: 34.5 KiB, free: 434.3 MiB)
2025-04-30T13:40:51,972 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 5 from parquet at <unknown>:0
2025-04-30T13:40:51,975 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T13:40:51,978 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_2_piece0 on 663dbd2abcb8:42961 in memory (size: 34.5 KiB, free: 434.3 MiB)
2025-04-30T13:40:51,981 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_2_piece0 on 172.18.0.4:37741 in memory (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T13:40:51,993 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: parquet at <unknown>:0
2025-04-30T13:40:51,995 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 3 (parquet at <unknown>:0) with 1 output partitions
2025-04-30T13:40:51,996 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 4 (parquet at <unknown>:0)
2025-04-30T13:40:51,997 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T13:40:52,001 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T13:40:52,007 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 4 (MapPartitionsRDD[13] at parquet at <unknown>:0), which has no missing parents
2025-04-30T13:40:52,049 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6 stored as values in memory (estimated size 215.4 KiB, free 433.7 MiB)
2025-04-30T13:40:52,058 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 77.5 KiB, free 433.7 MiB)
2025-04-30T13:40:52,063 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_6_piece0 in memory on 663dbd2abcb8:42961 (size: 77.5 KiB, free: 434.3 MiB)
2025-04-30T13:40:52,065 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 6 from broadcast at DAGScheduler.scala:1585
2025-04-30T13:40:52,066 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at parquet at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T13:40:52,067 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 4.0 with 1 tasks resource profile 0
2025-04-30T13:40:52,069 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 4.0 (TID 3) (172.18.0.4, executor 0, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T13:40:52,074 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 3
2025-04-30T13:40:52,076 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 4.0 (TID 3)
2025-04-30T13:40:52,082 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:40:52,117 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 77.5 KiB, free 434.3 MiB)
2025-04-30T13:40:52,119 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_0_piece0 on 663dbd2abcb8:42961 in memory (size: 34.5 KiB, free: 434.3 MiB)
2025-04-30T13:40:52,125 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_6_piece0 in memory on 172.18.0.4:37741 (size: 77.5 KiB, free: 434.3 MiB)
2025-04-30T13:40:52,131 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 6 took 47 ms
2025-04-30T13:40:52,132 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_0_piece0 on 172.18.0.5:45935 in memory (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T13:40:52,134 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6 stored as values in memory (estimated size 215.4 KiB, free 434.1 MiB)
2025-04-30T13:40:52,195 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:40:52,195 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:40:52,197 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:40:52,198 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:40:52,198 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:40:52,200 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:40:52,208 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:40:52,211 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:40:52,238 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat [] - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-04-30T13:40:52,299 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport [] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "date_birth",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "driver_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt_current_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "first_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "last_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "license_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "phone_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_make",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_model",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_year",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary city (STRING);
  optional binary country (STRING);
  optional binary date_birth (STRING);
  optional int64 driver_id;
  optional binary dt_current_timestamp (STRING);
  optional binary first_name (STRING);
  optional binary last_name (STRING);
  optional binary license_number (STRING);
  optional binary phone_number (STRING);
  optional binary uuid (STRING);
  optional binary vehicle_make (STRING);
  optional binary vehicle_model (STRING);
  optional binary vehicle_type (STRING);
  optional int64 vehicle_year;
}

       
2025-04-30T13:40:52,332 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.0 in stage 4.0 (TID 3)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301340512354999612800776914_0004_m_000000_3 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430134040-0000/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
2025-04-30T13:40:52,378 [task-result-getter-3] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.0 in stage 4.0 (TID 3) (172.18.0.4 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301340512354999612800776914_0004_m_000000_3 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430134040-0000/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

2025-04-30T13:40:52,383 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.1 in stage 4.0 (TID 4) (172.18.0.4, executor 0, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T13:40:52,392 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 4
2025-04-30T13:40:52,393 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.executor.Executor [] - Running task 0.1 in stage 4.0 (TID 4)
2025-04-30T13:40:52,425 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:40:52,426 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:40:52,428 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:40:52,428 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:40:52,429 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:40:52,430 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:40:52,431 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:40:52,433 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:40:52,437 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat [] - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-04-30T13:40:52,445 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport [] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "date_birth",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "driver_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt_current_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "first_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "last_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "license_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "phone_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_make",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_model",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_year",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary city (STRING);
  optional binary country (STRING);
  optional binary date_birth (STRING);
  optional int64 driver_id;
  optional binary dt_current_timestamp (STRING);
  optional binary first_name (STRING);
  optional binary last_name (STRING);
  optional binary license_number (STRING);
  optional binary phone_number (STRING);
  optional binary uuid (STRING);
  optional binary vehicle_make (STRING);
  optional binary vehicle_model (STRING);
  optional binary vehicle_type (STRING);
  optional int64 vehicle_year;
}

       
2025-04-30T13:40:52,473 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.1 in stage 4.0 (TID 4)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301340512354999612800776914_0004_m_000000_4 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430134040-0000/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
2025-04-30T13:40:52,481 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.1 in stage 4.0 (TID 4) (172.18.0.4 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301340512354999612800776914_0004_m_000000_4 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430134040-0000/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

2025-04-30T13:40:52,483 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.2 in stage 4.0 (TID 5) (172.18.0.4, executor 0, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T13:40:52,487 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 5
2025-04-30T13:40:52,488 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.executor.Executor [] - Running task 0.2 in stage 4.0 (TID 5)
2025-04-30T13:40:52,508 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:40:52,509 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:40:52,510 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:40:52,511 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:40:52,511 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:40:52,512 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:40:52,513 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:40:52,514 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:40:52,516 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat [] - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-04-30T13:40:52,522 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport [] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "date_birth",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "driver_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt_current_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "first_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "last_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "license_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "phone_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_make",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_model",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_year",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary city (STRING);
  optional binary country (STRING);
  optional binary date_birth (STRING);
  optional int64 driver_id;
  optional binary dt_current_timestamp (STRING);
  optional binary first_name (STRING);
  optional binary last_name (STRING);
  optional binary license_number (STRING);
  optional binary phone_number (STRING);
  optional binary uuid (STRING);
  optional binary vehicle_make (STRING);
  optional binary vehicle_model (STRING);
  optional binary vehicle_type (STRING);
  optional int64 vehicle_year;
}

       
2025-04-30T13:40:52,548 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.2 in stage 4.0 (TID 5)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301340512354999612800776914_0004_m_000000_5 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430134040-0000/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
2025-04-30T13:40:52,554 [task-result-getter-1] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.2 in stage 4.0 (TID 5) (172.18.0.4 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301340512354999612800776914_0004_m_000000_5 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430134040-0000/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

2025-04-30T13:40:52,556 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.3 in stage 4.0 (TID 6) (172.18.0.4, executor 0, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T13:40:52,561 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 6
2025-04-30T13:40:52,562 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.executor.Executor [] - Running task 0.3 in stage 4.0 (TID 6)
2025-04-30T13:40:52,577 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:40:52,577 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:40:52,579 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:40:52,579 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:40:52,580 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:40:52,581 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:40:52,581 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:40:52,582 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:40:52,585 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat [] - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-04-30T13:40:52,589 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport [] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "date_birth",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "driver_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt_current_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "first_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "last_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "license_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "phone_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_make",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_model",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_year",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary city (STRING);
  optional binary country (STRING);
  optional binary date_birth (STRING);
  optional int64 driver_id;
  optional binary dt_current_timestamp (STRING);
  optional binary first_name (STRING);
  optional binary last_name (STRING);
  optional binary license_number (STRING);
  optional binary phone_number (STRING);
  optional binary uuid (STRING);
  optional binary vehicle_make (STRING);
  optional binary vehicle_model (STRING);
  optional binary vehicle_type (STRING);
  optional int64 vehicle_year;
}

       
2025-04-30T13:40:52,619 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.3 in stage 4.0 (TID 6)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301340512354999612800776914_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430134040-0000/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
2025-04-30T13:40:52,628 [task-result-getter-2] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.3 in stage 4.0 (TID 6) (172.18.0.4 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301340512354999612800776914_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430134040-0000/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

2025-04-30T13:40:52,630 [task-result-getter-2] ERROR org.apache.spark.scheduler.TaskSetManager [] - Task 0 in stage 4.0 failed 4 times; aborting job
2025-04-30T13:40:52,632 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2025-04-30T13:40:52,635 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Cancelling stage 4
2025-04-30T13:40:52,636 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 4: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 6) (172.18.0.4 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301340512354999612800776914_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430134040-0000/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

Driver stacktrace:
2025-04-30T13:40:52,641 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 4 (parquet at <unknown>:0) failed in 0.630 s due to Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 6) (172.18.0.4 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301340512354999612800776914_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430134040-0000/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

Driver stacktrace:
2025-04-30T13:40:52,652 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 3 failed: parquet at <unknown>:0, took 0.657913 s
2025-04-30T13:40:52,653 [Thread-4] ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter [] - Aborting job c594a32e-1634-4fbb-8889-31000243e6ea.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 6) (172.18.0.4 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301340512354999612800776914_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430134040-0000/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[scala-library-2.12.18.jar:?]
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[scala-library-2.12.18.jar:?]
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at scala.Option.foreach(Option.scala:407) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461) ~[spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76) [spark-sql-api_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
	at java.lang.reflect.Method.invoke(Unknown Source) ~[?:?]
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) [py4j-0.10.9.7.jar:?]
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374) [py4j-0.10.9.7.jar:?]
	at py4j.Gateway.invoke(Gateway.java:282) [py4j-0.10.9.7.jar:?]
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) [py4j-0.10.9.7.jar:?]
	at py4j.commands.CallCommand.execute(CallCommand.java:79) [py4j-0.10.9.7.jar:?]
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182) [py4j-0.10.9.7.jar:?]
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106) [py4j-0.10.9.7.jar:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
Caused by: java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301340512354999612800776914_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430134040-0000/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]
	... 1 more
2025-04-30T13:40:53,103 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Invoking stop() from shutdown hook
2025-04-30T13:40:53,104 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-30T13:40:53,112 [shutdown-hook-0] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@1d6c4229{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T13:40:53,116 [shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://663dbd2abcb8:4040
2025-04-30T13:40:53,120 [shutdown-hook-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-30T13:40:53,121 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-30T13:40:53,127 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T13:40:53,127 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T13:40:53,127 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T13:40:53,134 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250430134040-0000
2025-04-30T13:40:53,136 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250430134040-0000
2025-04-30T13:40:53,155 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T13:40:53,154 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T13:40:53,156 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T13:40:53,158 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T13:40:53,164 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430134040-0000/2
2025-04-30T13:40:53,164 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T13:40:53,165 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T13:40:53,166 [ExecutorRunner for app-20250430134040-0000/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430134040-0000/2 interrupted
2025-04-30T13:40:53,169 [ExecutorRunner for app-20250430134040-0000/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T13:40:53,172 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430134040-0000/1
2025-04-30T13:40:53,172 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430134040-0000/0
2025-04-30T13:40:53,174 [ExecutorRunner for app-20250430134040-0000/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430134040-0000/1 interrupted
2025-04-30T13:40:53,175 [ExecutorRunner for app-20250430134040-0000/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430134040-0000/0 interrupted
2025-04-30T13:40:53,176 [ExecutorRunner for app-20250430134040-0000/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T13:40:53,178 [ExecutorRunner for app-20250430134040-0000/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T13:40:53,180 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T13:40:53,181 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T13:40:53,181 [dispatcher-event-loop-6] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-30T13:40:53,186 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T13:40:53,191 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T13:40:53,194 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T13:40:53,195 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T13:40:53,216 [shutdown-hook-0] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T13:40:53,217 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T13:40:53,222 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-30T13:40:53,228 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-30T13:40:53,246 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430134040-0000/1 finished with state KILLED exitStatus 143
2025-04-30T13:40:53,247 [dispatcher-event-loop-5] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430134040-0000/1
2025-04-30T13:40:53,260 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:40054 got disassociated, removing it.
2025-04-30T13:40:53,261 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-30T13:40:53,263 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - 663dbd2abcb8:38729 got disassociated, removing it.
2025-04-30T13:40:53,270 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430134040-0000, execId=1)
2025-04-30T13:40:53,272 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-30T13:40:53,273 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430134040-0000/0 finished with state KILLED exitStatus 143
2025-04-30T13:40:53,278 [dispatcher-event-loop-5] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430134040-0000/0
2025-04-30T13:40:53,279 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-30T13:40:53,285 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T13:40:53,288 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430134040-0000 removed, cleanupLocalDirs = true
2025-04-30T13:40:53,288 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430134040-0000
2025-04-30T13:40:53,297 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430134040-0000, execId=0)
2025-04-30T13:40:53,297 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-83cf65e8-c953-4862-a3a5-482c93443888/pyspark-3fac8bb5-d6e4-4d7d-8928-6a9ccc5d4047
2025-04-30T13:40:53,302 [dispatcher-event-loop-0] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430134040-0000/2
2025-04-30T13:40:53,307 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-83cf65e8-c953-4862-a3a5-482c93443888
2025-04-30T13:40:53,303 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430134040-0000/2 finished with state KILLED exitStatus 0
2025-04-30T13:40:53,311 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430134040-0000
2025-04-30T13:40:53,313 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-30T13:40:53,315 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430134040-0000, execId=2)
2025-04-30T13:40:53,315 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430134040-0000 removed, cleanupLocalDirs = true
2025-04-30T13:40:53,316 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-b10d79a9-9b6c-43d8-a29f-792fabf1b055
2025-04-30T13:40:53,321 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430134040-0000 removed, cleanupLocalDirs = true
2025-04-30T13:40:53,321 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430134040-0000
2025-04-30T13:41:36,145 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.4:42570 got disassociated, removing it.
2025-04-30T13:41:36,147 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.4:36861 got disassociated, removing it.
2025-04-30T13:41:36,150 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - Removing worker worker-20250430133934-172.18.0.4-36861 on 172.18.0.4:36861
2025-04-30T13:41:36,155 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - Telling app of lost worker: worker-20250430133934-172.18.0.4-36861
2025-04-30T13:41:36,164 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.3:44838 got disassociated, removing it.
2025-04-30T13:41:36,165 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.3:37703 got disassociated, removing it.
2025-04-30T13:41:36,166 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - Removing worker worker-20250430133934-172.18.0.3-37703 on 172.18.0.3:37703
2025-04-30T13:41:36,166 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - Telling app of lost worker: worker-20250430133934-172.18.0.3-37703
2025-04-30T13:41:36,170 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.5:47096 got disassociated, removing it.
2025-04-30T13:41:36,171 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.5:46325 got disassociated, removing it.
2025-04-30T13:41:36,173 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Removing worker worker-20250430133935-172.18.0.5-46325 on 172.18.0.5:46325
2025-04-30T13:41:36,174 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Telling app of lost worker: worker-20250430133935-172.18.0.5-46325
2025-04-30T13:42:05,101 [main] INFO  org.apache.spark.deploy.master.Master [] - Started daemon with process name: 62@22a90d2ca54f
2025-04-30T13:42:05,120 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T13:42:05,122 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T13:42:05,123 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T13:42:05,616 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:42:05,618 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:42:05,620 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:42:05,622 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:42:05,625 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:42:05,697 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Started daemon with process name: 60@30cc727c40d4
2025-04-30T13:42:05,714 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T13:42:05,720 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T13:42:05,721 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T13:42:05,859 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Started daemon with process name: 60@e89d73bffc2c
2025-04-30T13:42:05,877 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T13:42:05,883 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T13:42:05,884 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T13:42:05,901 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Started daemon with process name: 60@4cfd2bfc0474
2025-04-30T13:42:05,929 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T13:42:05,933 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T13:42:05,937 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T13:42:06,009 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:42:06,350 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:42:06,353 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:42:06,360 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:42:06,365 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:42:06,369 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:42:06,451 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:42:06,455 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:42:06,458 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:42:06,462 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:42:06,467 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:42:06,604 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:42:06,615 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:42:06,617 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:42:06,619 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:42:06,621 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:42:06,839 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:42:07,018 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:42:07,264 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:42:07,295 [main] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkMaster' on port 7077.
2025-04-30T13:42:07,337 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Starting Spark master at spark://22a90d2ca54f:7077
2025-04-30T13:42:07,348 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Running Spark version 3.5.5
2025-04-30T13:42:07,464 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @5450ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-30T13:42:07,597 [dispatcher-event-loop-1] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:8080 for MasterUI
2025-04-30T13:42:07,632 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-30T13:42:07,697 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - Started @5687ms
2025-04-30T13:42:07,799 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@d165a45{HTTP/1.1, (http/1.1)}{0.0.0.0:8080}
2025-04-30T13:42:07,800 [dispatcher-event-loop-1] INFO  org.apache.spark.util.Utils [] - Successfully started service 'MasterUI' on port 8080.
2025-04-30T13:42:07,872 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1b0dbe49{/app,null,AVAILABLE,@Spark}
2025-04-30T13:42:07,879 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7f7316ab{/app/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:07,884 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3813357a{/,null,AVAILABLE,@Spark}
2025-04-30T13:42:07,889 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40c77999{/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:07,909 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2b6c7a70{/static,null,AVAILABLE,@Spark}
2025-04-30T13:42:07,915 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@500f1987{/app/kill,null,AVAILABLE,@Spark}
2025-04-30T13:42:07,922 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5ce323b9{/driver/kill,null,AVAILABLE,@Spark}
2025-04-30T13:42:07,929 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4c4024ff{/workers/kill,null,AVAILABLE,@Spark}
2025-04-30T13:42:07,943 [main] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkWorker' on port 44603.
2025-04-30T13:42:07,944 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.ui.MasterWebUI [] - Bound MasterWebUI to 0.0.0.0, and started at http://22a90d2ca54f:8080
2025-04-30T13:42:07,953 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Worker decommissioning not enabled.
2025-04-30T13:42:08,051 [main] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkWorker' on port 42969.
2025-04-30T13:42:08,065 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Worker decommissioning not enabled.
2025-04-30T13:42:08,192 [main] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkWorker' on port 44831.
2025-04-30T13:42:08,200 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Worker decommissioning not enabled.
2025-04-30T13:42:08,309 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@49899694{/metrics/master/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:08,323 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7628d1d1{/metrics/applications/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:08,373 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - I have been elected leader! New state: ALIVE
2025-04-30T13:42:08,425 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Starting Spark worker 172.18.0.3:44603 with 2 cores, 3.0 GiB RAM
2025-04-30T13:42:08,443 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Running Spark version 3.5.5
2025-04-30T13:42:08,445 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Spark home: /opt/bitnami/spark
2025-04-30T13:42:08,494 [dispatcher-event-loop-0] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:42:08,497 [dispatcher-event-loop-0] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.worker.
2025-04-30T13:42:08,501 [dispatcher-event-loop-0] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:42:08,526 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Starting Spark worker 172.18.0.4:42969 with 2 cores, 3.0 GiB RAM
2025-04-30T13:42:08,542 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Running Spark version 3.5.5
2025-04-30T13:42:08,544 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Spark home: /opt/bitnami/spark
2025-04-30T13:42:08,584 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @5747ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-30T13:42:08,589 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:42:08,591 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.worker.
2025-04-30T13:42:08,594 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:42:08,643 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Starting Spark worker 172.18.0.5:44831 with 2 cores, 3.0 GiB RAM
2025-04-30T13:42:08,655 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Running Spark version 3.5.5
2025-04-30T13:42:08,659 [dispatcher-event-loop-0] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:8081 for WorkerUI
2025-04-30T13:42:08,661 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Spark home: /opt/bitnami/spark
2025-04-30T13:42:08,667 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @5841ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-30T13:42:08,685 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-30T13:42:08,693 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:42:08,696 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.worker.
2025-04-30T13:42:08,697 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:42:08,718 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.Server [] - Started @5884ms
2025-04-30T13:42:08,743 [dispatcher-event-loop-1] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:8081 for WorkerUI
2025-04-30T13:42:08,771 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-30T13:42:08,783 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @5951ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-30T13:42:08,797 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@1e4b7d20{HTTP/1.1, (http/1.1)}{0.0.0.0:8081}
2025-04-30T13:42:08,798 [dispatcher-event-loop-0] INFO  org.apache.spark.util.Utils [] - Successfully started service 'WorkerUI' on port 8081.
2025-04-30T13:42:08,813 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - Started @5989ms
2025-04-30T13:42:08,849 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@706e7b9c{/logPage,null,AVAILABLE,@Spark}
2025-04-30T13:42:08,856 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@71f5322b{/logPage/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:08,858 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3473f2d3{/,null,AVAILABLE,@Spark}
2025-04-30T13:42:08,862 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@225b3dec{/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:08,885 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1ac73d95{/static,null,AVAILABLE,@Spark}
2025-04-30T13:42:08,888 [dispatcher-event-loop-1] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:8081 for WorkerUI
2025-04-30T13:42:08,888 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5ab0dfb0{/log,null,AVAILABLE,@Spark}
2025-04-30T13:42:08,892 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.ui.WorkerWebUI [] - Bound WorkerWebUI to 0.0.0.0, and started at http://30cc727c40d4:8081
2025-04-30T13:42:08,899 [worker-register-master-threadpool-0] INFO  org.apache.spark.deploy.worker.Worker [] - Connecting to master spark-master:7077...
2025-04-30T13:42:08,908 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@33f885d7{HTTP/1.1, (http/1.1)}{0.0.0.0:8081}
2025-04-30T13:42:08,908 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-30T13:42:08,909 [dispatcher-event-loop-1] INFO  org.apache.spark.util.Utils [] - Successfully started service 'WorkerUI' on port 8081.
2025-04-30T13:42:08,934 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6a26fa2b{/metrics/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:08,950 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3b663a8f{/logPage,null,AVAILABLE,@Spark}
2025-04-30T13:42:08,957 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@73d89683{/logPage/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:08,960 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40f8f985{/,null,AVAILABLE,@Spark}
2025-04-30T13:42:08,964 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@598c8536{/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:08,971 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - Started @6140ms
2025-04-30T13:42:08,982 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@31dbd36b{/static,null,AVAILABLE,@Spark}
2025-04-30T13:42:08,986 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7b1da60{/log,null,AVAILABLE,@Spark}
2025-04-30T13:42:08,993 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.ui.WorkerWebUI [] - Bound WorkerWebUI to 0.0.0.0, and started at http://e89d73bffc2c:8081
2025-04-30T13:42:09,014 [worker-register-master-threadpool-0] INFO  org.apache.spark.deploy.worker.Worker [] - Connecting to master spark-master:7077...
2025-04-30T13:42:09,029 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 82 ms (0 ms spent in bootstraps)
2025-04-30T13:42:09,084 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@61fe7bc3{HTTP/1.1, (http/1.1)}{0.0.0.0:8081}
2025-04-30T13:42:09,086 [dispatcher-event-loop-1] INFO  org.apache.spark.util.Utils [] - Successfully started service 'WorkerUI' on port 8081.
2025-04-30T13:42:09,094 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@d6f93f5{/metrics/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:09,165 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@11053412{/logPage,null,AVAILABLE,@Spark}
2025-04-30T13:42:09,174 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3bdadb4b{/logPage/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:09,181 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2d957fac{/,null,AVAILABLE,@Spark}
2025-04-30T13:42:09,185 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@9aa2004{/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:09,204 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@8967c2c{/static,null,AVAILABLE,@Spark}
2025-04-30T13:42:09,207 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@67918a4b{/log,null,AVAILABLE,@Spark}
2025-04-30T13:42:09,216 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.ui.WorkerWebUI [] - Bound WorkerWebUI to 0.0.0.0, and started at http://4cfd2bfc0474:8081
2025-04-30T13:42:09,218 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 82 ms (0 ms spent in bootstraps)
2025-04-30T13:42:09,223 [worker-register-master-threadpool-0] INFO  org.apache.spark.deploy.worker.Worker [] - Connecting to master spark-master:7077...
2025-04-30T13:42:09,276 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6ce009ff{/metrics/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:09,335 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.master.Master [] - Registering worker 172.18.0.3:44603 with 2 cores, 3.0 GiB RAM
2025-04-30T13:42:09,407 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.worker.Worker [] - Successfully registered with master spark://22a90d2ca54f:7077
2025-04-30T13:42:09,436 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 136 ms (0 ms spent in bootstraps)
2025-04-30T13:42:09,447 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Registering worker 172.18.0.4:42969 with 2 cores, 3.0 GiB RAM
2025-04-30T13:42:09,479 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Successfully registered with master spark://22a90d2ca54f:7077
2025-04-30T13:42:09,646 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Registering worker 172.18.0.5:44831 with 2 cores, 3.0 GiB RAM
2025-04-30T13:42:09,672 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.worker.Worker [] - Successfully registered with master spark://22a90d2ca54f:7077
2025-04-30T13:42:23,298 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-30T13:42:23,303 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T13:42:23,304 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-30T13:42:23,332 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:42:23,334 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-30T13:42:23,335 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:42:23,336 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: mod-2-pr-4-data-ingestion.py
2025-04-30T13:42:23,361 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-30T13:42:23,369 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-30T13:42:23,371 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-30T13:42:23,426 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-30T13:42:23,427 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-30T13:42:23,428 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:42:23,429 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:42:23,431 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-30T13:42:23,498 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:42:23,917 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 33531.
2025-04-30T13:42:23,948 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-30T13:42:23,984 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-30T13:42:24,006 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-30T13:42:24,008 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-30T13:42:24,014 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-30T13:42:24,037 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-4107cdda-39b1-4434-8183-e68315482e71
2025-04-30T13:42:24,054 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T13:42:24,078 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-30T13:42:24,123 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @4107ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-30T13:42:24,212 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-30T13:42:24,222 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-30T13:42:24,242 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @4227ms
2025-04-30T13:42:24,271 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@29eae7bb{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T13:42:24,272 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-30T13:42:24,296 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1e6c1831{/,null,AVAILABLE,@Spark}
2025-04-30T13:42:24,408 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-30T13:42:24,461 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 28 ms (0 ms spent in bootstraps)
2025-04-30T13:42:24,588 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Registering app mod-2-pr-4-data-ingestion.py
2025-04-30T13:42:24,609 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Registered app mod-2-pr-4-data-ingestion.py with ID app-20250430134224-0000
2025-04-30T13:42:24,613 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430134224-0000 with rpId: 0
2025-04-30T13:42:24,617 [dispatcher-event-loop-10] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250430134224-0000
2025-04-30T13:42:24,628 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40547.
2025-04-30T13:42:24,629 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 22a90d2ca54f:40547
2025-04-30T13:42:24,632 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T13:42:24,635 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430134224-0000/0 on worker worker-20250430134208-172.18.0.4-42969
2025-04-30T13:42:24,638 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, 22a90d2ca54f, 40547, None)
2025-04-30T13:42:24,644 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430134224-0000/1 on worker worker-20250430134208-172.18.0.5-44831
2025-04-30T13:42:24,644 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 22a90d2ca54f:40547 with 434.4 MiB RAM, BlockManagerId(driver, 22a90d2ca54f, 40547, None)
2025-04-30T13:42:24,647 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430134224-0000/2 on worker worker-20250430134208-172.18.0.3-44603
2025-04-30T13:42:24,647 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, 22a90d2ca54f, 40547, None)
2025-04-30T13:42:24,650 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430134224-0000/0 on worker-20250430134208-172.18.0.4-42969 (172.18.0.4:42969) with 2 core(s)
2025-04-30T13:42:24,659 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, 22a90d2ca54f, 40547, None)
2025-04-30T13:42:24,662 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430134224-0000/0 on hostPort 172.18.0.4:42969 with 2 core(s), 1024.0 MiB RAM
2025-04-30T13:42:24,668 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430134224-0000/1 on worker-20250430134208-172.18.0.5-44831 (172.18.0.5:44831) with 2 core(s)
2025-04-30T13:42:24,671 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430134224-0000/1 on hostPort 172.18.0.5:44831 with 2 core(s), 1024.0 MiB RAM
2025-04-30T13:42:24,676 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430134224-0000/2 on worker-20250430134208-172.18.0.3-44603 (172.18.0.3:44603) with 2 core(s)
2025-04-30T13:42:24,678 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430134224-0000/2 on hostPort 172.18.0.3:44603 with 2 core(s), 1024.0 MiB RAM
2025-04-30T13:42:24,744 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430134224-0000/0 for mod-2-pr-4-data-ingestion.py
2025-04-30T13:42:24,752 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430134224-0000/2 for mod-2-pr-4-data-ingestion.py
2025-04-30T13:42:24,757 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430134224-0000/1 for mod-2-pr-4-data-ingestion.py
2025-04-30T13:42:24,806 [ExecutorRunner for app-20250430134224-0000/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:42:24,811 [ExecutorRunner for app-20250430134224-0000/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:42:24,814 [ExecutorRunner for app-20250430134224-0000/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:42:24,815 [ExecutorRunner for app-20250430134224-0000/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:42:24,816 [ExecutorRunner for app-20250430134224-0000/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:42:24,828 [ExecutorRunner for app-20250430134224-0000/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:42:24,829 [ExecutorRunner for app-20250430134224-0000/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:42:24,831 [ExecutorRunner for app-20250430134224-0000/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:42:24,833 [ExecutorRunner for app-20250430134224-0000/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:42:24,835 [ExecutorRunner for app-20250430134224-0000/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:42:24,850 [ExecutorRunner for app-20250430134224-0000/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:42:24,851 [ExecutorRunner for app-20250430134224-0000/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:42:24,852 [ExecutorRunner for app-20250430134224-0000/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:42:24,853 [ExecutorRunner for app-20250430134224-0000/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:42:24,853 [ExecutorRunner for app-20250430134224-0000/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:42:24,864 [ExecutorRunner for app-20250430134224-0000/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=33531" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22a90d2ca54f:33531" "--executor-id" "0" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250430134224-0000" "--worker-url" "spark://Worker@172.18.0.4:42969" "--resourceProfileId" "0"
2025-04-30T13:42:24,880 [ExecutorRunner for app-20250430134224-0000/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=33531" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22a90d2ca54f:33531" "--executor-id" "1" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250430134224-0000" "--worker-url" "spark://Worker@172.18.0.5:44831" "--resourceProfileId" "0"
2025-04-30T13:42:24,904 [ExecutorRunner for app-20250430134224-0000/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=33531" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22a90d2ca54f:33531" "--executor-id" "2" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250430134224-0000" "--worker-url" "spark://Worker@172.18.0.3:44603" "--resourceProfileId" "0"
2025-04-30T13:42:24,920 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430134224-0000 with rpId: 0
2025-04-30T13:42:24,924 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430134224-0000 with rpId: 0
2025-04-30T13:42:24,946 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430134224-0000/0 is now RUNNING
2025-04-30T13:42:24,948 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430134224-0000/1 is now RUNNING
2025-04-30T13:42:24,962 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430134224-0000 with rpId: 0
2025-04-30T13:42:24,975 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430134224-0000/2 is now RUNNING
2025-04-30T13:42:25,179 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250430134224-0000.inprogress
2025-04-30T13:42:25,478 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@1e6c1831{/,null,STOPPED,@Spark}
2025-04-30T13:42:25,494 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@bc19d02{/jobs,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,508 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@761cbb3a{/jobs/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,515 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@503d5c5f{/jobs/job,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,549 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@22fcf373{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,563 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4bdd11d{/stages,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,567 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7c37b9ad{/stages/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,582 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2a279c54{/stages/stage,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,587 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3226e90c{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,596 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@270fafbe{/stages/pool,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,599 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7af5297b{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,602 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@73d5fdae{/storage,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,605 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@29271c33{/storage/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,610 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3252b2c7{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,614 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@b80844c{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,617 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@725aac6e{/environment,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,620 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@59a94931{/environment/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,623 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@64d08a03{/executors,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,626 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1fbc6bc{/executors/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,634 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@51db85ca{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,637 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5bbfcafe{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,641 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3b9a3647{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,646 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6b9bf9e1{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,660 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@27190dc4{/static,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,665 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4227e747{/,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,670 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@c84e01b{/api,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,673 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@9ae6795{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,676 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@ebdd13e{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,687 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@30d92d52{/metrics/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:25,689 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-30T13:42:26,017 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-04-30T13:42:26,021 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
2025-04-30T13:42:26,042 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@146c2e0d{/SQL,null,AVAILABLE,@Spark}
2025-04-30T13:42:26,053 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@c8ef167{/SQL/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:26,056 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@151b8458{/SQL/execution,null,AVAILABLE,@Spark}
2025-04-30T13:42:26,059 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@25f727f0{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-04-30T13:42:26,066 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2156ed04{/static/sql,null,AVAILABLE,@Spark}
2025-04-30T13:42:27,444 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 146@30cc727c40d4
2025-04-30T13:42:27,462 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T13:42:27,464 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T13:42:27,465 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T13:42:27,462 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 148@4cfd2bfc0474
2025-04-30T13:42:27,469 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 149@e89d73bffc2c
2025-04-30T13:42:27,482 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T13:42:27,485 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T13:42:27,486 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T13:42:27,487 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T13:42:27,487 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T13:42:27,488 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T13:42:28,046 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 112 ms to list leaf files for 2 paths.
2025-04-30T13:42:28,118 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:42:28,167 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:42:28,241 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:42:28,342 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:42:28,344 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:42:28,346 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:42:28,349 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:42:28,351 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:42:28,410 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:42:28,414 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:42:28,420 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:42:28,423 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:42:28,426 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:42:28,526 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:42:28,530 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:42:28,532 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:42:28,534 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:42:28,537 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:42:28,934 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 22a90d2ca54f/172.18.0.2:33531 after 94 ms (0 ms spent in bootstraps)
2025-04-30T13:42:28,943 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 22a90d2ca54f/172.18.0.2:33531 after 91 ms (0 ms spent in bootstraps)
2025-04-30T13:42:29,014 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 22a90d2ca54f/172.18.0.2:33531 after 112 ms (0 ms spent in bootstraps)
2025-04-30T13:42:29,099 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:42:29,100 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:42:29,100 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:42:29,101 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:42:29,102 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:42:29,109 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:42:29,115 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:42:29,116 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:42:29,117 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:42:29,117 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:42:29,189 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:42:29,190 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:42:29,191 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:42:29,192 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:42:29,193 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:42:29,216 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 22a90d2ca54f/172.18.0.2:33531 after 14 ms (0 ms spent in bootstraps)
2025-04-30T13:42:29,239 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 22a90d2ca54f/172.18.0.2:33531 after 16 ms (0 ms spent in bootstraps)
2025-04-30T13:42:29,304 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 22a90d2ca54f/172.18.0.2:33531 after 20 ms (0 ms spent in bootstraps)
2025-04-30T13:42:29,352 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-214874f6-0358-491a-84ca-31bc1eb74c81/executor-7f76825e-1d5e-42e1-be91-aad549d332b1/blockmgr-4d6a6b86-cce6-4d83-884b-6985a1703c5e
2025-04-30T13:42:29,389 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-1f07eb6b-58dc-466c-8985-cf87c0363f09/executor-1ce66c55-8b21-4dc6-ab1a-03b05dfe459c/blockmgr-aeccd2dd-ed73-471e-8294-ebf71a4ed966
2025-04-30T13:42:29,445 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T13:42:29,483 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T13:42:29,500 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-c5b5d3c3-9fb6-49f9-8d71-d21534622fcd/executor-6f96ad5a-8c4e-4cfe-b3d9-474c3768bf31/blockmgr-6295dec4-f38c-4f19-97cc-bbd102f90a63
2025-04-30T13:42:29,564 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 34 ms to list leaf files for 1 paths.
2025-04-30T13:42:29,574 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T13:42:29,650 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 17 ms to list leaf files for 1 paths.
2025-04-30T13:42:29,839 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@22a90d2ca54f:33531
2025-04-30T13:42:29,840 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.3:44603
2025-04-30T13:42:29,850 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.3:44603 after 6 ms (0 ms spent in bootstraps)
2025-04-30T13:42:29,857 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.3:44603
2025-04-30T13:42:29,861 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.5:44831
2025-04-30T13:42:29,862 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@22a90d2ca54f:33531
2025-04-30T13:42:29,872 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:42:29,881 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:42:29,882 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T13:42:29,883 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:42:29,889 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T13:42:29,895 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:42:29,905 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.5:44831 after 29 ms (0 ms spent in bootstraps)
2025-04-30T13:42:29,905 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.5:44831
2025-04-30T13:42:29,928 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.4:42969
2025-04-30T13:42:29,928 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:41270) with ID 1,  ResourceProfileId 0
2025-04-30T13:42:29,930 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@22a90d2ca54f:33531
2025-04-30T13:42:29,940 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:59318) with ID 2,  ResourceProfileId 0
2025-04-30T13:42:29,940 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T13:42:29,949 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T13:42:29,949 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 1 on host 172.18.0.5
2025-04-30T13:42:29,951 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T13:42:29,952 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T13:42:29,958 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:42:29,960 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T13:42:29,960 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.4:42969 after 23 ms (0 ms spent in bootstraps)
2025-04-30T13:42:29,955 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 2 on host 172.18.0.3
2025-04-30T13:42:29,962 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:42:29,963 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.4:42969
2025-04-30T13:42:29,966 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T13:42:29,967 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T13:42:29,993 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:33508) with ID 0,  ResourceProfileId 0
2025-04-30T13:42:30,002 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T13:42:30,006 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 0 on host 172.18.0.4
2025-04-30T13:42:30,008 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T13:42:30,009 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T13:42:30,021 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41599.
2025-04-30T13:42:30,022 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.5:41599
2025-04-30T13:42:30,026 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T13:42:30,037 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(1, 172.18.0.5, 41599, None)
2025-04-30T13:42:30,040 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42137.
2025-04-30T13:42:30,041 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.3:42137
2025-04-30T13:42:30,049 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.5:41599 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.5, 41599, None)
2025-04-30T13:42:30,057 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(1, 172.18.0.5, 41599, None)
2025-04-30T13:42:30,049 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T13:42:30,061 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(1, 172.18.0.5, 41599, None)
2025-04-30T13:42:30,069 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(2, 172.18.0.3, 42137, None)
2025-04-30T13:42:30,078 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T13:42:30,081 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@5345d551 for default.
2025-04-30T13:42:30,083 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.3:42137 with 434.4 MiB RAM, BlockManagerId(2, 172.18.0.3, 42137, None)
2025-04-30T13:42:30,099 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(2, 172.18.0.3, 42137, None)
2025-04-30T13:42:30,101 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38505.
2025-04-30T13:42:30,102 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(2, 172.18.0.3, 42137, None)
2025-04-30T13:42:30,103 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.4:38505
2025-04-30T13:42:30,106 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T13:42:30,115 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T13:42:30,116 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(0, 172.18.0.4, 38505, None)
2025-04-30T13:42:30,117 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2f9a6030 for default.
2025-04-30T13:42:30,127 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.4:38505 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.4, 38505, None)
2025-04-30T13:42:30,132 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(0, 172.18.0.4, 38505, None)
2025-04-30T13:42:30,134 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(0, 172.18.0.4, 38505, None)
2025-04-30T13:42:30,144 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T13:42:30,146 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2f9a6030 for default.
2025-04-30T13:42:31,384 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T13:42:31,386 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T13:42:31,562 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 200.3 KiB, free 434.2 MiB)
2025-04-30T13:42:31,619 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
2025-04-30T13:42:31,623 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on 22a90d2ca54f:40547 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T13:42:31,629 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 0 from json at <unknown>:0
2025-04-30T13:42:31,642 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T13:42:31,778 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: json at <unknown>:0
2025-04-30T13:42:31,799 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 0 (json at <unknown>:0) with 1 output partitions
2025-04-30T13:42:31,800 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 0 (json at <unknown>:0)
2025-04-30T13:42:31,801 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T13:42:31,803 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T13:42:31,808 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0), which has no missing parents
2025-04-30T13:42:31,892 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.5 KiB, free 434.2 MiB)
2025-04-30T13:42:31,896 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
2025-04-30T13:42:31,897 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on 22a90d2ca54f:40547 (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T13:42:31,898 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 1 from broadcast at DAGScheduler.scala:1585
2025-04-30T13:42:31,917 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T13:42:31,919 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 0.0 with 1 tasks resource profile 0
2025-04-30T13:42:31,952 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.4, executor 0, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T13:42:32,000 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 0
2025-04-30T13:42:32,010 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 0.0 (TID 0)
2025-04-30T13:42:32,117 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:42:32,177 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 22a90d2ca54f/172.18.0.2:40547 after 3 ms (0 ms spent in bootstraps)
2025-04-30T13:42:32,258 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.4 MiB)
2025-04-30T13:42:32,265 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on 172.18.0.4:38505 (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T13:42:32,275 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 1 took 156 ms
2025-04-30T13:42:32,349 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.5 KiB, free 434.4 MiB)
2025-04-30T13:42:32,805 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl, range: 0-32688, partition values: [empty row]
2025-04-30T13:42:33,133 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 179.246231 ms
2025-04-30T13:42:33,135 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:42:33,144 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.3 MiB)
2025-04-30T13:42:33,147 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on 172.18.0.4:38505 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T13:42:33,149 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 0 took 13 ms
2025-04-30T13:42:33,217 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T13:42:33,369 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 0.0 (TID 0). 2450 bytes result sent to driver
2025-04-30T13:42:33,382 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 0.0 (TID 0) in 1442 ms on 172.18.0.4 (executor 0) (1/1)
2025-04-30T13:42:33,383 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-04-30T13:42:33,387 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 0 (json at <unknown>:0) finished in 1.563 s
2025-04-30T13:42:33,390 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T13:42:33,391 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 0: Stage finished
2025-04-30T13:42:33,395 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 finished: json at <unknown>:0, took 1.616472 s
2025-04-30T13:42:33,512 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T13:42:33,513 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T13:42:33,877 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 217.832783 ms
2025-04-30T13:42:33,883 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 200.2 KiB, free 434.0 MiB)
2025-04-30T13:42:33,895 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.9 MiB)
2025-04-30T13:42:33,897 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on 22a90d2ca54f:40547 (size: 34.4 KiB, free: 434.3 MiB)
2025-04-30T13:42:33,898 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 2 from count at <unknown>:0
2025-04-30T13:42:33,904 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T13:42:33,948 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Registering RDD 7 (count at <unknown>:0) as input to shuffle 0
2025-04-30T13:42:33,953 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got map stage job 1 (count at <unknown>:0) with 1 output partitions
2025-04-30T13:42:33,954 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ShuffleMapStage 1 (count at <unknown>:0)
2025-04-30T13:42:33,955 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T13:42:33,957 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T13:42:33,959 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0), which has no missing parents
2025-04-30T13:42:33,981 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 433.9 MiB)
2025-04-30T13:42:33,984 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 433.9 MiB)
2025-04-30T13:42:33,985 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on 22a90d2ca54f:40547 (size: 8.3 KiB, free: 434.3 MiB)
2025-04-30T13:42:33,987 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 3 from broadcast at DAGScheduler.scala:1585
2025-04-30T13:42:33,990 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T13:42:33,991 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 1.0 with 1 tasks resource profile 0
2025-04-30T13:42:33,995 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.3, executor 2, partition 0, PROCESS_LOCAL, 9638 bytes) 
2025-04-30T13:42:34,013 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 1
2025-04-30T13:42:34,027 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 1.0 (TID 1)
2025-04-30T13:42:34,149 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:42:34,202 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 22a90d2ca54f/172.18.0.2:40547 after 2 ms (0 ms spent in bootstraps)
2025-04-30T13:42:34,247 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 434.4 MiB)
2025-04-30T13:42:34,253 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on 172.18.0.3:42137 (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T13:42:34,261 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 3 took 111 ms
2025-04-30T13:42:34,350 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 434.4 MiB)
2025-04-30T13:42:34,980 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 217.726777 ms
2025-04-30T13:42:34,992 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl, range: 0-32688, partition values: [empty row]
2025-04-30T13:42:35,029 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 12.659774 ms
2025-04-30T13:42:35,048 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:42:35,056 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.3 MiB)
2025-04-30T13:42:35,059 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on 172.18.0.3:42137 (size: 34.4 KiB, free: 434.4 MiB)
2025-04-30T13:42:35,061 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 2 took 12 ms
2025-04-30T13:42:35,134 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T13:42:35,360 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 1.0 (TID 1). 2007 bytes result sent to driver
2025-04-30T13:42:35,369 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 1.0 (TID 1) in 1376 ms on 172.18.0.3 (executor 2) (1/1)
2025-04-30T13:42:35,370 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-04-30T13:42:35,373 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ShuffleMapStage 1 (count at <unknown>:0) finished in 1.408 s
2025-04-30T13:42:35,374 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - looking for newly runnable stages
2025-04-30T13:42:35,375 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - running: Set()
2025-04-30T13:42:35,377 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - waiting: Set()
2025-04-30T13:42:35,378 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - failed: Set()
2025-04-30T13:42:35,432 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 17.600936 ms
2025-04-30T13:42:35,469 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: count at <unknown>:0
2025-04-30T13:42:35,472 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 2 (count at <unknown>:0) with 1 output partitions
2025-04-30T13:42:35,473 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 3 (count at <unknown>:0)
2025-04-30T13:42:35,473 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List(ShuffleMapStage 2)
2025-04-30T13:42:35,474 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T13:42:35,475 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0), which has no missing parents
2025-04-30T13:42:35,496 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 433.9 MiB)
2025-04-30T13:42:35,505 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 433.9 MiB)
2025-04-30T13:42:35,515 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_4_piece0 in memory on 22a90d2ca54f:40547 (size: 6.0 KiB, free: 434.3 MiB)
2025-04-30T13:42:35,516 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 4 from broadcast at DAGScheduler.scala:1585
2025-04-30T13:42:35,519 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T13:42:35,522 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 3.0 with 1 tasks resource profile 0
2025-04-30T13:42:35,539 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 3.0 (TID 2) (172.18.0.3, executor 2, partition 0, NODE_LOCAL, 9003 bytes) 
2025-04-30T13:42:35,547 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 2
2025-04-30T13:42:35,553 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 3.0 (TID 2)
2025-04-30T13:42:35,566 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2025-04-30T13:42:35,588 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:42:35,601 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.0 MiB)
2025-04-30T13:42:35,603 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_4_piece0 in memory on 172.18.0.3:42137 (size: 6.0 KiB, free: 434.4 MiB)
2025-04-30T13:42:35,606 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 4 took 16 ms
2025-04-30T13:42:35,608 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 434.0 MiB)
2025-04-30T13:42:35,669 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Don't have map outputs for shuffle 0, fetching them
2025-04-30T13:42:35,671 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@22a90d2ca54f:33531)
2025-04-30T13:42:35,677 [dispatcher-event-loop-8] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - Asked to send map output locations for shuffle 0 to 172.18.0.3:59318
2025-04-30T13:42:35,717 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Got the map output locations
2025-04-30T13:42:35,753 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-04-30T13:42:35,757 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Started 0 remote fetches in 15 ms
2025-04-30T13:42:35,781 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 17.035606 ms
2025-04-30T13:42:35,805 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 3.0 (TID 2). 4081 bytes result sent to driver
2025-04-30T13:42:35,812 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 3.0 (TID 2) in 276 ms on 172.18.0.3 (executor 2) (1/1)
2025-04-30T13:42:35,813 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2025-04-30T13:42:35,815 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 3 (count at <unknown>:0) finished in 0.323 s
2025-04-30T13:42:35,815 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T13:42:35,816 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 3: Stage finished
2025-04-30T13:42:35,817 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 2 finished: count at <unknown>:0, took 0.347151 s
2025-04-30T13:42:35,903 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T13:42:35,904 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T13:42:35,976 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils [] - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:42:35,990 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:42:35,990 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:42:35,992 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:42:35,993 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:42:35,993 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:42:35,994 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:42:36,051 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5 stored as values in memory (estimated size 200.2 KiB, free 433.7 MiB)
2025-04-30T13:42:36,062 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.6 MiB)
2025-04-30T13:42:36,064 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_5_piece0 in memory on 22a90d2ca54f:40547 (size: 34.4 KiB, free: 434.3 MiB)
2025-04-30T13:42:36,065 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 5 from parquet at <unknown>:0
2025-04-30T13:42:36,068 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T13:42:36,079 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: parquet at <unknown>:0
2025-04-30T13:42:36,081 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 3 (parquet at <unknown>:0) with 1 output partitions
2025-04-30T13:42:36,081 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 4 (parquet at <unknown>:0)
2025-04-30T13:42:36,082 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T13:42:36,083 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T13:42:36,084 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 4 (MapPartitionsRDD[13] at parquet at <unknown>:0), which has no missing parents
2025-04-30T13:42:36,115 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6 stored as values in memory (estimated size 215.4 KiB, free 433.4 MiB)
2025-04-30T13:42:36,118 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 77.5 KiB, free 433.4 MiB)
2025-04-30T13:42:36,119 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_6_piece0 in memory on 22a90d2ca54f:40547 (size: 77.5 KiB, free: 434.2 MiB)
2025-04-30T13:42:36,120 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 6 from broadcast at DAGScheduler.scala:1585
2025-04-30T13:42:36,122 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at parquet at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T13:42:36,122 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 4.0 with 1 tasks resource profile 0
2025-04-30T13:42:36,125 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 4.0 (TID 3) (172.18.0.4, executor 0, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T13:42:36,129 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 3
2025-04-30T13:42:36,130 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 4.0 (TID 3)
2025-04-30T13:42:36,133 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2025-04-30T13:42:36,147 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:42:36,155 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 77.5 KiB, free 433.9 MiB)
2025-04-30T13:42:36,157 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_6_piece0 in memory on 172.18.0.4:38505 (size: 77.5 KiB, free: 434.3 MiB)
2025-04-30T13:42:36,160 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 6 took 11 ms
2025-04-30T13:42:36,162 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6 stored as values in memory (estimated size 215.4 KiB, free 433.7 MiB)
2025-04-30T13:42:36,232 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:42:36,232 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:42:36,234 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:42:36,235 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:42:36,236 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:42:36,237 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:42:36,245 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:42:36,248 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:42:36,279 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat [] - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-04-30T13:42:36,341 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport [] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "date_birth",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "driver_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt_current_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "first_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "last_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "license_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "phone_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_make",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_model",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_year",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary city (STRING);
  optional binary country (STRING);
  optional binary date_birth (STRING);
  optional int64 driver_id;
  optional binary dt_current_timestamp (STRING);
  optional binary first_name (STRING);
  optional binary last_name (STRING);
  optional binary license_number (STRING);
  optional binary phone_number (STRING);
  optional binary uuid (STRING);
  optional binary vehicle_make (STRING);
  optional binary vehicle_model (STRING);
  optional binary vehicle_type (STRING);
  optional int64 vehicle_year;
}

       
2025-04-30T13:42:36,374 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.0 in stage 4.0 (TID 3)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301342365194476820943386852_0004_m_000000_3 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430134224-0000/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
2025-04-30T13:42:36,411 [task-result-getter-3] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.0 in stage 4.0 (TID 3) (172.18.0.4 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301342365194476820943386852_0004_m_000000_3 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430134224-0000/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

2025-04-30T13:42:36,414 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.1 in stage 4.0 (TID 4) (172.18.0.3, executor 2, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T13:42:36,417 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 4
2025-04-30T13:42:36,418 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.executor.Executor [] - Running task 0.1 in stage 4.0 (TID 4)
2025-04-30T13:42:36,422 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:42:36,429 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.4:38505 after 1 ms (0 ms spent in bootstraps)
2025-04-30T13:42:36,447 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 77.5 KiB, free 433.9 MiB)
2025-04-30T13:42:36,450 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_6_piece0 in memory on 172.18.0.3:42137 (size: 77.5 KiB, free: 434.3 MiB)
2025-04-30T13:42:36,452 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 6 took 29 ms
2025-04-30T13:42:36,454 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6 stored as values in memory (estimated size 215.4 KiB, free 433.7 MiB)
2025-04-30T13:42:36,515 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:42:36,516 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:42:36,518 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:42:36,518 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:42:36,519 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:42:36,521 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:42:36,530 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:42:36,533 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:42:36,567 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat [] - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-04-30T13:42:36,690 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport [] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "date_birth",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "driver_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt_current_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "first_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "last_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "license_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "phone_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_make",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_model",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_year",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary city (STRING);
  optional binary country (STRING);
  optional binary date_birth (STRING);
  optional int64 driver_id;
  optional binary dt_current_timestamp (STRING);
  optional binary first_name (STRING);
  optional binary last_name (STRING);
  optional binary license_number (STRING);
  optional binary phone_number (STRING);
  optional binary uuid (STRING);
  optional binary vehicle_make (STRING);
  optional binary vehicle_model (STRING);
  optional binary vehicle_type (STRING);
  optional int64 vehicle_year;
}

       
2025-04-30T13:42:36,729 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.1 in stage 4.0 (TID 4)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301342365194476820943386852_0004_m_000000_4 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430134224-0000/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
2025-04-30T13:42:36,761 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.1 in stage 4.0 (TID 4) (172.18.0.3 executor 2): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301342365194476820943386852_0004_m_000000_4 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430134224-0000/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

2025-04-30T13:42:36,763 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.2 in stage 4.0 (TID 5) (172.18.0.4, executor 0, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T13:42:36,767 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 5
2025-04-30T13:42:36,768 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.executor.Executor [] - Running task 0.2 in stage 4.0 (TID 5)
2025-04-30T13:42:36,796 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:42:36,797 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:42:36,798 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:42:36,798 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:42:36,799 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:42:36,800 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:42:36,801 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:42:36,802 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:42:36,804 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat [] - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-04-30T13:42:36,811 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport [] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "date_birth",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "driver_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt_current_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "first_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "last_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "license_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "phone_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_make",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_model",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_year",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary city (STRING);
  optional binary country (STRING);
  optional binary date_birth (STRING);
  optional int64 driver_id;
  optional binary dt_current_timestamp (STRING);
  optional binary first_name (STRING);
  optional binary last_name (STRING);
  optional binary license_number (STRING);
  optional binary phone_number (STRING);
  optional binary uuid (STRING);
  optional binary vehicle_make (STRING);
  optional binary vehicle_model (STRING);
  optional binary vehicle_type (STRING);
  optional int64 vehicle_year;
}

       
2025-04-30T13:42:36,837 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.2 in stage 4.0 (TID 5)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301342365194476820943386852_0004_m_000000_5 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430134224-0000/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
2025-04-30T13:42:36,844 [task-result-getter-1] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.2 in stage 4.0 (TID 5) (172.18.0.4 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301342365194476820943386852_0004_m_000000_5 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430134224-0000/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

2025-04-30T13:42:36,846 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.3 in stage 4.0 (TID 6) (172.18.0.4, executor 0, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T13:42:36,851 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 6
2025-04-30T13:42:36,852 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.executor.Executor [] - Running task 0.3 in stage 4.0 (TID 6)
2025-04-30T13:42:36,878 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:42:36,879 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:42:36,880 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:42:36,881 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:42:36,881 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:42:36,882 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:42:36,882 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:42:36,884 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:42:36,886 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat [] - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-04-30T13:42:36,891 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport [] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "date_birth",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "driver_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt_current_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "first_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "last_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "license_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "phone_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_make",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_model",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_year",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary city (STRING);
  optional binary country (STRING);
  optional binary date_birth (STRING);
  optional int64 driver_id;
  optional binary dt_current_timestamp (STRING);
  optional binary first_name (STRING);
  optional binary last_name (STRING);
  optional binary license_number (STRING);
  optional binary phone_number (STRING);
  optional binary uuid (STRING);
  optional binary vehicle_make (STRING);
  optional binary vehicle_model (STRING);
  optional binary vehicle_type (STRING);
  optional int64 vehicle_year;
}

       
2025-04-30T13:42:36,920 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.3 in stage 4.0 (TID 6)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301342365194476820943386852_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430134224-0000/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
2025-04-30T13:42:36,927 [task-result-getter-2] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.3 in stage 4.0 (TID 6) (172.18.0.4 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301342365194476820943386852_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430134224-0000/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

2025-04-30T13:42:36,928 [task-result-getter-2] ERROR org.apache.spark.scheduler.TaskSetManager [] - Task 0 in stage 4.0 failed 4 times; aborting job
2025-04-30T13:42:36,931 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2025-04-30T13:42:36,933 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Cancelling stage 4
2025-04-30T13:42:36,934 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 4: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 6) (172.18.0.4 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301342365194476820943386852_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430134224-0000/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

Driver stacktrace:
2025-04-30T13:42:36,936 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 4 (parquet at <unknown>:0) failed in 0.849 s due to Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 6) (172.18.0.4 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301342365194476820943386852_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430134224-0000/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

Driver stacktrace:
2025-04-30T13:42:36,940 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 3 failed: parquet at <unknown>:0, took 0.860685 s
2025-04-30T13:42:36,942 [Thread-4] ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter [] - Aborting job f2983b89-aa68-407f-bc76-5194d1d941cf.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 6) (172.18.0.4 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301342365194476820943386852_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430134224-0000/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[scala-library-2.12.18.jar:?]
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[scala-library-2.12.18.jar:?]
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at scala.Option.foreach(Option.scala:407) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461) ~[spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76) [spark-sql-api_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
	at java.lang.reflect.Method.invoke(Unknown Source) ~[?:?]
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) [py4j-0.10.9.7.jar:?]
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374) [py4j-0.10.9.7.jar:?]
	at py4j.Gateway.invoke(Gateway.java:282) [py4j-0.10.9.7.jar:?]
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) [py4j-0.10.9.7.jar:?]
	at py4j.commands.CallCommand.execute(CallCommand.java:79) [py4j-0.10.9.7.jar:?]
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182) [py4j-0.10.9.7.jar:?]
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106) [py4j-0.10.9.7.jar:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
Caused by: java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301342365194476820943386852_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430134224-0000/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]
	... 1 more
2025-04-30T13:42:37,362 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Invoking stop() from shutdown hook
2025-04-30T13:42:37,363 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-30T13:42:37,373 [shutdown-hook-0] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@29eae7bb{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T13:42:37,378 [shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://22a90d2ca54f:4040
2025-04-30T13:42:37,381 [shutdown-hook-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-30T13:42:37,382 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-30T13:42:37,392 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T13:42:37,392 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T13:42:37,398 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T13:42:37,402 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250430134224-0000
2025-04-30T13:42:37,405 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250430134224-0000
2025-04-30T13:42:37,421 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T13:42:37,422 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T13:42:37,423 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T13:42:37,423 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T13:42:37,430 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T13:42:37,432 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T13:42:37,437 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T13:42:37,441 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T13:42:37,442 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430134224-0000/0
2025-04-30T13:42:37,443 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430134224-0000/1
2025-04-30T13:42:37,445 [ExecutorRunner for app-20250430134224-0000/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430134224-0000/0 interrupted
2025-04-30T13:42:37,445 [ExecutorRunner for app-20250430134224-0000/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430134224-0000/1 interrupted
2025-04-30T13:42:37,446 [ExecutorRunner for app-20250430134224-0000/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T13:42:37,447 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430134224-0000/2
2025-04-30T13:42:37,449 [dispatcher-event-loop-10] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-30T13:42:37,450 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T13:42:37,447 [ExecutorRunner for app-20250430134224-0000/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T13:42:37,456 [ExecutorRunner for app-20250430134224-0000/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430134224-0000/2 interrupted
2025-04-30T13:42:37,459 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T13:42:37,460 [ExecutorRunner for app-20250430134224-0000/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T13:42:37,468 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T13:42:37,477 [shutdown-hook-0] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T13:42:37,479 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T13:42:37,491 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-30T13:42:37,493 [dispatcher-event-loop-9] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430134224-0000/2
2025-04-30T13:42:37,493 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430134224-0000/2 finished with state KILLED exitStatus 0
2025-04-30T13:42:37,495 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-30T13:42:37,497 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-30T13:42:37,500 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430134224-0000, execId=2)
2025-04-30T13:42:37,504 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:48624 got disassociated, removing it.
2025-04-30T13:42:37,507 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430134224-0000
2025-04-30T13:42:37,507 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430134224-0000 removed, cleanupLocalDirs = true
2025-04-30T13:42:37,508 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - 22a90d2ca54f:33531 got disassociated, removing it.
2025-04-30T13:42:37,512 [dispatcher-event-loop-8] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430134224-0000/0
2025-04-30T13:42:37,512 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-30T13:42:37,514 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T13:42:37,513 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430134224-0000/0 finished with state KILLED exitStatus 0
2025-04-30T13:42:37,516 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-f1ab1ffa-448c-4e20-8fb9-7400d0e3487c
2025-04-30T13:42:37,520 [dispatcher-event-loop-6] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-30T13:42:37,521 [dispatcher-event-loop-6] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430134224-0000, execId=0)
2025-04-30T13:42:37,521 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-71ab731f-e845-475d-a757-0e8f4a183972
2025-04-30T13:42:37,525 [dispatcher-event-loop-6] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430134224-0000 removed, cleanupLocalDirs = true
2025-04-30T13:42:37,525 [dispatcher-event-loop-2] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430134224-0000/1
2025-04-30T13:42:37,527 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430134224-0000
2025-04-30T13:42:37,526 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430134224-0000/1 finished with state KILLED exitStatus 0
2025-04-30T13:42:37,529 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-71ab731f-e845-475d-a757-0e8f4a183972/pyspark-9ebeb2e9-ed1a-4c30-9afa-e7bef6b3186c
2025-04-30T13:42:37,530 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-30T13:42:37,533 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430134224-0000, execId=1)
2025-04-30T13:42:37,536 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430134224-0000 removed, cleanupLocalDirs = true
2025-04-30T13:42:37,536 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430134224-0000
2025-04-30T13:56:37,732 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-30T13:56:37,738 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T13:56:37,739 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-30T13:56:37,767 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:56:37,768 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-30T13:56:37,769 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:56:37,770 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: mod-2-pr-4-data-ingestion.py
2025-04-30T13:56:37,787 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-30T13:56:37,796 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-30T13:56:37,797 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-30T13:56:37,841 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-30T13:56:37,843 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-30T13:56:37,845 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:56:37,846 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:56:37,847 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-30T13:56:37,906 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:56:38,179 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 44543.
2025-04-30T13:56:38,206 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-30T13:56:38,238 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-30T13:56:38,257 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-30T13:56:38,258 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-30T13:56:38,264 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-30T13:56:38,287 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-09c1f608-9ef1-42aa-bce2-e03a18ecdbbb
2025-04-30T13:56:38,303 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T13:56:38,321 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-30T13:56:38,370 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3077ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-30T13:56:38,453 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-30T13:56:38,467 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-30T13:56:38,488 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @3195ms
2025-04-30T13:56:38,522 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@5f2f1763{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T13:56:38,522 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-30T13:56:38,553 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@47227198{/,null,AVAILABLE,@Spark}
2025-04-30T13:56:38,668 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-30T13:56:38,724 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 35 ms (0 ms spent in bootstraps)
2025-04-30T13:56:38,815 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Registering app mod-2-pr-4-data-ingestion.py
2025-04-30T13:56:38,818 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Registered app mod-2-pr-4-data-ingestion.py with ID app-20250430135638-0001
2025-04-30T13:56:38,819 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430135638-0001 with rpId: 0
2025-04-30T13:56:38,822 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430135638-0001/0 on worker worker-20250430134208-172.18.0.4-42969
2025-04-30T13:56:38,825 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430135638-0001/1 on worker worker-20250430134208-172.18.0.5-44831
2025-04-30T13:56:38,827 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250430135638-0001
2025-04-30T13:56:38,829 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430135638-0001/2 on worker worker-20250430134208-172.18.0.3-44603
2025-04-30T13:56:38,832 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430135638-0001/0 on worker-20250430134208-172.18.0.4-42969 (172.18.0.4:42969) with 2 core(s)
2025-04-30T13:56:38,833 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430135638-0001/0 for mod-2-pr-4-data-ingestion.py
2025-04-30T13:56:38,836 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430135638-0001/0 on hostPort 172.18.0.4:42969 with 2 core(s), 1024.0 MiB RAM
2025-04-30T13:56:38,838 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430135638-0001/1 on worker-20250430134208-172.18.0.5-44831 (172.18.0.5:44831) with 2 core(s)
2025-04-30T13:56:38,840 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430135638-0001/1 for mod-2-pr-4-data-ingestion.py
2025-04-30T13:56:38,842 [ExecutorRunner for app-20250430135638-0001/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:56:38,842 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430135638-0001/1 on hostPort 172.18.0.5:44831 with 2 core(s), 1024.0 MiB RAM
2025-04-30T13:56:38,844 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41001.
2025-04-30T13:56:38,844 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430135638-0001/2 on worker-20250430134208-172.18.0.3-44603 (172.18.0.3:44603) with 2 core(s)
2025-04-30T13:56:38,845 [ExecutorRunner for app-20250430135638-0001/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:56:38,847 [ExecutorRunner for app-20250430135638-0001/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:56:38,847 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430135638-0001/2 on hostPort 172.18.0.3:44603 with 2 core(s), 1024.0 MiB RAM
2025-04-30T13:56:38,846 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 22a90d2ca54f:41001
2025-04-30T13:56:38,849 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430135638-0001/2 for mod-2-pr-4-data-ingestion.py
2025-04-30T13:56:38,849 [ExecutorRunner for app-20250430135638-0001/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:56:38,851 [ExecutorRunner for app-20250430135638-0001/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:56:38,852 [ExecutorRunner for app-20250430135638-0001/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:56:38,853 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T13:56:38,853 [ExecutorRunner for app-20250430135638-0001/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:56:38,855 [ExecutorRunner for app-20250430135638-0001/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:56:38,856 [ExecutorRunner for app-20250430135638-0001/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:56:38,856 [ExecutorRunner for app-20250430135638-0001/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:56:38,857 [ExecutorRunner for app-20250430135638-0001/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:56:38,857 [ExecutorRunner for app-20250430135638-0001/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:56:38,858 [ExecutorRunner for app-20250430135638-0001/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:56:38,859 [ExecutorRunner for app-20250430135638-0001/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:56:38,860 [ExecutorRunner for app-20250430135638-0001/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:56:38,868 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, 22a90d2ca54f, 41001, None)
2025-04-30T13:56:38,875 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 22a90d2ca54f:41001 with 434.4 MiB RAM, BlockManagerId(driver, 22a90d2ca54f, 41001, None)
2025-04-30T13:56:38,881 [ExecutorRunner for app-20250430135638-0001/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=44543" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22a90d2ca54f:44543" "--executor-id" "0" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250430135638-0001" "--worker-url" "spark://Worker@172.18.0.4:42969" "--resourceProfileId" "0"
2025-04-30T13:56:38,881 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, 22a90d2ca54f, 41001, None)
2025-04-30T13:56:38,885 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, 22a90d2ca54f, 41001, None)
2025-04-30T13:56:38,886 [ExecutorRunner for app-20250430135638-0001/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=44543" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22a90d2ca54f:44543" "--executor-id" "2" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250430135638-0001" "--worker-url" "spark://Worker@172.18.0.3:44603" "--resourceProfileId" "0"
2025-04-30T13:56:38,888 [ExecutorRunner for app-20250430135638-0001/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=44543" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22a90d2ca54f:44543" "--executor-id" "1" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250430135638-0001" "--worker-url" "spark://Worker@172.18.0.5:44831" "--resourceProfileId" "0"
2025-04-30T13:56:38,902 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430135638-0001 with rpId: 0
2025-04-30T13:56:38,911 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430135638-0001 with rpId: 0
2025-04-30T13:56:38,915 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430135638-0001 with rpId: 0
2025-04-30T13:56:38,948 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430135638-0001/0 is now RUNNING
2025-04-30T13:56:38,950 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430135638-0001/1 is now RUNNING
2025-04-30T13:56:38,955 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430135638-0001/2 is now RUNNING
2025-04-30T13:56:39,322 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250430135638-0001.inprogress
2025-04-30T13:56:39,546 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@47227198{/,null,STOPPED,@Spark}
2025-04-30T13:56:39,549 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@e5e455c{/jobs,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,551 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5f7d9b82{/jobs/json,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,554 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@61a08f9{/jobs/job,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,557 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3c138251{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,562 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1ec6170e{/stages,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,564 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@52d5cde7{/stages/json,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,567 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5e975084{/stages/stage,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,572 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@10eb3fdc{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,578 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5694be8{/stages/pool,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,582 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1f0b4f45{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,587 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@405d0340{/storage,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,592 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6fa8cc59{/storage/json,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,595 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@69b75404{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,598 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@cfd797b{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,600 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5ea3e9bc{/environment,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,602 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@69a0fd62{/environment/json,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,606 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1982ab1d{/executors,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,611 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@72086c1a{/executors/json,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,615 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4c7c4c12{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,617 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@f32ba2f{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,620 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@37d62d67{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,624 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@48eb68af{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,635 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@70fe914b{/static,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,639 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@171c8b74{/,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,643 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7e69178e{/api,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,646 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@78436f07{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,649 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7f5515e3{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,663 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5324a0b1{/metrics/json,null,AVAILABLE,@Spark}
2025-04-30T13:56:39,667 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-30T13:56:40,019 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-04-30T13:56:40,024 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
2025-04-30T13:56:40,063 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@64cac292{/SQL,null,AVAILABLE,@Spark}
2025-04-30T13:56:40,069 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@774160d3{/SQL/json,null,AVAILABLE,@Spark}
2025-04-30T13:56:40,076 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@140a7db6{/SQL/execution,null,AVAILABLE,@Spark}
2025-04-30T13:56:40,083 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@74691a2e{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-04-30T13:56:40,090 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2623541b{/static/sql,null,AVAILABLE,@Spark}
2025-04-30T13:56:40,895 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 235@e89d73bffc2c
2025-04-30T13:56:40,908 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T13:56:40,911 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T13:56:40,912 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T13:56:40,962 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 229@30cc727c40d4
2025-04-30T13:56:40,975 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T13:56:40,977 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T13:56:40,977 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T13:56:40,980 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 226@4cfd2bfc0474
2025-04-30T13:56:40,995 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T13:56:40,997 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T13:56:40,998 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T13:56:41,515 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:56:41,609 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:56:41,751 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T13:56:41,765 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:56:41,776 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:56:41,780 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:56:41,796 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:56:41,801 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:56:41,820 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 210 ms to list leaf files for 2 paths.
2025-04-30T13:56:41,878 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:56:41,895 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:56:41,901 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:56:41,905 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:56:41,918 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:56:42,087 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:56:42,089 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:56:42,092 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:56:42,094 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:56:42,096 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:56:42,444 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 22a90d2ca54f/172.18.0.2:44543 after 74 ms (0 ms spent in bootstraps)
2025-04-30T13:56:42,453 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 22a90d2ca54f/172.18.0.2:44543 after 101 ms (0 ms spent in bootstraps)
2025-04-30T13:56:42,594 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 22a90d2ca54f/172.18.0.2:44543 after 102 ms (0 ms spent in bootstraps)
2025-04-30T13:56:42,657 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:56:42,659 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:56:42,659 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:56:42,660 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:56:42,660 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:56:42,661 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:56:42,662 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:56:42,662 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:56:42,663 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:56:42,665 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:56:42,754 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 22a90d2ca54f/172.18.0.2:44543 after 4 ms (0 ms spent in bootstraps)
2025-04-30T13:56:42,762 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 22a90d2ca54f/172.18.0.2:44543 after 10 ms (0 ms spent in bootstraps)
2025-04-30T13:56:42,771 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T13:56:42,772 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T13:56:42,773 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T13:56:42,774 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T13:56:42,774 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T13:56:42,850 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 22a90d2ca54f/172.18.0.2:44543 after 4 ms (0 ms spent in bootstraps)
2025-04-30T13:56:42,880 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-c5b5d3c3-9fb6-49f9-8d71-d21534622fcd/executor-7b65c479-47ea-4994-9379-641492ac118d/blockmgr-3d547fe4-cba6-4799-b2d3-3bad5d4b5a45
2025-04-30T13:56:42,880 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-214874f6-0358-491a-84ca-31bc1eb74c81/executor-4e6ea827-d5dc-4b2d-81cb-94dfc1802263/blockmgr-0c04d6d8-f204-4756-96ba-da31b4cddc31
2025-04-30T13:56:42,933 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T13:56:42,939 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T13:56:42,966 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-1f07eb6b-58dc-466c-8985-cf87c0363f09/executor-1396f19f-a68c-4e6f-9061-6fa3a30f72ee/blockmgr-0de37cd7-5432-4ee8-85a9-b32b91bc909c
2025-04-30T13:56:43,022 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T13:56:43,211 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 19 ms to list leaf files for 1 paths.
2025-04-30T13:56:43,282 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@22a90d2ca54f:44543
2025-04-30T13:56:43,284 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.4:42969
2025-04-30T13:56:43,287 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@22a90d2ca54f:44543
2025-04-30T13:56:43,291 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.3:44603
2025-04-30T13:56:43,297 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.4:42969 after 8 ms (0 ms spent in bootstraps)
2025-04-30T13:56:43,301 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.4:42969
2025-04-30T13:56:43,305 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:56:43,305 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.3:44603 after 4 ms (0 ms spent in bootstraps)
2025-04-30T13:56:43,307 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T13:56:43,312 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:56:43,314 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T13:56:43,315 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:56:43,315 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.3:44603
2025-04-30T13:56:43,316 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:56:43,323 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 45 ms to list leaf files for 1 paths.
2025-04-30T13:56:43,354 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:35938) with ID 2,  ResourceProfileId 0
2025-04-30T13:56:43,364 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:33950) with ID 0,  ResourceProfileId 0
2025-04-30T13:56:43,373 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T13:56:43,374 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T13:56:43,380 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 0 on host 172.18.0.4
2025-04-30T13:56:43,381 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 2 on host 172.18.0.3
2025-04-30T13:56:43,381 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T13:56:43,382 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T13:56:43,382 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T13:56:43,384 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T13:56:43,405 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@22a90d2ca54f:44543
2025-04-30T13:56:43,407 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.5:44831
2025-04-30T13:56:43,432 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:56:43,433 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T13:56:43,433 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.5:44831 after 19 ms (0 ms spent in bootstraps)
2025-04-30T13:56:43,435 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T13:56:43,442 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.5:44831
2025-04-30T13:56:43,452 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33901.
2025-04-30T13:56:43,458 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.4:33901
2025-04-30T13:56:43,461 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T13:56:43,473 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40597.
2025-04-30T13:56:43,474 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.3:40597
2025-04-30T13:56:43,477 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(0, 172.18.0.4, 33901, None)
2025-04-30T13:56:43,477 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T13:56:43,479 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:34226) with ID 1,  ResourceProfileId 0
2025-04-30T13:56:43,486 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T13:56:43,488 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(2, 172.18.0.3, 40597, None)
2025-04-30T13:56:43,491 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 1 on host 172.18.0.5
2025-04-30T13:56:43,495 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.4:33901 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.4, 33901, None)
2025-04-30T13:56:43,498 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T13:56:43,500 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T13:56:43,511 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(0, 172.18.0.4, 33901, None)
2025-04-30T13:56:43,514 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.3:40597 with 434.4 MiB RAM, BlockManagerId(2, 172.18.0.3, 40597, None)
2025-04-30T13:56:43,516 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(0, 172.18.0.4, 33901, None)
2025-04-30T13:56:43,520 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(2, 172.18.0.3, 40597, None)
2025-04-30T13:56:43,522 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(2, 172.18.0.3, 40597, None)
2025-04-30T13:56:43,529 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T13:56:43,532 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2f9a6030 for default.
2025-04-30T13:56:43,537 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T13:56:43,540 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@33cbe95 for default.
2025-04-30T13:56:43,575 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34683.
2025-04-30T13:56:43,576 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.5:34683
2025-04-30T13:56:43,581 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T13:56:43,588 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(1, 172.18.0.5, 34683, None)
2025-04-30T13:56:43,599 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.5:34683 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.5, 34683, None)
2025-04-30T13:56:43,604 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(1, 172.18.0.5, 34683, None)
2025-04-30T13:56:43,606 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(1, 172.18.0.5, 34683, None)
2025-04-30T13:56:43,617 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T13:56:43,618 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6441b79f for default.
2025-04-30T13:56:44,593 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T13:56:44,595 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T13:56:44,789 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 200.3 KiB, free 434.2 MiB)
2025-04-30T13:56:44,860 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
2025-04-30T13:56:44,865 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on 22a90d2ca54f:41001 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T13:56:44,871 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 0 from json at <unknown>:0
2025-04-30T13:56:44,885 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T13:56:45,004 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: json at <unknown>:0
2025-04-30T13:56:45,019 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 0 (json at <unknown>:0) with 1 output partitions
2025-04-30T13:56:45,020 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 0 (json at <unknown>:0)
2025-04-30T13:56:45,021 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T13:56:45,023 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T13:56:45,029 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0), which has no missing parents
2025-04-30T13:56:45,100 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.5 KiB, free 434.2 MiB)
2025-04-30T13:56:45,103 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
2025-04-30T13:56:45,104 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on 22a90d2ca54f:41001 (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T13:56:45,106 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 1 from broadcast at DAGScheduler.scala:1585
2025-04-30T13:56:45,124 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T13:56:45,125 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 0.0 with 1 tasks resource profile 0
2025-04-30T13:56:45,153 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.5, executor 1, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T13:56:45,173 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 0
2025-04-30T13:56:45,183 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 0.0 (TID 0)
2025-04-30T13:56:45,278 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:56:45,319 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 22a90d2ca54f/172.18.0.2:41001 after 1 ms (0 ms spent in bootstraps)
2025-04-30T13:56:45,387 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.4 MiB)
2025-04-30T13:56:45,396 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on 172.18.0.5:34683 (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T13:56:45,402 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 1 took 124 ms
2025-04-30T13:56:45,459 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.5 KiB, free 434.4 MiB)
2025-04-30T13:56:45,750 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl, range: 0-32688, partition values: [empty row]
2025-04-30T13:56:46,040 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 162.643316 ms
2025-04-30T13:56:46,042 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:56:46,050 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.3 MiB)
2025-04-30T13:56:46,053 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on 172.18.0.5:34683 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T13:56:46,055 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 0 took 11 ms
2025-04-30T13:56:46,122 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T13:56:46,261 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 0.0 (TID 0). 2450 bytes result sent to driver
2025-04-30T13:56:46,273 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 0.0 (TID 0) in 1131 ms on 172.18.0.5 (executor 1) (1/1)
2025-04-30T13:56:46,276 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-04-30T13:56:46,281 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 0 (json at <unknown>:0) finished in 1.236 s
2025-04-30T13:56:46,284 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T13:56:46,285 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 0: Stage finished
2025-04-30T13:56:46,289 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 finished: json at <unknown>:0, took 1.284554 s
2025-04-30T13:56:46,402 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T13:56:46,403 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T13:56:46,529 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_0_piece0 on 22a90d2ca54f:41001 in memory (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T13:56:46,540 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_0_piece0 on 172.18.0.5:34683 in memory (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T13:56:46,575 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on 172.18.0.5:34683 in memory (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T13:56:46,581 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on 22a90d2ca54f:41001 in memory (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T13:56:46,806 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 228.43076 ms
2025-04-30T13:56:46,813 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 200.2 KiB, free 434.2 MiB)
2025-04-30T13:56:46,825 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
2025-04-30T13:56:46,827 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on 22a90d2ca54f:41001 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T13:56:46,829 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 2 from count at <unknown>:0
2025-04-30T13:56:46,833 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T13:56:46,871 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Registering RDD 7 (count at <unknown>:0) as input to shuffle 0
2025-04-30T13:56:46,876 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got map stage job 1 (count at <unknown>:0) with 1 output partitions
2025-04-30T13:56:46,877 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ShuffleMapStage 1 (count at <unknown>:0)
2025-04-30T13:56:46,878 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T13:56:46,880 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T13:56:46,881 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0), which has no missing parents
2025-04-30T13:56:46,899 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 434.2 MiB)
2025-04-30T13:56:46,902 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 434.1 MiB)
2025-04-30T13:56:46,903 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on 22a90d2ca54f:41001 (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T13:56:46,905 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 3 from broadcast at DAGScheduler.scala:1585
2025-04-30T13:56:46,909 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T13:56:46,910 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 1.0 with 1 tasks resource profile 0
2025-04-30T13:56:46,914 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.4, executor 0, partition 0, PROCESS_LOCAL, 9638 bytes) 
2025-04-30T13:56:46,928 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 1
2025-04-30T13:56:46,940 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 1.0 (TID 1)
2025-04-30T13:56:47,053 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:56:47,106 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 22a90d2ca54f/172.18.0.2:41001 after 5 ms (0 ms spent in bootstraps)
2025-04-30T13:56:47,132 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 434.4 MiB)
2025-04-30T13:56:47,137 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on 172.18.0.4:33901 (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T13:56:47,142 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 3 took 88 ms
2025-04-30T13:56:47,199 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 434.4 MiB)
2025-04-30T13:56:47,746 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 179.647306 ms
2025-04-30T13:56:47,755 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl, range: 0-32688, partition values: [empty row]
2025-04-30T13:56:47,786 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 10.460848 ms
2025-04-30T13:56:47,803 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:56:47,811 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.3 MiB)
2025-04-30T13:56:47,814 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on 172.18.0.4:33901 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T13:56:47,816 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 2 took 12 ms
2025-04-30T13:56:47,886 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T13:56:48,067 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 1.0 (TID 1). 2007 bytes result sent to driver
2025-04-30T13:56:48,075 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 1.0 (TID 1) in 1162 ms on 172.18.0.4 (executor 0) (1/1)
2025-04-30T13:56:48,076 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-04-30T13:56:48,079 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ShuffleMapStage 1 (count at <unknown>:0) finished in 1.194 s
2025-04-30T13:56:48,080 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - looking for newly runnable stages
2025-04-30T13:56:48,081 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - running: Set()
2025-04-30T13:56:48,082 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - waiting: Set()
2025-04-30T13:56:48,083 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - failed: Set()
2025-04-30T13:56:48,133 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 17.167899 ms
2025-04-30T13:56:48,165 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: count at <unknown>:0
2025-04-30T13:56:48,168 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 2 (count at <unknown>:0) with 1 output partitions
2025-04-30T13:56:48,169 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 3 (count at <unknown>:0)
2025-04-30T13:56:48,169 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List(ShuffleMapStage 2)
2025-04-30T13:56:48,170 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T13:56:48,172 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0), which has no missing parents
2025-04-30T13:56:48,184 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 434.1 MiB)
2025-04-30T13:56:48,200 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.1 MiB)
2025-04-30T13:56:48,203 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_4_piece0 in memory on 22a90d2ca54f:41001 (size: 6.0 KiB, free: 434.4 MiB)
2025-04-30T13:56:48,208 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_3_piece0 on 22a90d2ca54f:41001 in memory (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T13:56:48,210 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 4 from broadcast at DAGScheduler.scala:1585
2025-04-30T13:56:48,219 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T13:56:48,219 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 3.0 with 1 tasks resource profile 0
2025-04-30T13:56:48,226 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_3_piece0 on 172.18.0.4:33901 in memory (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T13:56:48,231 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 3.0 (TID 2) (172.18.0.4, executor 0, partition 0, NODE_LOCAL, 9003 bytes) 
2025-04-30T13:56:48,236 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 2
2025-04-30T13:56:48,238 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 3.0 (TID 2)
2025-04-30T13:56:48,250 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2025-04-30T13:56:48,268 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:56:48,280 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.0 MiB)
2025-04-30T13:56:48,282 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_4_piece0 in memory on 172.18.0.4:33901 (size: 6.0 KiB, free: 434.4 MiB)
2025-04-30T13:56:48,285 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 4 took 16 ms
2025-04-30T13:56:48,287 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 434.0 MiB)
2025-04-30T13:56:48,360 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Don't have map outputs for shuffle 0, fetching them
2025-04-30T13:56:48,362 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@22a90d2ca54f:44543)
2025-04-30T13:56:48,367 [dispatcher-event-loop-9] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - Asked to send map output locations for shuffle 0 to 172.18.0.4:33950
2025-04-30T13:56:48,412 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Got the map output locations
2025-04-30T13:56:48,440 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-04-30T13:56:48,445 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Started 0 remote fetches in 15 ms
2025-04-30T13:56:48,466 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 15.835429 ms
2025-04-30T13:56:48,482 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 3.0 (TID 2). 4081 bytes result sent to driver
2025-04-30T13:56:48,491 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 3.0 (TID 2) in 264 ms on 172.18.0.4 (executor 0) (1/1)
2025-04-30T13:56:48,492 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2025-04-30T13:56:48,494 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 3 (count at <unknown>:0) finished in 0.313 s
2025-04-30T13:56:48,495 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T13:56:48,496 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 3: Stage finished
2025-04-30T13:56:48,497 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 2 finished: count at <unknown>:0, took 0.331461 s
2025-04-30T13:56:48,591 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T13:56:48,592 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T13:56:48,669 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils [] - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:56:48,683 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:56:48,684 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:56:48,685 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:56:48,686 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:56:48,687 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:56:48,688 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:56:48,748 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5 stored as values in memory (estimated size 200.2 KiB, free 434.0 MiB)
2025-04-30T13:56:48,763 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_4_piece0 on 22a90d2ca54f:41001 in memory (size: 6.0 KiB, free: 434.4 MiB)
2025-04-30T13:56:48,768 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_4_piece0 on 172.18.0.4:33901 in memory (size: 6.0 KiB, free: 434.4 MiB)
2025-04-30T13:56:48,774 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
2025-04-30T13:56:48,777 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_5_piece0 in memory on 22a90d2ca54f:41001 (size: 34.5 KiB, free: 434.3 MiB)
2025-04-30T13:56:48,779 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 5 from parquet at <unknown>:0
2025-04-30T13:56:48,783 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T13:56:48,793 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: parquet at <unknown>:0
2025-04-30T13:56:48,795 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 3 (parquet at <unknown>:0) with 1 output partitions
2025-04-30T13:56:48,795 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 4 (parquet at <unknown>:0)
2025-04-30T13:56:48,796 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T13:56:48,797 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T13:56:48,798 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 4 (MapPartitionsRDD[13] at parquet at <unknown>:0), which has no missing parents
2025-04-30T13:56:48,834 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6 stored as values in memory (estimated size 215.4 KiB, free 433.7 MiB)
2025-04-30T13:56:48,837 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 77.5 KiB, free 433.7 MiB)
2025-04-30T13:56:48,839 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_6_piece0 in memory on 22a90d2ca54f:41001 (size: 77.5 KiB, free: 434.3 MiB)
2025-04-30T13:56:48,841 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 6 from broadcast at DAGScheduler.scala:1585
2025-04-30T13:56:48,843 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at parquet at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T13:56:48,844 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 4.0 with 1 tasks resource profile 0
2025-04-30T13:56:48,847 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 4.0 (TID 3) (172.18.0.5, executor 1, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T13:56:48,851 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 3
2025-04-30T13:56:48,853 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 4.0 (TID 3)
2025-04-30T13:56:48,857 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2025-04-30T13:56:48,873 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:56:48,883 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 77.5 KiB, free 434.3 MiB)
2025-04-30T13:56:48,886 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_6_piece0 in memory on 172.18.0.5:34683 (size: 77.5 KiB, free: 434.3 MiB)
2025-04-30T13:56:48,888 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 6 took 13 ms
2025-04-30T13:56:48,890 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6 stored as values in memory (estimated size 215.4 KiB, free 434.1 MiB)
2025-04-30T13:56:48,950 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:56:48,950 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:56:48,952 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:56:48,953 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:56:48,953 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:56:48,955 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:56:48,963 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:56:48,966 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:56:48,993 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat [] - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-04-30T13:56:49,051 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport [] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "date_birth",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "driver_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt_current_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "first_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "last_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "license_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "phone_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_make",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_model",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_year",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary city (STRING);
  optional binary country (STRING);
  optional binary date_birth (STRING);
  optional int64 driver_id;
  optional binary dt_current_timestamp (STRING);
  optional binary first_name (STRING);
  optional binary last_name (STRING);
  optional binary license_number (STRING);
  optional binary phone_number (STRING);
  optional binary uuid (STRING);
  optional binary vehicle_make (STRING);
  optional binary vehicle_model (STRING);
  optional binary vehicle_type (STRING);
  optional int64 vehicle_year;
}

       
2025-04-30T13:56:49,084 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.0 in stage 4.0 (TID 3)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301356483709068110252475102_0004_m_000000_3 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430135638-0001/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
2025-04-30T13:56:49,119 [task-result-getter-3] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.0 in stage 4.0 (TID 3) (172.18.0.5 executor 1): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301356483709068110252475102_0004_m_000000_3 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430135638-0001/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

2025-04-30T13:56:49,122 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.1 in stage 4.0 (TID 4) (172.18.0.3, executor 2, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T13:56:49,133 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 4
2025-04-30T13:56:49,144 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.executor.Executor [] - Running task 0.1 in stage 4.0 (TID 4)
2025-04-30T13:56:49,187 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2025-04-30T13:56:49,229 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:56:49,264 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 22a90d2ca54f/172.18.0.2:41001 after 1 ms (0 ms spent in bootstraps)
2025-04-30T13:56:49,303 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 77.5 KiB, free 434.3 MiB)
2025-04-30T13:56:49,308 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_6_piece0 in memory on 172.18.0.3:40597 (size: 77.5 KiB, free: 434.3 MiB)
2025-04-30T13:56:49,313 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 6 took 83 ms
2025-04-30T13:56:49,359 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6 stored as values in memory (estimated size 215.4 KiB, free 434.1 MiB)
2025-04-30T13:56:49,581 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:56:49,581 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:56:49,670 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:56:49,670 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:56:49,671 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:56:49,673 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:56:49,680 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:56:49,686 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:56:49,718 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat [] - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-04-30T13:56:49,781 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport [] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "date_birth",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "driver_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt_current_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "first_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "last_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "license_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "phone_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_make",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_model",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_year",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary city (STRING);
  optional binary country (STRING);
  optional binary date_birth (STRING);
  optional int64 driver_id;
  optional binary dt_current_timestamp (STRING);
  optional binary first_name (STRING);
  optional binary last_name (STRING);
  optional binary license_number (STRING);
  optional binary phone_number (STRING);
  optional binary uuid (STRING);
  optional binary vehicle_make (STRING);
  optional binary vehicle_model (STRING);
  optional binary vehicle_type (STRING);
  optional int64 vehicle_year;
}

       
2025-04-30T13:56:49,834 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.1 in stage 4.0 (TID 4)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301356483709068110252475102_0004_m_000000_4 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430135638-0001/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
2025-04-30T13:56:49,867 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.1 in stage 4.0 (TID 4) (172.18.0.3 executor 2): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301356483709068110252475102_0004_m_000000_4 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430135638-0001/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

2025-04-30T13:56:49,869 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.2 in stage 4.0 (TID 5) (172.18.0.4, executor 0, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T13:56:49,873 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 5
2025-04-30T13:56:49,874 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.executor.Executor [] - Running task 0.2 in stage 4.0 (TID 5)
2025-04-30T13:56:49,880 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T13:56:49,891 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 77.5 KiB, free 433.9 MiB)
2025-04-30T13:56:49,896 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_6_piece0 in memory on 172.18.0.4:33901 (size: 77.5 KiB, free: 434.3 MiB)
2025-04-30T13:56:49,898 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 6 took 17 ms
2025-04-30T13:56:49,901 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6 stored as values in memory (estimated size 215.4 KiB, free 433.7 MiB)
2025-04-30T13:56:49,987 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:56:49,987 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:56:49,990 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:56:49,990 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:56:49,991 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:56:49,993 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:56:50,003 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:56:50,007 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:56:50,048 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat [] - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-04-30T13:56:50,133 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport [] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "date_birth",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "driver_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt_current_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "first_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "last_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "license_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "phone_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_make",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_model",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_year",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary city (STRING);
  optional binary country (STRING);
  optional binary date_birth (STRING);
  optional int64 driver_id;
  optional binary dt_current_timestamp (STRING);
  optional binary first_name (STRING);
  optional binary last_name (STRING);
  optional binary license_number (STRING);
  optional binary phone_number (STRING);
  optional binary uuid (STRING);
  optional binary vehicle_make (STRING);
  optional binary vehicle_model (STRING);
  optional binary vehicle_type (STRING);
  optional int64 vehicle_year;
}

       
2025-04-30T13:56:50,171 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.2 in stage 4.0 (TID 5)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301356483709068110252475102_0004_m_000000_5 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430135638-0001/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
2025-04-30T13:56:50,198 [task-result-getter-1] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.2 in stage 4.0 (TID 5) (172.18.0.4 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301356483709068110252475102_0004_m_000000_5 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430135638-0001/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

2025-04-30T13:56:50,200 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.3 in stage 4.0 (TID 6) (172.18.0.4, executor 0, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T13:56:50,204 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 6
2025-04-30T13:56:50,205 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.executor.Executor [] - Running task 0.3 in stage 4.0 (TID 6)
2025-04-30T13:56:50,229 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:56:50,230 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:56:50,232 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:56:50,233 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T13:56:50,234 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T13:56:50,235 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T13:56:50,236 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:56:50,237 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T13:56:50,240 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat [] - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-04-30T13:56:50,249 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport [] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "date_birth",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "driver_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt_current_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "first_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "last_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "license_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "phone_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_make",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_model",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_year",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary city (STRING);
  optional binary country (STRING);
  optional binary date_birth (STRING);
  optional int64 driver_id;
  optional binary dt_current_timestamp (STRING);
  optional binary first_name (STRING);
  optional binary last_name (STRING);
  optional binary license_number (STRING);
  optional binary phone_number (STRING);
  optional binary uuid (STRING);
  optional binary vehicle_make (STRING);
  optional binary vehicle_model (STRING);
  optional binary vehicle_type (STRING);
  optional int64 vehicle_year;
}

       
2025-04-30T13:56:50,281 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.3 in stage 4.0 (TID 6)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301356483709068110252475102_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430135638-0001/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
2025-04-30T13:56:50,288 [task-result-getter-2] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.3 in stage 4.0 (TID 6) (172.18.0.4 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301356483709068110252475102_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430135638-0001/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

2025-04-30T13:56:50,290 [task-result-getter-2] ERROR org.apache.spark.scheduler.TaskSetManager [] - Task 0 in stage 4.0 failed 4 times; aborting job
2025-04-30T13:56:50,292 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2025-04-30T13:56:50,295 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Cancelling stage 4
2025-04-30T13:56:50,295 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 4: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 6) (172.18.0.4 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301356483709068110252475102_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430135638-0001/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

Driver stacktrace:
2025-04-30T13:56:50,297 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 4 (parquet at <unknown>:0) failed in 1.497 s due to Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 6) (172.18.0.4 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301356483709068110252475102_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430135638-0001/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

Driver stacktrace:
2025-04-30T13:56:50,301 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 3 failed: parquet at <unknown>:0, took 1.507600 s
2025-04-30T13:56:50,302 [Thread-4] ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter [] - Aborting job 5f435f9e-a86e-4a3c-ba80-948c2454c5cb.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 6) (172.18.0.4 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301356483709068110252475102_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430135638-0001/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[scala-library-2.12.18.jar:?]
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[scala-library-2.12.18.jar:?]
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at scala.Option.foreach(Option.scala:407) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461) ~[spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76) [spark-sql-api_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
	at java.lang.reflect.Method.invoke(Unknown Source) ~[?:?]
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) [py4j-0.10.9.7.jar:?]
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374) [py4j-0.10.9.7.jar:?]
	at py4j.Gateway.invoke(Gateway.java:282) [py4j-0.10.9.7.jar:?]
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) [py4j-0.10.9.7.jar:?]
	at py4j.commands.CallCommand.execute(CallCommand.java:79) [py4j-0.10.9.7.jar:?]
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182) [py4j-0.10.9.7.jar:?]
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106) [py4j-0.10.9.7.jar:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
Caused by: java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_202504301356483709068110252475102_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430135638-0001/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]
	... 1 more
2025-04-30T13:56:50,797 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Invoking stop() from shutdown hook
2025-04-30T13:56:50,798 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-30T13:56:50,806 [shutdown-hook-0] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@5f2f1763{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T13:56:50,811 [shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://22a90d2ca54f:4040
2025-04-30T13:56:50,815 [shutdown-hook-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-30T13:56:50,815 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-30T13:56:50,821 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T13:56:50,821 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T13:56:50,821 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T13:56:50,825 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250430135638-0001
2025-04-30T13:56:50,827 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250430135638-0001
2025-04-30T13:56:50,836 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430135638-0001/0
2025-04-30T13:56:50,837 [ExecutorRunner for app-20250430135638-0001/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430135638-0001/0 interrupted
2025-04-30T13:56:50,837 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430135638-0001/2
2025-04-30T13:56:50,838 [ExecutorRunner for app-20250430135638-0001/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T13:56:50,838 [ExecutorRunner for app-20250430135638-0001/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430135638-0001/2 interrupted
2025-04-30T13:56:50,839 [ExecutorRunner for app-20250430135638-0001/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T13:56:50,841 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T13:56:50,847 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T13:56:50,850 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T13:56:50,851 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430135638-0001/1
2025-04-30T13:56:50,855 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T13:56:50,856 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T13:56:50,858 [ExecutorRunner for app-20250430135638-0001/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430135638-0001/1 interrupted
2025-04-30T13:56:50,862 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T13:56:50,863 [ExecutorRunner for app-20250430135638-0001/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T13:56:50,863 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T13:56:50,865 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T13:56:50,866 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T13:56:50,874 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T13:56:50,874 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T13:56:50,877 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T13:56:50,884 [dispatcher-event-loop-1] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-30T13:56:50,902 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430135638-0001/0 finished with state KILLED exitStatus 143
2025-04-30T13:56:50,903 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-30T13:56:50,903 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430135638-0001, execId=0)
2025-04-30T13:56:50,904 [dispatcher-event-loop-1] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430135638-0001/0
2025-04-30T13:56:50,904 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430135638-0001 removed, cleanupLocalDirs = true
2025-04-30T13:56:50,905 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430135638-0001
2025-04-30T13:56:50,909 [shutdown-hook-0] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T13:56:50,911 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T13:56:50,917 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-30T13:56:50,923 [dispatcher-event-loop-10] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-30T13:56:50,928 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430135638-0001/1 finished with state KILLED exitStatus 143
2025-04-30T13:56:50,929 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-30T13:56:50,930 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430135638-0001, execId=1)
2025-04-30T13:56:50,932 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430135638-0001 removed, cleanupLocalDirs = true
2025-04-30T13:56:50,932 [dispatcher-event-loop-9] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430135638-0001/1
2025-04-30T13:56:50,932 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430135638-0001
2025-04-30T13:56:50,933 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:47552 got disassociated, removing it.
2025-04-30T13:56:50,934 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430135638-0001/2 finished with state KILLED exitStatus 143
2025-04-30T13:56:50,935 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - 22a90d2ca54f:44543 got disassociated, removing it.
2025-04-30T13:56:50,935 [dispatcher-event-loop-0] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-30T13:56:50,935 [dispatcher-event-loop-0] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430135638-0001, execId=2)
2025-04-30T13:56:50,937 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430135638-0001
2025-04-30T13:56:50,937 [dispatcher-event-loop-0] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430135638-0001 removed, cleanupLocalDirs = true
2025-04-30T13:56:50,938 [dispatcher-event-loop-8] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430135638-0001/2
2025-04-30T13:56:50,941 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-30T13:56:50,942 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T13:56:50,945 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-3b3d1379-a922-48e1-ae25-7fe1dbd2ae73
2025-04-30T13:56:50,949 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-456c2ca5-d329-46b3-ae3b-38ee64cca67a
2025-04-30T13:56:50,953 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-3b3d1379-a922-48e1-ae25-7fe1dbd2ae73/pyspark-d344cd83-a92e-4c52-86f7-2965d7808364
2025-04-30T14:02:08,991 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-30T14:02:08,995 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T14:02:08,997 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-30T14:02:09,023 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T14:02:09,025 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-30T14:02:09,026 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T14:02:09,027 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: mod-2-pr-4-data-ingestion.py
2025-04-30T14:02:09,048 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-30T14:02:09,058 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-30T14:02:09,060 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-30T14:02:09,113 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-30T14:02:09,114 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-30T14:02:09,116 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T14:02:09,118 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T14:02:09,119 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-30T14:02:09,177 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T14:02:09,440 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 44353.
2025-04-30T14:02:09,470 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-30T14:02:09,506 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-30T14:02:09,528 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-30T14:02:09,529 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-30T14:02:09,536 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-30T14:02:09,556 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-9d3e158f-075b-414b-8a83-f5d95efc450c
2025-04-30T14:02:09,572 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T14:02:09,590 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-30T14:02:09,632 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3236ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-30T14:02:09,723 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-30T14:02:09,734 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-30T14:02:09,752 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @3356ms
2025-04-30T14:02:09,785 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@3022cfda{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T14:02:09,786 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-30T14:02:09,809 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@46c08dc4{/,null,AVAILABLE,@Spark}
2025-04-30T14:02:09,925 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-30T14:02:09,972 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 26 ms (0 ms spent in bootstraps)
2025-04-30T14:02:10,058 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Registering app mod-2-pr-4-data-ingestion.py
2025-04-30T14:02:10,060 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Registered app mod-2-pr-4-data-ingestion.py with ID app-20250430140210-0002
2025-04-30T14:02:10,061 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430140210-0002 with rpId: 0
2025-04-30T14:02:10,063 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430140210-0002/0 on worker worker-20250430134208-172.18.0.4-42969
2025-04-30T14:02:10,065 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430140210-0002/1 on worker worker-20250430134208-172.18.0.5-44831
2025-04-30T14:02:10,067 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250430140210-0002/2 on worker worker-20250430134208-172.18.0.3-44603
2025-04-30T14:02:10,069 [dispatcher-event-loop-10] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250430140210-0002
2025-04-30T14:02:10,072 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430140210-0002/0 for mod-2-pr-4-data-ingestion.py
2025-04-30T14:02:10,073 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430140210-0002/0 on worker-20250430134208-172.18.0.4-42969 (172.18.0.4:42969) with 2 core(s)
2025-04-30T14:02:10,074 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430140210-0002/1 for mod-2-pr-4-data-ingestion.py
2025-04-30T14:02:10,077 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430140210-0002/0 on hostPort 172.18.0.4:42969 with 2 core(s), 1024.0 MiB RAM
2025-04-30T14:02:10,078 [ExecutorRunner for app-20250430140210-0002/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T14:02:10,077 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250430140210-0002/2 for mod-2-pr-4-data-ingestion.py
2025-04-30T14:02:10,078 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430140210-0002/1 on worker-20250430134208-172.18.0.5-44831 (172.18.0.5:44831) with 2 core(s)
2025-04-30T14:02:10,079 [ExecutorRunner for app-20250430140210-0002/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T14:02:10,080 [ExecutorRunner for app-20250430140210-0002/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T14:02:10,081 [ExecutorRunner for app-20250430140210-0002/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T14:02:10,081 [ExecutorRunner for app-20250430140210-0002/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T14:02:10,081 [ExecutorRunner for app-20250430140210-0002/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T14:02:10,082 [ExecutorRunner for app-20250430140210-0002/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T14:02:10,083 [ExecutorRunner for app-20250430140210-0002/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T14:02:10,084 [ExecutorRunner for app-20250430140210-0002/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T14:02:10,085 [ExecutorRunner for app-20250430140210-0002/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T14:02:10,085 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430140210-0002/1 on hostPort 172.18.0.5:44831 with 2 core(s), 1024.0 MiB RAM
2025-04-30T14:02:10,086 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46667.
2025-04-30T14:02:10,086 [ExecutorRunner for app-20250430140210-0002/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T14:02:10,086 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250430140210-0002/2 on worker-20250430134208-172.18.0.3-44603 (172.18.0.3:44603) with 2 core(s)
2025-04-30T14:02:10,087 [ExecutorRunner for app-20250430140210-0002/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T14:02:10,087 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 22a90d2ca54f:46667
2025-04-30T14:02:10,088 [ExecutorRunner for app-20250430140210-0002/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T14:02:10,088 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250430140210-0002/2 on hostPort 172.18.0.3:44603 with 2 core(s), 1024.0 MiB RAM
2025-04-30T14:02:10,088 [ExecutorRunner for app-20250430140210-0002/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T14:02:10,089 [ExecutorRunner for app-20250430140210-0002/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T14:02:10,090 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T14:02:10,101 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, 22a90d2ca54f, 46667, None)
2025-04-30T14:02:10,105 [ExecutorRunner for app-20250430140210-0002/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=44353" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22a90d2ca54f:44353" "--executor-id" "0" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250430140210-0002" "--worker-url" "spark://Worker@172.18.0.4:42969" "--resourceProfileId" "0"
2025-04-30T14:02:10,107 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 22a90d2ca54f:46667 with 434.4 MiB RAM, BlockManagerId(driver, 22a90d2ca54f, 46667, None)
2025-04-30T14:02:10,110 [ExecutorRunner for app-20250430140210-0002/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=44353" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22a90d2ca54f:44353" "--executor-id" "1" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250430140210-0002" "--worker-url" "spark://Worker@172.18.0.5:44831" "--resourceProfileId" "0"
2025-04-30T14:02:10,111 [ExecutorRunner for app-20250430140210-0002/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=44353" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22a90d2ca54f:44353" "--executor-id" "2" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250430140210-0002" "--worker-url" "spark://Worker@172.18.0.3:44603" "--resourceProfileId" "0"
2025-04-30T14:02:10,120 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, 22a90d2ca54f, 46667, None)
2025-04-30T14:02:10,121 [dispatcher-event-loop-11] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430140210-0002 with rpId: 0
2025-04-30T14:02:10,123 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, 22a90d2ca54f, 46667, None)
2025-04-30T14:02:10,125 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430140210-0002 with rpId: 0
2025-04-30T14:02:10,128 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250430140210-0002 with rpId: 0
2025-04-30T14:02:10,147 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430140210-0002/0 is now RUNNING
2025-04-30T14:02:10,150 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430140210-0002/1 is now RUNNING
2025-04-30T14:02:10,156 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250430140210-0002/2 is now RUNNING
2025-04-30T14:02:10,529 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250430140210-0002.inprogress
2025-04-30T14:02:10,856 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@46c08dc4{/,null,STOPPED,@Spark}
2025-04-30T14:02:10,859 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5e0fd8eb{/jobs,null,AVAILABLE,@Spark}
2025-04-30T14:02:10,861 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@79c880ff{/jobs/json,null,AVAILABLE,@Spark}
2025-04-30T14:02:10,865 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6c6f1890{/jobs/job,null,AVAILABLE,@Spark}
2025-04-30T14:02:10,868 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@e6a4ecf{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-30T14:02:10,870 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@26dd5b67{/stages,null,AVAILABLE,@Spark}
2025-04-30T14:02:10,873 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@72a3c55b{/stages/json,null,AVAILABLE,@Spark}
2025-04-30T14:02:10,876 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5d110cd0{/stages/stage,null,AVAILABLE,@Spark}
2025-04-30T14:02:10,881 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1d388398{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-30T14:02:10,888 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@43d5ad60{/stages/pool,null,AVAILABLE,@Spark}
2025-04-30T14:02:10,891 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@21053f9b{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-30T14:02:10,894 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6329ad7f{/storage,null,AVAILABLE,@Spark}
2025-04-30T14:02:10,900 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@51a0492c{/storage/json,null,AVAILABLE,@Spark}
2025-04-30T14:02:10,905 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3f0d90c4{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-30T14:02:10,909 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3f56f44b{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-30T14:02:10,912 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@71dc16c2{/environment,null,AVAILABLE,@Spark}
2025-04-30T14:02:10,920 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@748fcc72{/environment/json,null,AVAILABLE,@Spark}
2025-04-30T14:02:10,923 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@66bcf2a0{/executors,null,AVAILABLE,@Spark}
2025-04-30T14:02:10,928 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@c9136cf{/executors/json,null,AVAILABLE,@Spark}
2025-04-30T14:02:10,933 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4631d710{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-30T14:02:10,936 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@796f119f{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-30T14:02:10,941 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3e1d6e13{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-30T14:02:10,947 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2899d8d1{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-30T14:02:10,966 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6e1a3f9a{/static,null,AVAILABLE,@Spark}
2025-04-30T14:02:10,969 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2fd70199{/,null,AVAILABLE,@Spark}
2025-04-30T14:02:10,976 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7ec4f736{/api,null,AVAILABLE,@Spark}
2025-04-30T14:02:10,982 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@777d6353{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-30T14:02:10,991 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7c5ecc51{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-30T14:02:11,005 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5e64aa88{/metrics/json,null,AVAILABLE,@Spark}
2025-04-30T14:02:11,008 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-30T14:02:11,424 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-04-30T14:02:11,429 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
2025-04-30T14:02:11,456 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@386cb839{/SQL,null,AVAILABLE,@Spark}
2025-04-30T14:02:11,460 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4efe9503{/SQL/json,null,AVAILABLE,@Spark}
2025-04-30T14:02:11,463 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7afe98f1{/SQL/execution,null,AVAILABLE,@Spark}
2025-04-30T14:02:11,467 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@51a64e68{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-04-30T14:02:11,476 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2b8bf0c3{/static/sql,null,AVAILABLE,@Spark}
2025-04-30T14:02:12,408 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 334@4cfd2bfc0474
2025-04-30T14:02:12,426 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T14:02:12,428 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T14:02:12,429 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T14:02:12,432 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 337@30cc727c40d4
2025-04-30T14:02:12,448 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T14:02:12,450 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T14:02:12,451 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T14:02:12,507 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 344@e89d73bffc2c
2025-04-30T14:02:12,526 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-30T14:02:12,528 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-30T14:02:12,529 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-30T14:02:13,262 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T14:02:13,301 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T14:02:13,334 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30T14:02:13,522 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T14:02:13,525 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T14:02:13,528 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T14:02:13,533 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T14:02:13,536 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T14:02:13,556 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T14:02:13,558 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T14:02:13,560 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T14:02:13,563 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T14:02:13,565 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T14:02:13,576 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T14:02:13,581 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T14:02:13,584 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T14:02:13,586 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T14:02:13,593 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T14:02:13,702 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 108 ms to list leaf files for 2 paths.
2025-04-30T14:02:14,057 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 22a90d2ca54f/172.18.0.2:44353 after 107 ms (0 ms spent in bootstraps)
2025-04-30T14:02:14,099 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 22a90d2ca54f/172.18.0.2:44353 after 125 ms (0 ms spent in bootstraps)
2025-04-30T14:02:14,120 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 22a90d2ca54f/172.18.0.2:44353 after 106 ms (0 ms spent in bootstraps)
2025-04-30T14:02:14,268 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T14:02:14,270 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T14:02:14,271 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T14:02:14,273 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T14:02:14,274 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T14:02:14,304 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T14:02:14,306 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T14:02:14,308 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T14:02:14,310 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T14:02:14,311 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T14:02:14,325 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-30T14:02:14,327 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-30T14:02:14,328 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-30T14:02:14,329 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-30T14:02:14,329 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-30T14:02:14,395 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 22a90d2ca54f/172.18.0.2:44353 after 9 ms (0 ms spent in bootstraps)
2025-04-30T14:02:14,433 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 22a90d2ca54f/172.18.0.2:44353 after 4 ms (0 ms spent in bootstraps)
2025-04-30T14:02:14,436 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 22a90d2ca54f/172.18.0.2:44353 after 4 ms (0 ms spent in bootstraps)
2025-04-30T14:02:14,565 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-214874f6-0358-491a-84ca-31bc1eb74c81/executor-4b3aae66-e56b-4343-99f4-ab7dd5d7f0f9/blockmgr-f20744c7-af32-4b1b-866c-da0309feb136
2025-04-30T14:02:14,611 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-1f07eb6b-58dc-466c-8985-cf87c0363f09/executor-d26c3cd6-4737-4079-91b6-3286778a6b96/blockmgr-93984039-6e86-4a27-b5ed-91a5432ae0cd
2025-04-30T14:02:14,619 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-c5b5d3c3-9fb6-49f9-8d71-d21534622fcd/executor-cf334b85-0ea8-443b-9c70-cf172de93aee/blockmgr-ef123843-41b2-4741-9933-b717f19887b4
2025-04-30T14:02:14,627 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T14:02:14,669 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T14:02:14,675 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-30T14:02:14,939 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@22a90d2ca54f:44353
2025-04-30T14:02:14,940 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.3:44603
2025-04-30T14:02:14,956 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.3:44603 after 3 ms (0 ms spent in bootstraps)
2025-04-30T14:02:14,958 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.3:44603
2025-04-30T14:02:14,965 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T14:02:14,969 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T14:02:14,970 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T14:02:14,977 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.4:42969
2025-04-30T14:02:14,977 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@22a90d2ca54f:44353
2025-04-30T14:02:14,981 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@22a90d2ca54f:44353
2025-04-30T14:02:14,982 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.5:44831
2025-04-30T14:02:14,991 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.4:42969 after 6 ms (0 ms spent in bootstraps)
2025-04-30T14:02:14,998 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T14:02:15,001 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T14:02:15,001 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T14:02:15,003 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T14:02:15,005 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.5:44831 after 13 ms (0 ms spent in bootstraps)
2025-04-30T14:02:15,007 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-30T14:02:15,012 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.5:44831
2025-04-30T14:02:15,022 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:40738) with ID 2,  ResourceProfileId 0
2025-04-30T14:02:15,014 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-30T14:02:15,023 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.4:42969
2025-04-30T14:02:15,048 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:42418) with ID 0,  ResourceProfileId 0
2025-04-30T14:02:15,055 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T14:02:15,059 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T14:02:15,069 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 2 on host 172.18.0.3
2025-04-30T14:02:15,070 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T14:02:15,071 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 0 on host 172.18.0.4
2025-04-30T14:02:15,072 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T14:02:15,072 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T14:02:15,074 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T14:02:15,084 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:46524) with ID 1,  ResourceProfileId 0
2025-04-30T14:02:15,098 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-30T14:02:15,108 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 1 on host 172.18.0.5
2025-04-30T14:02:15,111 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
2025-04-30T14:02:15,114 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-30T14:02:15,185 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34421.
2025-04-30T14:02:15,186 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43405.
2025-04-30T14:02:15,186 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.3:34421
2025-04-30T14:02:15,187 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.4:43405
2025-04-30T14:02:15,191 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T14:02:15,191 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T14:02:15,202 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(2, 172.18.0.3, 34421, None)
2025-04-30T14:02:15,203 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(0, 172.18.0.4, 43405, None)
2025-04-30T14:02:15,214 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41567.
2025-04-30T14:02:15,216 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.5:41567
2025-04-30T14:02:15,217 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.3:34421 with 434.4 MiB RAM, BlockManagerId(2, 172.18.0.3, 34421, None)
2025-04-30T14:02:15,221 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-30T14:02:15,223 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.4:43405 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.4, 43405, None)
2025-04-30T14:02:15,227 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 30 ms to list leaf files for 1 paths.
2025-04-30T14:02:15,233 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(0, 172.18.0.4, 43405, None)
2025-04-30T14:02:15,234 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(2, 172.18.0.3, 34421, None)
2025-04-30T14:02:15,236 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(0, 172.18.0.4, 43405, None)
2025-04-30T14:02:15,237 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(2, 172.18.0.3, 34421, None)
2025-04-30T14:02:15,239 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(1, 172.18.0.5, 41567, None)
2025-04-30T14:02:15,255 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T14:02:15,256 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T14:02:15,258 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2f9a6030 for default.
2025-04-30T14:02:15,261 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.5:41567 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.5, 41567, None)
2025-04-30T14:02:15,272 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@51b1068 for default.
2025-04-30T14:02:15,272 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(1, 172.18.0.5, 41567, None)
2025-04-30T14:02:15,275 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(1, 172.18.0.5, 41567, None)
2025-04-30T14:02:15,289 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-30T14:02:15,290 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1282537c for default.
2025-04-30T14:02:15,322 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 21 ms to list leaf files for 1 paths.
2025-04-30T14:02:16,543 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T14:02:16,545 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T14:02:16,698 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 200.3 KiB, free 434.2 MiB)
2025-04-30T14:02:16,751 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
2025-04-30T14:02:16,755 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on 22a90d2ca54f:46667 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T14:02:16,761 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 0 from json at <unknown>:0
2025-04-30T14:02:16,774 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T14:02:16,890 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: json at <unknown>:0
2025-04-30T14:02:16,905 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 0 (json at <unknown>:0) with 1 output partitions
2025-04-30T14:02:16,906 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 0 (json at <unknown>:0)
2025-04-30T14:02:16,907 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T14:02:16,908 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T14:02:16,913 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0), which has no missing parents
2025-04-30T14:02:16,987 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.5 KiB, free 434.2 MiB)
2025-04-30T14:02:16,998 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
2025-04-30T14:02:16,999 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on 22a90d2ca54f:46667 (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T14:02:17,002 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 1 from broadcast at DAGScheduler.scala:1585
2025-04-30T14:02:17,020 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T14:02:17,022 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 0.0 with 1 tasks resource profile 0
2025-04-30T14:02:17,051 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.4, executor 0, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T14:02:17,075 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 0
2025-04-30T14:02:17,085 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 0.0 (TID 0)
2025-04-30T14:02:17,174 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T14:02:17,223 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 22a90d2ca54f/172.18.0.2:46667 after 3 ms (0 ms spent in bootstraps)
2025-04-30T14:02:17,276 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.4 MiB)
2025-04-30T14:02:17,282 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on 172.18.0.4:43405 (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T14:02:17,288 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 1 took 113 ms
2025-04-30T14:02:17,363 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.5 KiB, free 434.4 MiB)
2025-04-30T14:02:17,704 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl, range: 0-32688, partition values: [empty row]
2025-04-30T14:02:18,033 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 191.12678 ms
2025-04-30T14:02:18,036 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T14:02:18,044 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.3 MiB)
2025-04-30T14:02:18,047 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on 172.18.0.4:43405 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T14:02:18,050 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 0 took 13 ms
2025-04-30T14:02:18,134 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T14:02:18,284 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 0.0 (TID 0). 2450 bytes result sent to driver
2025-04-30T14:02:18,297 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 0.0 (TID 0) in 1257 ms on 172.18.0.4 (executor 0) (1/1)
2025-04-30T14:02:18,300 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-04-30T14:02:18,305 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 0 (json at <unknown>:0) finished in 1.378 s
2025-04-30T14:02:18,308 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T14:02:18,309 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 0: Stage finished
2025-04-30T14:02:18,313 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 finished: json at <unknown>:0, took 1.422913 s
2025-04-30T14:02:18,443 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T14:02:18,444 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T14:02:18,831 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 202.720621 ms
2025-04-30T14:02:18,838 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 200.2 KiB, free 434.0 MiB)
2025-04-30T14:02:18,858 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
2025-04-30T14:02:18,866 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on 22a90d2ca54f:46667 (size: 34.5 KiB, free: 434.3 MiB)
2025-04-30T14:02:18,871 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 2 from count at <unknown>:0
2025-04-30T14:02:18,877 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T14:02:18,878 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on 22a90d2ca54f:46667 in memory (size: 7.7 KiB, free: 434.3 MiB)
2025-04-30T14:02:18,891 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on 172.18.0.4:43405 in memory (size: 7.7 KiB, free: 434.4 MiB)
2025-04-30T14:02:18,939 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Registering RDD 7 (count at <unknown>:0) as input to shuffle 0
2025-04-30T14:02:18,944 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got map stage job 1 (count at <unknown>:0) with 1 output partitions
2025-04-30T14:02:18,946 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ShuffleMapStage 1 (count at <unknown>:0)
2025-04-30T14:02:18,947 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T14:02:18,949 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T14:02:18,952 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0), which has no missing parents
2025-04-30T14:02:18,974 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 433.9 MiB)
2025-04-30T14:02:18,988 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 433.9 MiB)
2025-04-30T14:02:18,989 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on 22a90d2ca54f:46667 (size: 8.3 KiB, free: 434.3 MiB)
2025-04-30T14:02:18,991 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 3 from broadcast at DAGScheduler.scala:1585
2025-04-30T14:02:18,995 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T14:02:18,996 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 1.0 with 1 tasks resource profile 0
2025-04-30T14:02:19,002 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.5, executor 1, partition 0, PROCESS_LOCAL, 9638 bytes) 
2025-04-30T14:02:19,015 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 1
2025-04-30T14:02:19,029 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 1.0 (TID 1)
2025-04-30T14:02:19,133 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T14:02:19,167 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 22a90d2ca54f/172.18.0.2:46667 after 1 ms (0 ms spent in bootstraps)
2025-04-30T14:02:19,199 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 434.4 MiB)
2025-04-30T14:02:19,212 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on 172.18.0.5:41567 (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T14:02:19,217 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 3 took 83 ms
2025-04-30T14:02:19,275 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 434.4 MiB)
2025-04-30T14:02:19,810 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 181.032684 ms
2025-04-30T14:02:19,824 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl, range: 0-32688, partition values: [empty row]
2025-04-30T14:02:19,857 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 10.932773 ms
2025-04-30T14:02:19,874 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T14:02:19,883 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.3 MiB)
2025-04-30T14:02:19,886 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on 172.18.0.5:41567 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T14:02:19,889 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 2 took 13 ms
2025-04-30T14:02:19,959 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-30T14:02:20,146 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 1.0 (TID 1). 2007 bytes result sent to driver
2025-04-30T14:02:20,155 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 1.0 (TID 1) in 1155 ms on 172.18.0.5 (executor 1) (1/1)
2025-04-30T14:02:20,156 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-04-30T14:02:20,159 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ShuffleMapStage 1 (count at <unknown>:0) finished in 1.202 s
2025-04-30T14:02:20,160 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - looking for newly runnable stages
2025-04-30T14:02:20,162 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - running: Set()
2025-04-30T14:02:20,162 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - waiting: Set()
2025-04-30T14:02:20,164 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - failed: Set()
2025-04-30T14:02:20,214 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 16.679774 ms
2025-04-30T14:02:20,244 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: count at <unknown>:0
2025-04-30T14:02:20,248 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 2 (count at <unknown>:0) with 1 output partitions
2025-04-30T14:02:20,248 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 3 (count at <unknown>:0)
2025-04-30T14:02:20,249 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List(ShuffleMapStage 2)
2025-04-30T14:02:20,250 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T14:02:20,252 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0), which has no missing parents
2025-04-30T14:02:20,263 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 433.9 MiB)
2025-04-30T14:02:20,272 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 433.9 MiB)
2025-04-30T14:02:20,275 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_4_piece0 in memory on 22a90d2ca54f:46667 (size: 6.0 KiB, free: 434.3 MiB)
2025-04-30T14:02:20,278 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_3_piece0 on 22a90d2ca54f:46667 in memory (size: 8.3 KiB, free: 434.3 MiB)
2025-04-30T14:02:20,278 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 4 from broadcast at DAGScheduler.scala:1585
2025-04-30T14:02:20,281 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T14:02:20,283 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 3.0 with 1 tasks resource profile 0
2025-04-30T14:02:20,291 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_3_piece0 on 172.18.0.5:41567 in memory (size: 8.3 KiB, free: 434.4 MiB)
2025-04-30T14:02:20,292 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 3.0 (TID 2) (172.18.0.5, executor 1, partition 0, NODE_LOCAL, 9003 bytes) 
2025-04-30T14:02:20,299 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 2
2025-04-30T14:02:20,301 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 3.0 (TID 2)
2025-04-30T14:02:20,312 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2025-04-30T14:02:20,329 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T14:02:20,338 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.0 MiB)
2025-04-30T14:02:20,340 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_4_piece0 in memory on 172.18.0.5:41567 (size: 6.0 KiB, free: 434.4 MiB)
2025-04-30T14:02:20,343 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 4 took 13 ms
2025-04-30T14:02:20,344 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 434.0 MiB)
2025-04-30T14:02:20,398 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Don't have map outputs for shuffle 0, fetching them
2025-04-30T14:02:20,401 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@22a90d2ca54f:44353)
2025-04-30T14:02:20,406 [dispatcher-event-loop-7] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - Asked to send map output locations for shuffle 0 to 172.18.0.5:46524
2025-04-30T14:02:20,457 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_0_piece0 on 22a90d2ca54f:46667 in memory (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T14:02:20,467 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_0_piece0 on 172.18.0.4:43405 in memory (size: 34.5 KiB, free: 434.4 MiB)
2025-04-30T14:02:20,472 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Got the map output locations
2025-04-30T14:02:20,521 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-04-30T14:02:20,524 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Started 0 remote fetches in 28 ms
2025-04-30T14:02:20,555 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 21.18491 ms
2025-04-30T14:02:20,574 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 3.0 (TID 2). 4081 bytes result sent to driver
2025-04-30T14:02:20,582 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 3.0 (TID 2) in 291 ms on 172.18.0.5 (executor 1) (1/1)
2025-04-30T14:02:20,582 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2025-04-30T14:02:20,586 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 3 (count at <unknown>:0) finished in 0.324 s
2025-04-30T14:02:20,587 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-30T14:02:20,588 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 3: Stage finished
2025-04-30T14:02:20,589 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 2 finished: count at <unknown>:0, took 0.343623 s
2025-04-30T14:02:20,726 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-30T14:02:20,727 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-30T14:02:20,804 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils [] - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T14:02:20,820 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T14:02:20,821 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T14:02:20,822 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T14:02:20,823 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T14:02:20,823 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T14:02:20,825 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T14:02:20,891 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5 stored as values in memory (estimated size 200.2 KiB, free 434.0 MiB)
2025-04-30T14:02:20,904 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_4_piece0 on 22a90d2ca54f:46667 in memory (size: 6.0 KiB, free: 434.4 MiB)
2025-04-30T14:02:20,914 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
2025-04-30T14:02:20,917 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_5_piece0 in memory on 22a90d2ca54f:46667 (size: 34.5 KiB, free: 434.3 MiB)
2025-04-30T14:02:20,919 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_4_piece0 on 172.18.0.5:41567 in memory (size: 6.0 KiB, free: 434.4 MiB)
2025-04-30T14:02:20,919 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 5 from parquet at <unknown>:0
2025-04-30T14:02:20,923 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-30T14:02:20,938 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: parquet at <unknown>:0
2025-04-30T14:02:20,940 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 3 (parquet at <unknown>:0) with 1 output partitions
2025-04-30T14:02:20,945 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 4 (parquet at <unknown>:0)
2025-04-30T14:02:20,946 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-30T14:02:20,948 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-30T14:02:20,949 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 4 (MapPartitionsRDD[13] at parquet at <unknown>:0), which has no missing parents
2025-04-30T14:02:20,987 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6 stored as values in memory (estimated size 215.4 KiB, free 433.7 MiB)
2025-04-30T14:02:20,995 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 77.5 KiB, free 433.7 MiB)
2025-04-30T14:02:20,999 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_6_piece0 in memory on 22a90d2ca54f:46667 (size: 77.5 KiB, free: 434.3 MiB)
2025-04-30T14:02:21,001 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 6 from broadcast at DAGScheduler.scala:1585
2025-04-30T14:02:21,004 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at parquet at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-30T14:02:21,004 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 4.0 with 1 tasks resource profile 0
2025-04-30T14:02:21,007 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 4.0 (TID 3) (172.18.0.3, executor 2, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T14:02:21,022 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 3
2025-04-30T14:02:21,032 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 4.0 (TID 3)
2025-04-30T14:02:21,078 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2025-04-30T14:02:21,122 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T14:02:21,156 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 22a90d2ca54f/172.18.0.2:46667 after 2 ms (0 ms spent in bootstraps)
2025-04-30T14:02:21,193 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 77.5 KiB, free 434.3 MiB)
2025-04-30T14:02:21,198 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_6_piece0 in memory on 172.18.0.3:34421 (size: 77.5 KiB, free: 434.3 MiB)
2025-04-30T14:02:21,203 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 6 took 81 ms
2025-04-30T14:02:21,254 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6 stored as values in memory (estimated size 215.4 KiB, free 434.1 MiB)
2025-04-30T14:02:21,518 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T14:02:21,519 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T14:02:21,636 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T14:02:21,637 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T14:02:21,637 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T14:02:21,639 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T14:02:21,648 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T14:02:21,655 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T14:02:21,693 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat [] - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-04-30T14:02:21,790 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport [] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "date_birth",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "driver_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt_current_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "first_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "last_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "license_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "phone_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_make",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_model",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_year",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary city (STRING);
  optional binary country (STRING);
  optional binary date_birth (STRING);
  optional int64 driver_id;
  optional binary dt_current_timestamp (STRING);
  optional binary first_name (STRING);
  optional binary last_name (STRING);
  optional binary license_number (STRING);
  optional binary phone_number (STRING);
  optional binary uuid (STRING);
  optional binary vehicle_make (STRING);
  optional binary vehicle_model (STRING);
  optional binary vehicle_type (STRING);
  optional int64 vehicle_year;
}

       
2025-04-30T14:02:21,835 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.0 in stage 4.0 (TID 3)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_20250430140220845281815475394548_0004_m_000000_3 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430140210-0002/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
2025-04-30T14:02:21,887 [task-result-getter-3] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.0 in stage 4.0 (TID 3) (172.18.0.3 executor 2): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_20250430140220845281815475394548_0004_m_000000_3 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430140210-0002/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

2025-04-30T14:02:21,889 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.1 in stage 4.0 (TID 4) (172.18.0.5, executor 1, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T14:02:21,893 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 4
2025-04-30T14:02:21,895 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.executor.Executor [] - Running task 0.1 in stage 4.0 (TID 4)
2025-04-30T14:02:21,899 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
2025-04-30T14:02:21,915 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 77.5 KiB, free 433.9 MiB)
2025-04-30T14:02:21,919 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_6_piece0 in memory on 172.18.0.5:41567 (size: 77.5 KiB, free: 434.3 MiB)
2025-04-30T14:02:21,921 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 6 took 21 ms
2025-04-30T14:02:21,923 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6 stored as values in memory (estimated size 215.4 KiB, free 433.7 MiB)
2025-04-30T14:02:21,991 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T14:02:21,992 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T14:02:21,994 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T14:02:21,995 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T14:02:21,995 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T14:02:21,996 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T14:02:22,008 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T14:02:22,011 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T14:02:22,039 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat [] - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-04-30T14:02:22,108 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport [] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "date_birth",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "driver_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt_current_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "first_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "last_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "license_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "phone_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_make",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_model",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_year",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary city (STRING);
  optional binary country (STRING);
  optional binary date_birth (STRING);
  optional int64 driver_id;
  optional binary dt_current_timestamp (STRING);
  optional binary first_name (STRING);
  optional binary last_name (STRING);
  optional binary license_number (STRING);
  optional binary phone_number (STRING);
  optional binary uuid (STRING);
  optional binary vehicle_make (STRING);
  optional binary vehicle_model (STRING);
  optional binary vehicle_type (STRING);
  optional int64 vehicle_year;
}

       
2025-04-30T14:02:22,138 [Executor task launch worker for task 0.1 in stage 4.0 (TID 4)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.1 in stage 4.0 (TID 4)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_20250430140220845281815475394548_0004_m_000000_4 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430140210-0002/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
2025-04-30T14:02:22,164 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.1 in stage 4.0 (TID 4) (172.18.0.5 executor 1): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_20250430140220845281815475394548_0004_m_000000_4 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430140210-0002/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

2025-04-30T14:02:22,166 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.2 in stage 4.0 (TID 5) (172.18.0.3, executor 2, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T14:02:22,171 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 5
2025-04-30T14:02:22,172 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.executor.Executor [] - Running task 0.2 in stage 4.0 (TID 5)
2025-04-30T14:02:22,209 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T14:02:22,209 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T14:02:22,211 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T14:02:22,211 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T14:02:22,212 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T14:02:22,213 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T14:02:22,214 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T14:02:22,216 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T14:02:22,220 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat [] - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-04-30T14:02:22,226 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport [] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "date_birth",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "driver_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt_current_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "first_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "last_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "license_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "phone_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_make",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_model",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_year",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary city (STRING);
  optional binary country (STRING);
  optional binary date_birth (STRING);
  optional int64 driver_id;
  optional binary dt_current_timestamp (STRING);
  optional binary first_name (STRING);
  optional binary last_name (STRING);
  optional binary license_number (STRING);
  optional binary phone_number (STRING);
  optional binary uuid (STRING);
  optional binary vehicle_make (STRING);
  optional binary vehicle_model (STRING);
  optional binary vehicle_type (STRING);
  optional int64 vehicle_year;
}

       
2025-04-30T14:02:22,254 [Executor task launch worker for task 0.2 in stage 4.0 (TID 5)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.2 in stage 4.0 (TID 5)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_20250430140220845281815475394548_0004_m_000000_5 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430140210-0002/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
2025-04-30T14:02:22,264 [task-result-getter-1] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.2 in stage 4.0 (TID 5) (172.18.0.3 executor 2): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_20250430140220845281815475394548_0004_m_000000_5 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430140210-0002/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

2025-04-30T14:02:22,266 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.3 in stage 4.0 (TID 6) (172.18.0.3, executor 2, partition 0, PROCESS_LOCAL, 9649 bytes) 
2025-04-30T14:02:22,270 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 6
2025-04-30T14:02:22,271 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.executor.Executor [] - Running task 0.3 in stage 4.0 (TID 6)
2025-04-30T14:02:22,299 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T14:02:22,300 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T14:02:22,302 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T14:02:22,303 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2025-04-30T14:02:22,303 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-30T14:02:22,304 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2025-04-30T14:02:22,305 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T14:02:22,306 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.parquet.hadoop.codec.CodecConfig [] - Compression: SNAPPY
2025-04-30T14:02:22,309 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat [] - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-04-30T14:02:22,316 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport [] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "date_birth",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "driver_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt_current_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "first_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "last_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "license_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "phone_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_make",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_model",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "vehicle_year",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary city (STRING);
  optional binary country (STRING);
  optional binary date_birth (STRING);
  optional int64 driver_id;
  optional binary dt_current_timestamp (STRING);
  optional binary first_name (STRING);
  optional binary last_name (STRING);
  optional binary license_number (STRING);
  optional binary phone_number (STRING);
  optional binary uuid (STRING);
  optional binary vehicle_make (STRING);
  optional binary vehicle_model (STRING);
  optional binary vehicle_type (STRING);
  optional int64 vehicle_year;
}

       
2025-04-30T14:02:22,344 [Executor task launch worker for task 0.3 in stage 4.0 (TID 6)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.3 in stage 4.0 (TID 6)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_20250430140220845281815475394548_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430140210-0002/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
2025-04-30T14:02:22,352 [task-result-getter-2] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.3 in stage 4.0 (TID 6) (172.18.0.3 executor 2): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_20250430140220845281815475394548_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430140210-0002/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

2025-04-30T14:02:22,353 [task-result-getter-2] ERROR org.apache.spark.scheduler.TaskSetManager [] - Task 0 in stage 4.0 failed 4 times; aborting job
2025-04-30T14:02:22,355 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2025-04-30T14:02:22,357 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Cancelling stage 4
2025-04-30T14:02:22,358 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 4: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 6) (172.18.0.3 executor 2): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_20250430140220845281815475394548_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430140210-0002/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

Driver stacktrace:
2025-04-30T14:02:22,360 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 4 (parquet at <unknown>:0) failed in 1.406 s due to Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 6) (172.18.0.3 executor 2): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_20250430140220845281815475394548_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430140210-0002/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

Driver stacktrace:
2025-04-30T14:02:22,364 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 3 failed: parquet at <unknown>:0, took 1.425673 s
2025-04-30T14:02:22,366 [Thread-4] ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter [] - Aborting job d2472995-e11f-4b31-b35e-bdbf1952d601.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 6) (172.18.0.3 executor 2): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_20250430140220845281815475394548_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430140210-0002/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[scala-library-2.12.18.jar:?]
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[scala-library-2.12.18.jar:?]
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at scala.Option.foreach(Option.scala:407) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461) ~[spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76) [spark-sql-api_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437) [spark-catalyst_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802) [spark-sql_2.12-3.5.5.jar:3.5.5]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
	at java.lang.reflect.Method.invoke(Unknown Source) ~[?:?]
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) [py4j-0.10.9.7.jar:?]
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374) [py4j-0.10.9.7.jar:?]
	at py4j.Gateway.invoke(Gateway.java:282) [py4j-0.10.9.7.jar:?]
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) [py4j-0.10.9.7.jar:?]
	at py4j.commands.CallCommand.execute(CallCommand.java:79) [py4j-0.10.9.7.jar:?]
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182) [py4j-0.10.9.7.jar:?]
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106) [py4j-0.10.9.7.jar:?]
	at java.lang.Thread.run(Unknown Source) [?:?]
Caused by: java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/storage/processed/drivers/_temporary/0/_temporary/attempt_20250430140220845281815475394548_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250430140210-0002/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[parquet-hadoop-1.13.1.jar:1.13.1]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) ~[spark-core_2.12-3.5.5.jar:3.5.5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]
	... 1 more
2025-04-30T14:02:22,809 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Invoking stop() from shutdown hook
2025-04-30T14:02:22,809 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-30T14:02:22,818 [shutdown-hook-0] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@3022cfda{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-30T14:02:22,821 [shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://22a90d2ca54f:4040
2025-04-30T14:02:22,825 [shutdown-hook-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-30T14:02:22,826 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-30T14:02:22,833 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T14:02:22,833 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T14:02:22,834 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-30T14:02:22,838 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250430140210-0002
2025-04-30T14:02:22,839 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250430140210-0002
2025-04-30T14:02:22,842 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430140210-0002/1
2025-04-30T14:02:22,843 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430140210-0002/0
2025-04-30T14:02:22,844 [ExecutorRunner for app-20250430140210-0002/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430140210-0002/1 interrupted
2025-04-30T14:02:22,844 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250430140210-0002/2
2025-04-30T14:02:22,845 [ExecutorRunner for app-20250430140210-0002/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T14:02:22,845 [ExecutorRunner for app-20250430140210-0002/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430140210-0002/0 interrupted
2025-04-30T14:02:22,845 [ExecutorRunner for app-20250430140210-0002/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250430140210-0002/2 interrupted
2025-04-30T14:02:22,846 [ExecutorRunner for app-20250430140210-0002/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T14:02:22,846 [ExecutorRunner for app-20250430140210-0002/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-30T14:02:22,848 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T14:02:22,850 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T14:02:22,852 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-30T14:02:22,854 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T14:02:22,862 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T14:02:22,867 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T14:02:22,869 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T14:02:22,874 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T14:02:22,886 [dispatcher-event-loop-1] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-30T14:02:22,886 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T14:02:22,887 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T14:02:22,887 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T14:02:22,895 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T14:02:22,912 [shutdown-hook-0] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-30T14:02:22,913 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-30T14:02:22,914 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430140210-0002/1 finished with state KILLED exitStatus 143
2025-04-30T14:02:22,915 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-30T14:02:22,915 [dispatcher-event-loop-9] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430140210-0002/1
2025-04-30T14:02:22,918 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430140210-0002, execId=1)
2025-04-30T14:02:22,920 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430140210-0002
2025-04-30T14:02:22,920 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430140210-0002 removed, cleanupLocalDirs = true
2025-04-30T14:02:22,922 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-30T14:02:22,929 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-30T14:02:22,933 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:35302 got disassociated, removing it.
2025-04-30T14:02:22,935 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - 22a90d2ca54f:44353 got disassociated, removing it.
2025-04-30T14:02:22,942 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-30T14:02:22,943 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-30T14:02:22,946 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-d652ecc5-4c8d-48b2-a43e-949d65790a24/pyspark-1a48c584-2800-45bb-b344-1914df9513a3
2025-04-30T14:02:22,947 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430140210-0002/0 finished with state KILLED exitStatus 143
2025-04-30T14:02:22,948 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-30T14:02:22,949 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430140210-0002, execId=0)
2025-04-30T14:02:22,951 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430140210-0002
2025-04-30T14:02:22,951 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430140210-0002 removed, cleanupLocalDirs = true
2025-04-30T14:02:22,950 [dispatcher-event-loop-3] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430140210-0002/0
2025-04-30T14:02:22,954 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-d652ecc5-4c8d-48b2-a43e-949d65790a24
2025-04-30T14:02:22,961 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-d40c9258-890a-4c70-9f0e-bc485fc68bde
2025-04-30T14:02:22,962 [dispatcher-event-loop-10] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250430140210-0002/2 finished with state KILLED exitStatus 143
2025-04-30T14:02:22,963 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-30T14:02:22,964 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250430140210-0002, execId=2)
2025-04-30T14:02:22,964 [dispatcher-event-loop-0] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250430140210-0002/2
2025-04-30T14:02:22,965 [dispatcher-event-loop-10] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250430140210-0002 removed, cleanupLocalDirs = true
2025-04-30T14:02:22,965 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250430140210-0002
