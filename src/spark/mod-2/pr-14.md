# Spark SQL Foundations - Part 1: Data Ingestion

## Introduction

Spark SQL provides a SQL interface for processing data in Spark, allowing you to leverage your existing SQL knowledge for distributed processing. In this lesson, we'll explore how to ingest data into Spark SQL, create tables and views, and work with the catalog.

## Setting Up Your Environment

Let's start by setting up our Spark session with Spark SQL support:

```python
from pyspark.sql import SparkSession

# Create a Spark session with Spark SQL support
spark = SparkSession.builder \
    .appName("Spark SQL Ingestion") \
    .master("local[*]") \
    .getOrCreate()

# Import types for schema definition
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType

# Check version
print(f"Spark version: {spark.version}")
```

## 1. Reading External Data Sources

Spark SQL can read data from various sources to make it available for SQL queries.

### Reading JSON Data

```python
# Read JSON data from our UberEats dataset
restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
drivers_df = spark.read.json("./storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl")
orders_df = spark.read.json("./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl")

# Display basic information
print(f"Restaurants: {restaurants_df.count()} records")
print(f"Drivers: {drivers_df.count()} records")
print(f"Orders: {orders_df.count()} records")

# Show schema
print("\nRestaurants Schema:")
restaurants_df.printSchema()
```

### Reading Other File Formats

```python
# For demonstration, let's write some data in different formats and read it back

# Write to CSV
restaurants_df.write.csv("./storage/restaurants.csv", header=True)

# Read CSV
restaurants_csv = spark.read.format("csv") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .load("./storage/restaurants.csv")

print("\nRestaurants from CSV (first 5 rows):")
restaurants_csv.show(5)

# Write to Parquet
restaurants_df.write.parquet("./storage/restaurants.parquet")

# Read Parquet
restaurants_parquet = spark.read.parquet("./storage/restaurants.parquet")

print("\nRestaurants from Parquet (first 5 rows):")
restaurants_parquet.show(5)
```

## 2. Creating Tables and Views

To use SQL queries with your data, you need to create tables or views that register your data in the Spark catalog.

### Creating Temporary Views

Temporary views exist only within the current Spark session:

```python
# Create temporary views for our datasets
restaurants_df.createOrReplaceTempView("restaurants")
drivers_df.createOrReplaceTempView("drivers")
orders_df.createOrReplaceTempView("orders")

# Now we can run SQL queries on these views
high_rated_restaurants = spark.sql("""
    SELECT name, cuisine_type, city, average_rating
    FROM restaurants
    WHERE average_rating > 4.0
    ORDER BY average_rating DESC
    LIMIT 5
""")

print("\nHigh Rated Restaurants (from SQL):")
high_rated_restaurants.show()
```

### Creating Global Temporary Views

Global temporary views exist across SparkSession objects within the same Spark application:

```python
# Create a global temporary view
restaurants_df.createOrReplaceGlobalTempView("global_restaurants")

# To use a global temporary view, prefix with "global_temp."
global_high_rated = spark.sql("""
    SELECT name, cuisine_type, city, average_rating
    FROM global_temp.global_restaurants
    WHERE average_rating > 4.5
    ORDER BY average_rating DESC
    LIMIT 5
""")

print("\nHigh Rated Restaurants (from global view):")
global_high_rated.show()
```

### Creating Permanent Tables

Permanent tables persist across Spark sessions and applications, allowing for reuse:

```python
# Create a database for our permanent tables
spark.sql("CREATE DATABASE IF NOT EXISTS ubereats")
spark.sql("USE ubereats")

# Create a permanent table
restaurants_df.write.saveAsTable("ubereats.restaurants")
drivers_df.write.saveAsTable("ubereats.drivers")
orders_df.write.saveAsTable("ubereats.orders")

# Query the permanent table
permanent_results = spark.sql("""
    SELECT cuisine_type, COUNT(*) as restaurant_count, 
           ROUND(AVG(average_rating), 2) as avg_rating
    FROM ubereats.restaurants
    GROUP BY cuisine_type
    ORDER BY restaurant_count DESC
    LIMIT 5
""")

print("\nRestaurant Count by Cuisine (from permanent table):")
permanent_results.show()
```

## 3. Schema Registration and Management

You can explicitly define schemas for your data or let Spark infer them.

### Schema Inference

By default, Spark infers schema from your data:

```python
# Schema inference from JSON
print("\nInferred Schema for Restaurants:")
restaurants_df.printSchema()

# Check data types in our temporary view
data_types = spark.sql("""
    DESCRIBE restaurants
""")

print("\nDescribed Schema for Restaurants Table:")
data_types.show(10)
```

### Explicit Schema Definition

For more control, you can define schemas explicitly:

```python
# Define an explicit schema for restaurants
restaurant_schema = StructType([
    StructField("restaurant_id", IntegerType(), True),
    StructField("name", StringType(), True),
    StructField("cuisine_type", StringType(), True),
    StructField("city", StringType(), True),
    StructField("country", StringType(), True),
    StructField("average_rating", DoubleType(), True),
    StructField("num_reviews", IntegerType(), True),
    StructField("cnpj", StringType(), True),
    StructField("address", StringType(), True),
    StructField("opening_time", StringType(), True),
    StructField("closing_time", StringType(), True),
    StructField("phone_number", StringType(), True),
    StructField("uuid", StringType(), True),
    StructField("dt_current_timestamp", StringType(), True)
])

# Read with explicit schema
restaurants_with_schema = spark.read \
    .schema(restaurant_schema) \
    .json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")

# Create a view with the explicit schema
restaurants_with_schema.createOrReplaceTempView("restaurants_explicit")

# Verify the explicit schema
explicit_schema = spark.sql("""
    DESCRIBE restaurants_explicit
""")

print("\nExplicit Schema for Restaurants:")
explicit_schema.show(10)
```

## 4. Catalog Management

The Spark Catalog helps you manage tables, databases, and functions.

### Exploring the Catalog

```python
# List all databases
databases = spark.sql("SHOW DATABASES")
print("\nAvailable Databases:")
databases.show()

# List all tables in a database
tables = spark.sql("SHOW TABLES IN ubereats")
print("\nTables in 'ubereats' Database:")
tables.show()

# List all columns in a table
columns = spark.sql("SHOW COLUMNS IN ubereats.restaurants")
print("\nColumns in 'restaurants' Table:")
columns.show()
```

### Using the Catalog API

For programmatic access, you can use the catalog API:

```python
# List all tables in current database
print("\nTables in current database (from API):")
for table in spark.catalog.listTables():
    print(f"- {table.name} (type: {table.tableType})")

# List all tables in a specific database
print("\nTables in 'ubereats' database (from API):")
for table in spark.catalog.listTables("ubereats"):
    print(f"- {table.name} (type: {table.tableType})")

# Get detailed information for a specific table
print("\nTable details for 'restaurants':")
table_info = spark.catalog.getTable("ubereats", "restaurants")
print(f"Database: {table_info.database}")
print(f"Name: {table_info.name}")
print(f"Description: {table_info.description}")
print(f"Table Type: {table_info.tableType}")
```

## 5. Practical Application: UberEats Data Ingestion Pipeline

Let's build a complete data ingestion pipeline for our UberEats data:

```python
def ingest_ubereats_data(spark, input_path, database_name="ubereats"):
    """
    Create a complete ingestion pipeline for UberEats data with Spark SQL.
    
    Parameters:
    - spark: SparkSession
    - input_path: Base path to input data
    - database_name: Name of the database to create
    
    Returns:
    - Dictionary of tables created
    """
    # Create the database
    spark.sql(f"CREATE DATABASE IF NOT EXISTS {database_name}")
    spark.sql(f"USE {database_name}")
    
    # Define paths and table names
    sources = {
        "restaurants": f"{input_path}/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl",
        "drivers": f"{input_path}/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl",
        "orders": f"{input_path}/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl",
        "ratings": f"{input_path}/mysql/ratings/01JS4W5A7YWTYRQKDA7F7N95VZ.jsonl"
    }
    
    # Read and register each dataset
    tables = {}
    for name, path in sources.items():
        print(f"Ingesting {name} from {path}")
        
        # Read the data
        df = spark.read.json(path)
        
        # Create a temporary view for immediate use
        df.createOrReplaceTempView(f"{name}_temp")
        
        # Create a permanent table
        df.write.mode("overwrite").saveAsTable(f"{database_name}.{name}")
        
        # Store reference
        tables[name] = df
        
        # Print basic stats
        row_count = df.count()
        col_count = len(df.columns)
        print(f"  - Registered table '{name}' with {row_count} rows and {col_count} columns")
    
    # Create a few useful views
    
    # 1. Popular restaurants view
    spark.sql(f"""
        CREATE OR REPLACE VIEW {database_name}.popular_restaurants AS
        SELECT 
            name, 
            cuisine_type, 
            city, 
            country, 
            average_rating, 
            num_reviews,
            average_rating * SQRT(num_reviews/1000) as popularity_score
        FROM {database_name}.restaurants
        WHERE num_reviews > 1000
        ORDER BY popularity_score DESC
    """)
    
    # 2. Restaurant-order summary view
    spark.sql(f"""
        CREATE OR REPLACE VIEW {database_name}.restaurant_orders AS
        SELECT 
            r.name, 
            r.cuisine_type, 
            r.city, 
            COUNT(o.order_id) as order_count,
            SUM(o.total_amount) as total_revenue,
            AVG(o.total_amount) as avg_order_value
        FROM {database_name}.restaurants r
        JOIN {database_name}.orders o ON r.cnpj = o.restaurant_key
        GROUP BY r.name, r.cuisine_type, r.city
        ORDER BY total_revenue DESC
    """)
    
    # Print all objects created
    print("\nDatabase objects created:")
    print(f"- Database: {database_name}")
    print("- Tables:")
    for table in spark.catalog.listTables(database_name):
        print(f"  - {table.name} ({table.tableType})")
    
    return tables

# Run the ingestion pipeline
tables = ingest_ubereats_data(spark, "./storage")

# Show a sample query from our new views
restaurant_orders = spark.sql("""
    SELECT * FROM ubereats.restaurant_orders LIMIT 5
""")

print("\nRestaurant Orders Summary:")
restaurant_orders.show()

popular_restaurants = spark.sql("""
    SELECT * FROM ubereats.popular_restaurants LIMIT 5
""")

print("\nPopular Restaurants:")
popular_restaurants.show()
```

## Conclusion and Best Practices

In this lesson, we've covered the fundamentals of data ingestion with Spark SQL:

1. **Reading External Data**: Loading data from JSON, CSV, Parquet, and other formats
2. **Creating Tables and Views**: Registering data for SQL queries
3. **Schema Management**: Inferring and defining schemas
4. **Catalog Management**: Working with databases, tables, and views
5. **Complete Ingestion Pipeline**: Building an end-to-end data ingestion workflow

**Best Practices:**

- Define schemas explicitly for production workloads to ensure data consistency
- Use temporary views for exploratory analysis and permanent tables for production data
- Leverage the catalog for managing and organizing your data
- Consider partitioning large tables for better performance
- Document your tables and columns for better maintainability
- Use appropriate table formats (like Parquet) for optimal performance

In the next lesson, we'll explore basic transformation operations using Spark SQL.

## Exercise

1. Create a database schema for all UberEats data sources:
   - Define appropriate data types for each column
   - Consider relationships between tables
   - Include appropriate primary and foreign keys

2. Build an ingestion pipeline that:
   - Reads data from all available sources
   - Applies explicit schemas with appropriate data types
   - Creates a new denormalized view combining data from multiple sources
   - Registers everything in the catalog with appropriate naming conventions

## Resources

- [Spark SQL Documentation](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [Spark SQL Data Sources](https://spark.apache.org/docs/latest/sql-data-sources.html)
- [Spark Catalog API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.catalog.Catalog.html)
