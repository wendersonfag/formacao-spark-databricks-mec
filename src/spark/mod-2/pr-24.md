# First Steps with Pandas API on Spark

This guide explores how to migrate Pandas code to a distributed environment using the Pandas API on Spark (previously known as Koalas). We'll focus on paradigm adjustments, optimization techniques, and debugging strategies.

## 1. Understanding the Transition

Pandas API on Spark allows you to leverage familiar Pandas syntax while gaining the power of distributed computing. Key differences to understand:

- **Lazy vs. Eager Execution**: Spark operations are lazy (executed only when needed)
- **Distributed Data**: Data is partitioned across a cluster
- **Performance Considerations**: Different optimization techniques apply

## 2. Setup and Basic Migration

Let's migrate a simple Pandas workflow to Spark:

```python
# app/pandas_to_spark.py

import pandas as pd
import pyspark.pandas as ps
from pyspark.sql import SparkSession

def create_spark_session():
    """Create a Spark session for our application"""
    return SparkSession.builder \
        .appName("UberEats-PandasAPI") \
        .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
        .getOrCreate()

def pandas_workflow():
    """Traditional Pandas workflow"""
    # Read data
    users_df = pd.read_json("./storage/mongodb/users/01JS4W5A7WWZBQ6Y1C465EYR76.jsonl", lines=True)
    restaurants_df = pd.read_json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl", lines=True)
    
    # Perform analysis
    print("=== Pandas Workflow ===")
    print(f"Users count: {len(users_df)}")
    print(f"Restaurants count: {len(restaurants_df)}")
    
    # Group by cuisine type
    cuisine_counts = restaurants_df.groupby('cuisine_type')['restaurant_id'].count()
    print("\nCuisine distribution:")
    print(cuisine_counts)
    
    # Calculate average rating by cuisine
    avg_ratings = restaurants_df.groupby('cuisine_type')['average_rating'].mean().round(2)
    print("\nAverage ratings by cuisine:")
    print(avg_ratings)
    
    # Join example
    # In a real scenario, we'd join on proper keys
    if 'city' in users_df.columns and 'city' in restaurants_df.columns:
        city_match = pd.merge(
            users_df[['user_id', 'city']], 
            restaurants_df[['restaurant_id', 'city']],
            on='city'
        )
        print(f"\nUsers and restaurants in same city: {len(city_match)}")

def spark_pandas_workflow():
    """Migrated workflow using Pandas API on Spark"""
    # Create Spark session
    spark = create_spark_session()
    
    # Read data - with Pandas API on Spark
    users_df = ps.read_json("./storage/mongodb/users/01JS4W5A7WWZBQ6Y1C465EYR76.jsonl", lines=True)
    restaurants_df = ps.read_json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl", lines=True)
    
    # Perform analysis (same syntax as Pandas!)
    print("\n=== Spark Pandas API Workflow ===")
    print(f"Users count: {len(users_df)}")
    print(f"Restaurants count: {len(restaurants_df)}")
    
    # Group by cuisine type
    cuisine_counts = restaurants_df.groupby('cuisine_type')['restaurant_id'].count()
    print("\nCuisine distribution:")
    print(cuisine_counts.to_pandas())  # Convert to Pandas for display
    
    # Calculate average rating by cuisine
    avg_ratings = restaurants_df.groupby('cuisine_type')['average_rating'].mean().round(2)
    print("\nAverage ratings by cuisine:")
    print(avg_ratings.to_pandas())  # Convert to Pandas for display
    
    # Join example - note the same syntax!
    if 'city' in users_df.columns and 'city' in restaurants_df.columns:
        city_match = ps.merge(
            users_df[['user_id', 'city']], 
            restaurants_df[['restaurant_id', 'city']],
            on='city'
        )
        print(f"\nUsers and restaurants in same city: {len(city_match)}")
    
    # Stop Spark session
    spark.stop()

if __name__ == "__main__":
    print("Running both workflows for comparison...")
    pandas_workflow()
    spark_pandas_workflow()
```

## 3. Optimizing for Scale

When moving to a distributed environment, consider these optimizations:

```python
# app/pandas_api_optimized.py

import pyspark.pandas as ps
from pyspark.sql import SparkSession
import time

def optimized_workflow():
    """Optimized workflow using Pandas API on Spark"""
    # Create Spark session with optimized configs
    spark = SparkSession.builder \
        .appName("UberEats-OptimizedPandasAPI") \
        .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
        .config("spark.sql.execution.arrow.maxRecordsPerBatch", "10000") \
        .config("spark.sql.shuffle.partitions", "10") \
        .getOrCreate()
    
    # Set the number of partitions (adjust based on your cluster)
    ps.set_option('compute.default_index_type', 'distributed')  # Better for large datasets
    
    # Demonstration of optimization techniques
    
    # 1. Technique: Cache frequently used DataFrames
    start = time.time()
    restaurants_df = ps.read_json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl", lines=True)
    # Cache the DataFrame if you'll use it multiple times
    restaurants_df = restaurants_df.spark.cache()
    
    # 2. Technique: Select only needed columns early
    # This reduces memory usage and network transfer
    restaurants_slim = restaurants_df[['restaurant_id', 'cuisine_type', 'average_rating', 'city']]
    
    # 3. Technique: Use native Spark functions for complex operations
    # Convert to Spark DataFrame for operations not well supported in Pandas API
    spark_df = restaurants_slim.to_spark()
    
    # 4. Technique: Intelligent repartitioning
    # Repartition if needed (e.g., after heavy filtering)
    if spark_df.count() < 1000:  # Small dataset after filtering
        spark_df = spark_df.repartition(2)
    
    # Convert back to Pandas API on Spark
    restaurants_slim = spark_df.pandas_api()
    
    # 5. Technique: Persist final results to memory when needed
    # Persist the result if it's used multiple times in downstream operations
    cuisine_ratings = restaurants_slim.groupby('cuisine_type')['average_rating'].mean()
    cuisine_ratings = cuisine_ratings.spark.cache()
    
    print("\n=== Optimized Pipeline Results ===")
    print(cuisine_ratings.to_pandas())
    print(f"Processing time: {time.time() - start:.2f} seconds")
    
    # Clean up resources
    cuisine_ratings.spark.unpersist()
    restaurants_df.spark.unpersist()
    spark.stop()

if __name__ == "__main__":
    optimized_workflow()
```

## 4. Debugging Techniques

Unlike Pandas, debugging distributed operations requires different strategies:

```python
# app/pandas_api_debugging.py

import pyspark.pandas as ps
from pyspark.sql import SparkSession
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def debug_demo():
    """Demonstrate debugging techniques for Pandas API on Spark"""
    spark = SparkSession.builder.appName("PandasAPI-Debugging").getOrCreate()
    
    try:
        # 1. Technique: Sample data for debugging
        logger.info("Loading data...")
        users_df = ps.read_json("./storage/mongodb/users/01JS4W5A7WWZBQ6Y1C465EYR76.jsonl", lines=True)
        
        # Debug tip: Convert small samples to Pandas for quick inspection
        logger.info("Sampling data for debugging...")
        sample_df = users_df.head(5).to_pandas()
        logger.info(f"Sample data types:\n{sample_df.dtypes}")
        
        # 2. Technique: Check partition sizes
        num_partitions = users_df.spark.data.rdd.getNumPartitions()
        logger.info(f"Number of partitions: {num_partitions}")
        
        # 3. Technique: Validate operations incrementally
        # Start with simple operations and build up
        logger.info("Validating operations incrementally...")
        
        # Simple count
        user_count = len(users_df)
        logger.info(f"User count: {user_count}")
        
        # Simple filter
        if 'city' in users_df.columns:
            city_count = users_df['city'].value_counts()
            logger.info(f"Number of unique cities: {len(city_count)}")
            
            # 4. Technique: Explain query plans
            logger.info("Checking query plan:")
            # Convert to Spark DataFrame to use explain
            users_df.spark.data.filter("city is not null").explain()
        
        # 5. Technique: Handle errors properly
        try:
            # Intentional error - divide by zero
            users_df['error_column'] = 1 / (users_df['user_id'] - users_df['user_id'])
        except Exception as e:
            logger.error(f"Expected error occurred: {str(e)}")
            # In production, you'd handle this appropriately
            
        # 6. Technique: Check progress with actions
        # Force execution to see if earlier steps work
        logger.info("Executing and checking DataFrame schema...")
        users_df.dtypes
        
    except Exception as e:
        logger.error(f"Error in pipeline: {str(e)}")
    finally:
        spark.stop()

if __name__ == "__main__":
    debug_demo()
```

## 5. Key Differences to Remember

| Feature | Pandas | Pandas API on Spark |
|---------|--------|---------------------|
| Execution | Eager | Lazy |
| Memory | Single machine | Distributed |
| Operations | In-memory | Distributed, optimized |
| Index | Always available | Performance cost |
| Function Application | Easy, flexible | Limited, use Spark UDFs |
| Data Size | GB max | TB+ |

## 6. Common Pitfalls to Avoid

1. **Collecting large DataFrames**: Avoid `.to_pandas()` on large datasets
2. **Row-by-row operations**: Replace with vectorized operations
3. **Ignoring partitioning**: Poor partitioning hurts performance
4. **Excessive shuffling**: Minimize operations that cause shuffles
5. **Missing cache/persist**: Cache intermediate results used multiple times

## 7. Moving Forward

1. Start by converting your existing Pandas code with minimal changes
2. Test with smaller datasets first to ensure correct results
3. Optimize incrementally, focusing on performance bottlenecks
4. Use Spark UI to monitor job execution and identify issues
5. Consider native PySpark functions for complex operations

By leveraging the Pandas API on Spark, you can scale your data processing capabilities while maintaining most of your familiar Pandas code patterns.
