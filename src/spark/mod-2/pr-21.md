# Integrating Spark with Object Storage (MinIO/S3)

## Introduction

In this practical session, we'll explore how to integrate Apache Spark with object storage systems like MinIO (which implements the S3 protocol). Object storage is the foundation of modern data lake architectures, enabling scalable and cost-effective processing of massive data volumes. We'll learn how to configure access to object storage, work with different storage formats, implement effective partitioning strategies, and optimize read/write performance.

## Setting Up Our Environment

Let's start by setting up our Spark session with the necessary configurations for S3/MinIO:

```python
# File: /src/app/spark_minio_integration.py

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, year, month, dayofmonth, hour
import os

# Initialize Spark with S3/MinIO configuration
def create_spark_session():
    return (SparkSession.builder
            .appName("Spark MinIO Integration")
            .master("local[*]")
            # AWS S3 configuration
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
            .config("spark.hadoop.fs.s3a.endpoint", "http://localhost:9000")  # MinIO server address
            .config("spark.hadoop.fs.s3a.access.key", "minioadmin")           # Default MinIO access key
            .config("spark.hadoop.fs.s3a.secret.key", "minioadmin")           # Default MinIO secret key
            .config("spark.hadoop.fs.s3a.path.style.access", "true")          # Use path-style access
            .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")    # Disable SSL for local MinIO
            # For performance
            .config("spark.hadoop.fs.s3a.connection.maximum", "100")          # Connection pooling
            .config("spark.hadoop.fs.s3a.threads.max", "20")                  # Thread pool size for uploads
            .getOrCreate())

# Create Spark session
spark = create_spark_session()
spark.sparkContext.setLogLevel("WARN")

print("Spark session initialized with MinIO/S3 configuration!")
```

## Step 1: Loading Data from Local Storage

Before working with object storage, let's load some data from our local storage:

```python
# Load UberEats data from local storage
def load_local_data():
    # Load restaurant data
    restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
    
    # Load driver data
    drivers_df = spark.read.json("./storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl")
    
    # Load order data
    orders_df = spark.read.json("./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl")
    
    return restaurants_df, drivers_df, orders_df

# Load data
restaurants_df, drivers_df, orders_df = load_local_data()

print(f"Loaded {restaurants_df.count()} restaurants")
print(f"Loaded {drivers_df.count()} drivers")
print(f"Loaded {orders_df.count()} orders")
```

## Step 2: Writing Data to Object Storage

Now, let's write our data to MinIO using different file formats. We'll compare commonly used formats like Parquet, ORC, and Avro:

```python
# Base S3 path - replace with your bucket name
s3_base_path = "s3a://ubereats-datalake"

# Write data in different formats
def write_data_in_formats(df, base_path, name):
    """
    Write DataFrame in multiple formats to object storage
    
    Args:
        df: DataFrame to write
        base_path: Base S3 path
        name: Entity name (e.g., restaurants, drivers)
    """
    # Parquet format (columnar, compressed)
    parquet_path = f"{base_path}/parquet/{name}"
    df.write.format("parquet").mode("overwrite").save(parquet_path)
    print(f"Wrote {name} in Parquet format to {parquet_path}")
    
    # ORC format (optimized row columnar)
    orc_path = f"{base_path}/orc/{name}"
    df.write.format("orc").mode("overwrite").save(orc_path)
    print(f"Wrote {name} in ORC format to {orc_path}")
    
    # Avro format (row-based, schema evolution)
    avro_path = f"{base_path}/avro/{name}"
    df.write.format("avro").mode("overwrite").save(avro_path)
    print(f"Wrote {name} in Avro format to {avro_path}")
    
    # JSON format (for comparison)
    json_path = f"{base_path}/json/{name}"
    df.write.format("json").mode("overwrite").save(json_path)
    print(f"Wrote {name} in JSON format to {json_path}")

# Write each dataset in different formats
write_data_in_formats(restaurants_df, s3_base_path, "restaurants")
write_data_in_formats(drivers_df, s3_base_path, "drivers")
write_data_in_formats(orders_df, s3_base_path, "orders")
```

## Step 3: Implementing Partitioning Strategies

Partitioning is crucial for efficient data lake operations. Let's implement common partitioning strategies:

```python
# Partition data by logical dimensions
def write_partitioned_data(df, base_path, name, partition_columns):
    """
    Write DataFrame with partitioning
    
    Args:
        df: DataFrame to write
        base_path: Base S3 path
        name: Entity name
        partition_columns: List of columns to partition by
    """
    path = f"{base_path}/partitioned/{name}"
    
    (df.write
     .format("parquet")
     .partitionBy(*partition_columns)
     .mode("overwrite")
     .save(path))
    
    print(f"Wrote {name} with partitioning by {partition_columns} to {path}")

# Partition orders by logical dimensions
orders_with_date = orders_df.withColumn("year", year(col("order_date"))) \
                           .withColumn("month", month(col("order_date"))) \
                           .withColumn("day", dayofmonth(col("order_date"))) \
                           .withColumn("hour", hour(col("order_date")))

# Different partitioning strategies
write_partitioned_data(orders_with_date, s3_base_path, "orders_by_year", ["year"])
write_partitioned_data(orders_with_date, s3_base_path, "orders_by_year_month", ["year", "month"])
write_partitioned_data(orders_with_date, s3_base_path, "orders_by_date", ["year", "month", "day"])

# Partition restaurants by city and cuisine
write_partitioned_data(restaurants_df, s3_base_path, "restaurants_by_city", ["city"])
write_partitioned_data(restaurants_df, s3_base_path, "restaurants_by_city_cuisine", ["city", "cuisine_type"])

# Partition drivers by city and vehicle type
write_partitioned_data(drivers_df, s3_base_path, "drivers_by_city", ["city"])
write_partitioned_data(drivers_df, s3_base_path, "drivers_by_city_vehicle", ["city", "vehicle_type"])
```

## Step 4: Reading Data from Object Storage

Now that we've written data, let's read it back efficiently:

```python
# Read data from object storage
def read_data(format_type, path):
    """
    Read data from object storage with specified format
    
    Args:
        format_type: Format to use (parquet, orc, avro, json)
        path: S3 path to read from
    
    Returns:
        DataFrame with the data
    """
    return spark.read.format(format_type).load(path)

# Compare reading from different formats
parquet_df = read_data("parquet", f"{s3_base_path}/parquet/restaurants")
orc_df = read_data("orc", f"{s3_base_path}/orc/restaurants")
avro_df = read_data("avro", f"{s3_base_path}/avro/restaurants")
json_df = read_data("json", f"{s3_base_path}/json/restaurants")

print("Data sample from Parquet:")
parquet_df.show(5)
```

## Step 5: Efficient Reading with Partition Pruning

One of the key benefits of partitioning is partition pruning - reading only the data you need:

```python
# Read with partition pruning
def read_with_partition_pruning(path, partition_filters):
    """
    Read data using partition pruning
    
    Args:
        path: S3 path to partitioned data
        partition_filters: List of filter conditions
    
    Returns:
        DataFrame with filtered data
    """
    df = spark.read.format("parquet").load(path)
    
    # Apply filters
    for filter_condition in partition_filters:
        df = df.filter(filter_condition)
    
    return df

# Example: Read only data for a specific year
orders_2023 = read_with_partition_pruning(
    f"{s3_base_path}/partitioned/orders_by_year", 
    [col("year") == 2023]
)

print(f"Orders from 2023: {orders_2023.count()}")

# Example: Read data for a specific month range
orders_q1_2023 = read_with_partition_pruning(
    f"{s3_base_path}/partitioned/orders_by_year_month", 
    [col("year") == 2023, col("month").isin(1, 2, 3)]
)

print(f"Orders from Q1 2023: {orders_q1_2023.count()}")

# Example: Read restaurants from a specific city and cuisine type
italian_restaurants_city = read_with_partition_pruning(
    f"{s3_base_path}/partitioned/restaurants_by_city_cuisine", 
    [col("city") == "São Paulo", col("cuisine_type") == "Italian"]
)

print(f"Italian restaurants in São Paulo: {italian_restaurants_city.count()}")
```

## Step 6: Performance Tuning for Object Storage

Let's explore performance tuning options for optimal S3/MinIO integration:

```python
# Configure a performance-optimized Spark session
def create_optimized_spark_session():
    return (SparkSession.builder
            .appName("Spark MinIO Optimized")
            .master("local[*]")
            # Basic S3 configuration
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
            .config("spark.hadoop.fs.s3a.endpoint", "http://localhost:9000")
            .config("spark.hadoop.fs.s3a.access.key", "minioadmin")
            .config("spark.hadoop.fs.s3a.secret.key", "minioadmin")
            .config("spark.hadoop.fs.s3a.path.style.access", "true")
            # Performance optimizations
            .config("spark.hadoop.fs.s3a.connection.maximum", "100")             # Connection pooling
            .config("spark.hadoop.fs.s3a.threads.max", "20")                     # Thread pool size for uploads
            .config("spark.hadoop.fs.s3a.connection.timeout", "300000")          # 5 minutes connection timeout
            .config("spark.hadoop.fs.s3a.connection.establish.timeout", "5000")  # 5 seconds to establish a connection
            .config("spark.hadoop.fs.s3a.readahead.range", "256K")               # Readahead buffer size
            .config("spark.hadoop.fs.s3a.fast.upload", "true")                   # Enable fast upload path
            .config("spark.hadoop.fs.s3a.fast.upload.buffer", "disk")            # Buffer type (memory, disk, array)
            .config("spark.hadoop.fs.s3a.fast.upload.active.blocks", "4")        # Active upload blocks per thread
            .config("spark.hadoop.fs.s3a.multipart.size", "64M")                 # Size of each multipart chunk
            .config("spark.hadoop.fs.s3a.multipart.threshold", "64M")            # Threshold for multipart uploads
            # Committer optimizations for writing
            .config("spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version", "2")  # Faster commit algorithm
            .config("spark.hadoop.fs.s3a.committer.name", "directory")                    # Directory committer
            .config("spark.hadoop.fs.s3a.committer.staging.conflict-mode", "append")      # Append mode for conflicts
            .config("spark.hadoop.fs.s3a.committer.staging.tmp.path", "/tmp/spark_staging")  # Local staging path
            # Parquet optimizations
            .config("spark.sql.parquet.filterPushdown", "true")                  # Push filters to Parquet
            .config("spark.sql.parquet.mergeSchema", "false")                    # Disable schema merging
            .config("spark.sql.parquet.columnarReaderBatchSize", "4096")         # Batch size for columnar reader
            .config("spark.sql.files.maxPartitionBytes", "128m")                 # Size per partition
            .config("spark.sql.files.openCostInBytes", "4194304")                # File open cost estimation
            # Adaptive query execution
            .config("spark.sql.adaptive.enabled", "true")                        # Enable adaptive query execution
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true")     # Coalesce partitions
            .config("spark.sql.adaptive.advisoryPartitionSizeInBytes", "64m")    # Target size when coalescing
            .getOrCreate())

# Create optimized session
optimized_spark = create_optimized_spark_session()
optimized_spark.sparkContext.setLogLevel("WARN")

print("Optimized Spark session initialized!")
```

## Step 7: Implementing Bucketing for Join Optimization

Bucketing can further optimize your data organization, especially for join operations:

```python
# Implement bucketing for join optimization
def write_bucketed_data(spark, df, base_path, name, bucket_columns, num_buckets):
    """
    Write DataFrame with bucketing
    
    Args:
        spark: SparkSession
        df: DataFrame to write
        base_path: Base S3 path
        name: Entity name
        bucket_columns: List of columns to bucket by
        num_buckets: Number of buckets
    """
    # Create a temporary view
    df.createOrReplaceTempView(f"{name}_temp")
    
    # Create a bucketed table
    spark.sql(f"""
        CREATE TABLE IF NOT EXISTS {name}_bucketed
        USING PARQUET
        CLUSTERED BY ({', '.join(bucket_columns)})
        INTO {num_buckets} BUCKETS
        LOCATION '{base_path}/bucketed/{name}'
        AS SELECT * FROM {name}_temp
    """)
    
    print(f"Created bucketed table {name}_bucketed at {base_path}/bucketed/{name}")

# Bucket restaurants by restaurant_id
write_bucketed_data(
    optimized_spark, 
    restaurants_df, 
    s3_base_path, 
    "restaurants", 
    ["restaurant_id"], 
    4
)

# Bucket orders by restaurant_key for efficient joins
write_bucketed_data(
    optimized_spark, 
    orders_df, 
    s3_base_path, 
    "orders", 
    ["restaurant_key"], 
    4
)

# Perform an optimized join using bucketed data
joined_data = optimized_spark.sql("""
    SELECT 
        r.name AS restaurant_name,
        r.cuisine_type,
        COUNT(o.order_id) AS order_count,
        SUM(o.total_amount) AS total_revenue
    FROM 
        restaurants_bucketed r
    JOIN 
        orders_bucketed o ON r.cnpj = o.restaurant_key
    GROUP BY 
        r.name, r.cuisine_type
    ORDER BY 
        total_revenue DESC
""")

print("Restaurant performance from bucketed data:")
joined_data.show(5)
```

## Step 8: Complete Example - Building a Data Lake

Let's put everything together to build a simple data lake architecture:

```python
# File: /src/app/ubereats_datalake.py

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, year, month, dayofmonth, hour, date_format, to_date
import os

def create_spark_session():
    return (SparkSession.builder
            .appName("UberEats Data Lake")
            .master("local[*]")
            # S3 configuration
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
            .config("spark.hadoop.fs.s3a.endpoint", "http://localhost:9000")
            .config("spark.hadoop.fs.s3a.access.key", "minioadmin")
            .config("spark.hadoop.fs.s3a.secret.key", "minioadmin")
            .config("spark.hadoop.fs.s3a.path.style.access", "true")
            .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")
            # Performance optimizations
            .config("spark.sql.adaptive.enabled", "true")
            .config("spark.sql.parquet.filterPushdown", "true")
            .getOrCreate())

def load_source_data(spark, base_path):
    """Load data from source files"""
    restaurants_df = spark.read.json(f"{base_path}/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
    drivers_df = spark.read.json(f"{base_path}/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl")
    orders_df = spark.read.json(f"{base_path}/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl")
    
    return restaurants_df, drivers_df, orders_df

def preprocess_data(restaurants_df, drivers_df, orders_df):
    """Clean and prepare data"""
    # Process restaurants
    restaurants_clean = (restaurants_df
                        .select(
                            "restaurant_id", "name", "cuisine_type", "city", 
                            "average_rating", "num_reviews", "cnpj"
                        )
                        .filter(col("average_rating").isNotNull()))
    
    # Process drivers
    drivers_clean = (drivers_df
                    .select(
                        "driver_id", "first_name", "last_name", "city", 
                        "vehicle_type", "license_number"
                    ))
    
    # Process orders - add date dimensions
    orders_clean = (orders_df
                   .select(
                       "order_id", "restaurant_key", "driver_key", 
                       "total_amount", "order_date"
                   )
                   .filter(col("total_amount").isNotNull())
                   .withColumn("order_date", to_date(col("order_date")))
                   .withColumn("year", year(col("order_date")))
                   .withColumn("month", month(col("order_date")))
                   .withColumn("day", dayofmonth(col("order_date")))
                   .withColumn("hour", hour(col("order_date"))))
    
    return restaurants_clean, drivers_clean, orders_clean

def build_bronze_layer(restaurants_df, drivers_df, orders_df, s3_path):
    """Build bronze layer (raw data)"""
    bronze_path = f"{s3_path}/bronze"
    
    # Write with partitioning appropriate for each entity
    restaurants_df.write.format("parquet").mode("overwrite").save(f"{bronze_path}/restaurants")
    drivers_df.write.format("parquet").mode("overwrite").save(f"{bronze_path}/drivers")
    
    # Partition orders by time dimensions
    orders_df.write.format("parquet") \
        .partitionBy("year", "month") \
        .mode("overwrite") \
        .save(f"{bronze_path}/orders")
    
    print(f"Bronze layer built at {bronze_path}")

def build_silver_layer(spark, s3_path):
    """Build silver layer (cleaned, transformed data)"""
    bronze_path = f"{s3_path}/bronze"
    silver_path = f"{s3_path}/silver"
    
    # Read from bronze
    restaurants_df = spark.read.parquet(f"{bronze_path}/restaurants")
    drivers_df = spark.read.parquet(f"{bronze_path}/drivers")
    orders_df = spark.read.parquet(f"{bronze_path}/orders")
    
    # Transform restaurants - add rating category
    restaurants_silver = restaurants_df.withColumn(
        "rating_category",
        when(col("average_rating") >= 4.5, "Excellent")
        .when(col("average_rating") >= 4.0, "Very Good")
        .when(col("average_rating") >= 3.5, "Good")
        .when(col("average_rating") >= 3.0, "Average")
        .otherwise("Below Average")
    )
    
    # Transform drivers - combine name fields
    drivers_silver = drivers_df.withColumn(
        "full_name", 
        concat(col("first_name"), lit(" "), col("last_name"))
    )
    
    # Transform orders - add day of week
    orders_silver = orders_df.withColumn(
        "day_of_week", 
        date_format(col("order_date"), "E")
    )
    
    # Write silver tables
    restaurants_silver.write.format("parquet").mode("overwrite").save(f"{silver_path}/restaurants")
    drivers_silver.write.format("parquet").mode("overwrite").save(f"{silver_path}/drivers")
    
    # Partition orders differently in silver layer
    orders_silver.write.format("parquet") \
        .partitionBy("year", "month", "day_of_week") \
        .mode("overwrite") \
        .save(f"{silver_path}/orders")
    
    print(f"Silver layer built at {silver_path}")

def build_gold_layer(spark, s3_path):
    """Build gold layer (business-ready data)"""
    silver_path = f"{s3_path}/silver"
    gold_path = f"{s3_path}/gold"
    
    # Read from silver
    restaurants_df = spark.read.parquet(f"{silver_path}/restaurants")
    drivers_df = spark.read.parquet(f"{silver_path}/drivers")
    orders_df = spark.read.parquet(f"{silver_path}/orders")
    
    # Create restaurant performance analytics
    restaurant_performance = (restaurants_df
                            .join(
                                orders_df,
                                restaurants_df["cnpj"] == orders_df["restaurant_key"],
                                "left"
                            )
                            .groupBy(
                                "restaurant_id", "name", "cuisine_type", 
                                "city", "average_rating", "rating_category"
                            )
                            .agg(
                                count("order_id").alias("order_count"),
                                round(sum("total_amount"), 2).alias("total_revenue"),
                                round(avg("total_amount"), 2).alias("avg_order_value")
                            ))
    
    # Create cuisine performance analytics
    cuisine_performance = (restaurant_performance
                          .groupBy("cuisine_type")
                          .agg(
                              count("restaurant_id").alias("restaurant_count"),
                              round(avg("average_rating"), 2).alias("avg_rating"),
                              sum("order_count").alias("total_orders"),
                              round(sum("total_revenue"), 2).alias("total_revenue")
                          )
                          .orderBy(desc("total_revenue")))
    
    # Create driver performance analytics
    driver_performance = (drivers_df
                         .join(
                             orders_df,
                             drivers_df["license_number"] == orders_df["driver_key"],
                             "left"
                         )
                         .groupBy(
                             "driver_id", "full_name", "city", "vehicle_type"
                         )
                         .agg(
                             count("order_id").alias("delivery_count"),
                             round(sum("total_amount"), 2).alias("total_revenue"),
                             round(avg("total_amount"), 2).alias("avg_revenue")
                         ))
    
    # Write gold tables - optimized for analytics
    restaurant_performance.write.format("parquet") \
        .partitionBy("cuisine_type", "city") \
        .mode("overwrite") \
        .save(f"{gold_path}/restaurant_performance")
    
    cuisine_performance.write.format("parquet") \
        .mode("overwrite") \
        .save(f"{gold_path}/cuisine_performance")
    
    driver_performance.write.format("parquet") \
        .partitionBy("city", "vehicle_type") \
        .mode("overwrite") \
        .save(f"{gold_path}/driver_performance")
    
    # Create time-based analytics
    time_analytics = (orders_df
                     .groupBy("year", "month", "day_of_week", "hour")
                     .agg(
                         count("order_id").alias("order_count"),
                         round(sum("total_amount"), 2).alias("total_revenue"),
                         round(avg("total_amount"), 2).alias("avg_order_value")
                     ))
    
    time_analytics.write.format("parquet") \
        .partitionBy("year", "month") \
        .mode("overwrite") \
        .save(f"{gold_path}/time_analytics")
    
    print(f"Gold layer built at {gold_path}")

def main():
    """Main data lake building function"""
    # Create Spark session
    spark = create_spark_session()
    spark.sparkContext.setLogLevel("WARN")
    
    # S3 base path
    s3_path = "s3a://ubereats-datalake"
    
    try:
        # Load source data
        print("Loading source data...")
        restaurants_df, drivers_df, orders_df = load_source_data(spark, "./storage")
        
        # Preprocess data
        print("Preprocessing data...")
        restaurants_clean, drivers_clean, orders_clean = preprocess_data(
            restaurants_df, drivers_df, orders_df
        )
        
        # Build data lake layers
        print("Building data lake layers...")
        build_bronze_layer(restaurants_clean, drivers_clean, orders_clean, s3_path)
        build_silver_layer(spark, s3_path)
        build_gold_layer(spark, s3_path)
        
        print("Data lake built successfully!")
        
    except Exception as e:
        print(f"Error building data lake: {str(e)}")
        
    finally:
        # Stop Spark
        spark.stop()
        print("Spark session stopped")

if __name__ == "__main__":
    main()
```

## Best Practices for Spark and Object Storage Integration

### 1. Storage Format Selection

| Format | Pros | Cons | Best Use Cases |
|--------|------|------|----------------|
| **Parquet** | - Columnar storage<br>- Good compression<br>- Schema evolution<br>- Type preservation | - Slower writes<br>- Less human-readable | - Default choice for most data lake applications<br>- Best for analytics workloads |
| **ORC** | - Very good compression<br>- Fast reads<br>- Indexing | - Slower writes<br>- Limited schema evolution | - When extra compression is critical<br>- Hive-centric environments |
| **Avro** | - Superior schema evolution<br>- Row-based format<br>- Good for streaming | - Slower analytical queries<br>- Larger storage footprint | - When schemas change frequently<br>- For data ingestion layers |
| **JSON** | - Human-readable<br>- Schema flexibility<br>- Universal compatibility | - Poor compression<br>- Slower performance<br>- Larger storage footprint | - For data interchange<br>- When human-readability is required |

### 2. Partitioning Strategies

| Strategy | Description | Pros | Cons |
|----------|-------------|------|------|
| **Time-based** | Partition by year/month/day/hour | - Efficient for time-series data<br>- Good for retention policies | - Can create many small files<br>- Time skew issues |
| **Categorical** | Partition by categories (e.g., city, type) | - Excellent for filtering by categories<br>- Predictable partition sizes | - Too many categories can cause small files<br>- Hotspotting if distribution is skewed |
| **Range-based** | Partition by value ranges | - Controls partition size<br>- Works well for numeric data | - Requires knowledge of data distribution<br>- More complex implementation |
| **Composite** | Combine multiple fields | - Very fine-grained filtering<br>- Works well for complex queries | - Creates many partitions<br>- Complex maintenance |

### 3. Performance Tuning Tips

1. **File Size Optimization**:
   - Target file sizes between 64MB and 1GB
   - Avoid many small files (reduces metadata overhead)
   - Consider repartitioning before writing

2. **Partitioning Best Practices**:
   - Don't over-partition
   - Choose high-cardinality columns carefully
   - Align partitioning with query patterns

3. **Read Performance**:
   - Enable predicate pushdown
   - Use partition pruning
   - Set appropriate read options:
     ```python
     df = spark.read.option("mergeSchema", "false") \
                   .option("recursiveFileLookup", "true") \
                   .parquet("s3a://path")
     ```

4. **Write Performance**:
   - Use optimized committers
   - Tune number of partitions
   - Consider bucketing for join-heavy workloads

5. **Connection Management**:
   - Increase connection pool size
   - Tune timeouts appropriately
   - Enable fast upload features

### 4. Common Production Patterns

1. **Medallion Architecture** (Bronze, Silver, Gold)
   - Bronze: Raw data, minimal processing
   - Silver: Cleaned, validated data
   - Gold: Business-ready, aggregated data

2. **Incremental Processing**
   - Use partition discovery for efficient loading
   - Track processed timestamps/markers
   - Implement merge patterns for updates

3. **Data Lifecycle Management**
   - Implement retention policies by partition
   - Archive cold data to less expensive storage
   - Maintain metadata for historical tracking

## Conclusion

In this session, we've explored how to integrate Spark with object storage systems like MinIO and S3. We've learned:

1. **Connection setup** for S3-compatible storage
2. **Storage format selection** (Parquet, ORC, Avro)
3. **Partitioning strategies** for optimal data organization
4. **Performance tuning** for efficient data access
5. **Implementing a data lake** with the medallion architecture

These skills enable you to build scalable and cost-effective data processing pipelines using object storage as the foundation of your data architecture. The flexibility of object storage, combined with Spark's processing power, provides an ideal platform for handling massive data volumes in modern data engineering.

## Next Steps

- Experiment with different partitioning strategies for your specific data patterns
- Explore Delta Lake for ACID transactions on object storage
- Implement incremental processing patterns
- Set up automated data quality checks
- Create a monitoring system for your data lake
