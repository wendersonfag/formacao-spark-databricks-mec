# Deep Dive & Best Practices with Pandas API on Spark

This guide explores advanced techniques, optimizations, and best practices for using the Pandas API on Spark (formerly Koalas) effectively. We'll focus on writing performant pandas-like code that scales to big data while avoiding common pitfalls.

## 1. Understanding Pandas API Architecture

The Pandas API on Spark translates pandas-like operations to Spark operations under the hood:

```python
# app/architecture_example.py
import pandas as pd
import pyspark.pandas as ps
from pyspark.sql import SparkSession

def demonstrate_architecture():
    """Show how Pandas API translates to Spark operations"""
    # Create Spark session
    spark = SparkSession.builder \
        .appName("PandasAPI-Architecture") \
        .getOrCreate()
    
    # Load restaurant data
    # Pandas approach
    pd_df = pd.read_json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl", lines=True)
    
    # Pandas API approach
    ps_df = ps.read_json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl", lines=True)
    
    print("Pandas object type:", type(pd_df))
    print("Pandas API object type:", type(ps_df))
    
    # Show the underlying Spark execution plan
    print("\nUnderlying Spark execution plan:")
    ps_df.to_spark().explain()
    
    # Example transformation
    ps_result = ps_df.groupby('cuisine_type').agg({'average_rating': 'mean'}).reset_index()
    
    print("\nPandas API result (showing Spark plan):")
    ps_result.to_spark().explain()
    
    # Clean up
    spark.stop()
```

## 2. Key Optimization Techniques

### 2.1 Switching Between Pandas API and PySpark

```python
# app/optimization_techniques.py
import pandas as pd
import pyspark.pandas as ps
from pyspark.sql import SparkSession
import time

def demonstrate_optimizations():
    """Show key optimization techniques for Pandas API"""
    spark = SparkSession.builder \
        .appName("PandasAPI-Optimizations") \
        .getOrCreate()
    
    # Load restaurant data
    ps_df = ps.read_json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl", lines=True)
    
    # Technique 1: Switch to Spark for complex joins
    print("Technique 1: Switch to Spark for complex operations")
    
    def pandas_api_join():
        start = time.time()
        users_df = ps.read_json("./storage/mongodb/users/01JS4W5A7WWZBQ6Y1C465EYR76.jsonl", lines=True)
        
        # Join using Pandas API
        result = ps_df.merge(users_df, on='city', how='inner')
        count = len(result)
        duration = time.time() - start
        print(f"Pandas API join: {count} rows in {duration:.2f} seconds")
        return result
    
    def hybrid_join():
        start = time.time()
        users_df = ps.read_json("./storage/mongodb/users/01JS4W5A7WWZBQ6Y1C465EYR76.jsonl", lines=True)
        
        # Convert to Spark DataFrames
        spark_restaurants = ps_df.to_spark()
        spark_users = users_df.to_spark()
        
        # Optimize with broadcast join
        from pyspark.sql.functions import broadcast
        result_spark = spark_restaurants.join(
            broadcast(spark_users),
            on='city', 
            how='inner'
        )
        
        # Convert back to Pandas API
        result = result_spark.pandas_api()
        
        count = len(result)
        duration = time.time() - start
        print(f"Hybrid join: {count} rows in {duration:.2f} seconds")
        return result
    
    # Execute and compare
    pandas_result = pandas_api_join()
    hybrid_result = hybrid_join()
    
    # Technique 2: Use native Spark functions for string operations
    print("\nTechnique 2: Use native Spark functions for string operations")
    
    def pandas_api_string_ops():
        start = time.time()
        # Using pandas-like string operations - potentially slow
        result = ps_df.copy()
        result['name_clean'] = ps_df['name'].str.replace(' Restaurante', '')
        result['name_upper'] = ps_df['name'].str.upper()
        duration = time.time() - start
        print(f"Pandas API string ops: {duration:.2f} seconds")
        return result
    
    def spark_string_ops():
        start = time.time()
        # Convert to Spark
        spark_df = ps_df.to_spark()
        
        # Use Spark functions
        from pyspark.sql.functions import regexp_replace, upper, col
        spark_df = spark_df.withColumn(
            'name_clean', 
            regexp_replace(col('name'), ' Restaurante', '')
        ).withColumn(
            'name_upper',
            upper(col('name'))
        )
        
        # Convert back
        result = spark_df.pandas_api()
        duration = time.time() - start
        print(f"Spark string ops: {duration:.2f} seconds")
        return result
    
    # Execute and compare
    pandas_api_string_ops()
    spark_string_ops()
    
    # Technique 3: Cache strategically
    print("\nTechnique 3: Cache strategically")
    
    def without_caching():
        start = time.time()
        
        # Multiple operations on same DataFrame
        result1 = ps_df.groupby('cuisine_type').agg({'average_rating': 'mean'})
        result2 = ps_df.groupby('cuisine_type').agg({'num_reviews': 'sum'})
        result3 = ps_df.groupby('cuisine_type').agg({'restaurant_id': 'count'})
        
        # Force computation
        result1.compute()
        result2.compute()
        result3.compute()
        
        duration = time.time() - start
        print(f"Without caching: {duration:.2f} seconds")
    
    def with_caching():
        start = time.time()
        
        # Cache the DataFrame using Spark's cache
        ps_df.spark.cache()
        
        # Same operations
        result1 = ps_df.groupby('cuisine_type').agg({'average_rating': 'mean'})
        result2 = ps_df.groupby('cuisine_type').agg({'num_reviews': 'sum'})
        result3 = ps_df.groupby('cuisine_type').agg({'restaurant_id': 'count'})
        
        # Force computation
        result1.compute()
        result2.compute()
        result3.compute()
        
        # Unpersist when done
        ps_df.spark.unpersist()
        
        duration = time.time() - start
        print(f"With caching: {duration:.2f} seconds")
    
    # Execute and compare
    without_caching()
    with_caching()
    
    # Clean up
    spark.stop()
```

### 2.2 Partitioning Strategies

```python
# app/partitioning_strategies.py
import pyspark.pandas as ps
from pyspark.sql import SparkSession
import time

def demonstrate_partitioning():
    """Show partitioning strategies for Pandas API on Spark"""
    spark = SparkSession.builder \
        .appName("PandasAPI-Partitioning") \
        .getOrCreate()
    
    # Load order data
    ps_df = ps.read_json("./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl", lines=True)
    
    # Check current partitioning
    spark_df = ps_df.to_spark()
    num_partitions = spark_df.rdd.getNumPartitions()
    print(f"Initial number of partitions: {num_partitions}")
    
    # Strategy 1: Reduce partitions for small data
    print("\nStrategy 1: Reduce partitions for small data")
    
    def too_many_partitions():
        start = time.time()
        
        # Many partitions on small data is inefficient
        result = ps_df.to_spark().repartition(100).pandas_api()
        
        # Force computation
        result.count()
        
        duration = time.time() - start
        print(f"Too many partitions (100): {duration:.2f} seconds")
    
    def optimal_partitions():
        start = time.time()
        
        # Rule of thumb: partition size between 10MB-1GB
        # For small data, fewer partitions are better
        result = ps_df.to_spark().coalesce(5).pandas_api()
        
        # Force computation
        result.count()
        
        duration = time.time() - start
        print(f"Optimal partitions (5): {duration:.2f} seconds")
    
    # Execute and compare
    too_many_partitions()
    optimal_partitions()
    
    # Strategy 2: Repartition for join efficiency
    print("\nStrategy 2: Repartition by join key")
    
    def unaligned_join():
        start = time.time()
        
        # Load restaurants
        restaurants_df = ps.read_json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl", lines=True)
        
        # This join would not have aligned partitions
        # Assuming restaurant_key in orders could be mapped to restaurant_id
        restaurants_spark = restaurants_df.to_spark()
        orders_spark = ps_df.to_spark()
        
        # Extract restaurant_id from restaurant_key (simplified example)
        from pyspark.sql.functions import substring, col
        orders_spark = orders_spark.withColumn(
            "extracted_id", 
            substring(col("restaurant_key"), 1, 5)
        )
        
        # Join DataFrames
        result = orders_spark.join(
            restaurants_spark,
            orders_spark.extracted_id == restaurants_spark.restaurant_id,
            "left"
        )
        
        count = result.count()
        
        duration = time.time() - start
        print(f"Unaligned join: {count} rows in {duration:.2f} seconds")
    
    def aligned_join():
        start = time.time()
        
        # Load restaurants
        restaurants_df = ps.read_json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl", lines=True)
        
        # This time, we'll repartition both DataFrames by the join key
        restaurants_spark = restaurants_df.to_spark()
        orders_spark = ps_df.to_spark()
        
        # Extract restaurant_id from restaurant_key (simplified example)
        from pyspark.sql.functions import substring, col
        orders_spark = orders_spark.withColumn(
            "extracted_id", 
            substring(col("restaurant_key"), 1, 5)
        )
        
        # Repartition both DataFrames by the join key
        restaurants_spark = restaurants_spark.repartition(10, "restaurant_id")
        orders_spark = orders_spark.repartition(10, "extracted_id")
        
        # Join DataFrames
        result = orders_spark.join(
            restaurants_spark,
            orders_spark.extracted_id == restaurants_spark.restaurant_id,
            "left"
        )
        
        count = result.count()
        
        duration = time.time() - start
        print(f"Aligned join: {count} rows in {duration:.2f} seconds")
    
    # Execute and compare
    unaligned_join()
    aligned_join()
    
    # Clean up
    spark.stop()
```

## 3. Common Pitfalls and How to Avoid Them

```python
# app/common_pitfalls.py
import pandas as pd
import pyspark.pandas as ps
from pyspark.sql import SparkSession
import time

def demonstrate_pitfalls():
    """Show common pitfalls when using Pandas API on Spark"""
    spark = SparkSession.builder \
        .appName("PandasAPI-Pitfalls") \
        .getOrCreate()
    
    # Load restaurant data
    ps_df = ps.read_json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl", lines=True)
    
    # Pitfall 1: Converting large DataFrames to Pandas
    print("Pitfall 1: Converting large DataFrames to Pandas")
    
    def bad_approach():
        print("Bad approach (don't do this with large data):")
        # WARNING: This would cause OOM with large data
        # pd_df = ps_df.to_pandas()
        print("Comment out to prevent memory issues")
        
        # Instead, sample first:
        sample_pd = ps_df.head(5).to_pandas()
        print(f"Sampled {len(sample_pd)} rows to Pandas")
    
    bad_approach()
    
    # Pitfall 2: Index operations
    print("\nPitfall 2: Index operations")
    
    def costly_index_ops():
        print("Costly index operations:")
        start = time.time()
        
        # Create index on large dataset - expensive
        indexed_df = ps_df.set_index('restaurant_id')
        
        # Access by index - can be inefficient for large data
        if len(indexed_df) > 0:
            first_id = indexed_df.index[0]
            # Accessing by index can be expensive
            # row = indexed_df.loc[first_id]
        
        duration = time.time() - start
        print(f"Index operations: {duration:.2f} seconds")
        
        print("Better approach:")
        start = time.time()
        
        # Filter rather than using index lookups
        filtered = ps_df[ps_df['restaurant_id'] == ps_df['restaurant_id'].iloc[0]]
        
        duration = time.time() - start
        print(f"Filter operations: {duration:.2f} seconds")
    
    costly_index_ops()
    
    # Pitfall 3: Row-by-row operations
    print("\nPitfall 3: Row-by-row operations")
    
    def row_by_row_ops():
        print("Row-by-row operations (very slow):")
        
        # In pandas, you might do:
        # for index, row in pd_df.iterrows():
        #     result = some_function(row['column'])
        
        print("Don't use .iterrows() or .apply() with row axis in Pandas API!")
        
        print("\nBetter approach - vectorized operations:")
        start = time.time()
        
        # Vectorized operation
        ps_df['rating_category'] = ps_df['average_rating'].apply(
            lambda x: 'High' if x >= 4.0 else 'Medium' if x >= 3.0 else 'Low'
        )
        
        duration = time.time() - start
        print(f"Vectorized operations: {duration:.2f} seconds")
        
        print("\nEven better - use Spark functions:")
        start = time.time()
        
        # Convert to Spark for optimal performance
        from pyspark.sql.functions import when, col
        
        spark_df = ps_df.to_spark()
        spark_df = spark_df.withColumn(
            'rating_category',
            when(col('average_rating') >= 4.0, 'High')
            .when(col('average_rating') >= 3.0, 'Medium')
            .otherwise('Low')
        )
        result = spark_df.pandas_api()
        
        duration = time.time() - start
        print(f"Spark functions: {duration:.2f} seconds")
    
    row_by_row_ops()
    
    # Pitfall 4: Ignoring lazy evaluation
    print("\nPitfall 4: Ignoring lazy evaluation")
    
    def misunderstanding_laziness():
        print("Misunderstanding lazy evaluation:")
        
        # Example 1: Chain of transformations
        start = time.time()
        
        # None of these are executed yet
        result = ps_df.copy()
        result = result[result['average_rating'] >= 3.0]
        result = result.sort_values('average_rating', ascending=False)
        result = result[['name', 'cuisine_type', 'average_rating']]
        
        # Force evaluation
        count = len(result)
        
        duration = time.time() - start
        print(f"Chain with single evaluation: {duration:.2f} seconds, {count} rows")
        
        # Example 2: Multiple actions causing repeated evaluation
        start = time.time()
        
        filtered = ps_df[ps_df['average_rating'] >= 3.0]
        
        # These each trigger separate evaluations of the chain
        count1 = len(filtered)
        max_rating = filtered['average_rating'].max()
        min_rating = filtered['average_rating'].min()
        
        duration = time.time() - start
        print(f"Multiple evaluations: {duration:.2f} seconds")
        
        # Better approach - cache intermediate results
        start = time.time()
        
        filtered = ps_df[ps_df['average_rating'] >= 3.0]
        filtered.spark.cache()  # Cache the result
        
        # Now these use the cached result
        count2 = len(filtered)
        max_rating = filtered['average_rating'].max()
        min_rating = filtered['average_rating'].min()
        
        # Clean up
        filtered.spark.unpersist()
        
        duration = time.time() - start
        print(f"Cached evaluations: {duration:.2f} seconds")
    
    misunderstanding_laziness()
    
    # Pitfall 5: Not optimizing groupby operations
    print("\nPitfall 5: Not optimizing groupby operations")
    
    def groupby_pitfalls():
        print("Naive groupby operation:")
        start = time.time()
        
        # Can be inefficient with many groups
        result = ps_df.groupby('cuisine_type').agg({
            'average_rating': 'mean',
            'num_reviews': 'sum'
        })
        
        count = len(result)
        duration = time.time() - start
        print(f"Basic groupby: {duration:.2f} seconds, {count} groups")
        
        print("\nOptimized groupby:")
        start = time.time()
        
        # Use Spark native operations for better performance
        spark_df = ps_df.to_spark()
        
        from pyspark.sql.functions import avg, sum as spark_sum, count
        
        result_spark = spark_df.groupBy('cuisine_type').agg(
            avg('average_rating').alias('avg_rating'),
            spark_sum('num_reviews').alias('total_reviews')
        )
        
        # Convert back if needed
        result_ps = result_spark.pandas_api()
        
        count = len(result_ps)
        duration = time.time() - start
        print(f"Spark optimized groupby: {duration:.2f} seconds, {count} groups")
    
    groupby_pitfalls()
    
    # Clean up
    spark.stop()
```

## 4. Interoperability with the Pandas Ecosystem

```python
# app/interoperability.py
import pandas as pd
import pyspark.pandas as ps
import numpy as np
import matplotlib.pyplot as plt
from pyspark.sql import SparkSession
import tempfile
import os

def demonstrate_interoperability():
    """Show interoperability between Pandas API and ecosystem"""
    spark = SparkSession.builder \
        .appName("PandasAPI-Interoperability") \
        .getOrCreate()
    
    # Load restaurant data
    ps_df = ps.read_json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl", lines=True)
    
    # 1. Integration with NumPy
    print("NumPy Integration:")
    
    # Convert Pandas API column to NumPy array
    if len(ps_df) > 0:
        ratings_numpy = ps_df['average_rating'].to_numpy()
        print(f"NumPy array stats: mean={np.mean(ratings_numpy):.2f}, std={np.std(ratings_numpy):.2f}")
    
    # 2. Integration with Matplotlib
    print("\nMatplotlib Integration:")
    
    # For visualization, we typically need to convert to Pandas
    # This is reasonable for aggregated results that are small
    cuisine_counts = ps_df.groupby('cuisine_type').size().reset_index(name='count')
    
    # Only get top 5 for visualization
    top_cuisines = cuisine_counts.sort_values('count', ascending=False).head(5)
    
    # Convert to pandas for plotting
    top_cuisines_pd = top_cuisines.to_pandas()
    
    print(f"Top cuisines:\n{top_cuisines_pd}")
    print("(For plotting, we'd use matplotlib with the pandas DataFrame)")
    
    # 3. Working with external file formats
    print("\nExternal File Formats:")
    
    # Pandas API supports many formats
    with tempfile.TemporaryDirectory() as temp_dir:
        # Write to CSV
        csv_path = os.path.join(temp_dir, "restaurants.csv")
        ps_df.to_csv(csv_path)
        print(f"Wrote CSV file: {os.path.getsize(csv_path)} bytes")
        
        # Read CSV with Pandas API
        ps_df_csv = ps.read_csv(csv_path)
        print(f"Read {len(ps_df_csv)} rows from CSV")
        
        # Write to Parquet (efficient columnar format)
        parquet_path = os.path.join(temp_dir, "restaurants.parquet")
        ps_df.to_parquet(parquet_path)
        print(f"Wrote Parquet file")
        
        # Read Parquet with Pandas API
        ps_df_parquet = ps.read_parquet(parquet_path)
        print(f"Read {len(ps_df_parquet)} rows from Parquet")
    
    # 4. Integration with scikit-learn (for small data)
    print("\nScikit-learn Integration:")
    
    try:
        from sklearn.preprocessing import StandardScaler
        
        # Extract numerical columns
        numerical_cols = ['average_rating', 'num_reviews']
        ps_numerical = ps_df[numerical_cols]
        
        # Convert to pandas for sklearn
        pd_numerical = ps_numerical.to_pandas()
        
        # Apply scaling
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(pd_numerical)
        
        print(f"Scaled data stats: mean={scaled_data.mean():.2f}, std={scaled_data.std():.2f}")
    except ImportError:
        print("scikit-learn not installed. Skipping example.")
    
    # 5. Advanced: Combining Pandas and Pandas API workflows
    print("\nCombining Pandas and Pandas API:")
    
    # Scenario: Use Pandas API for big data preprocessing, 
    # then Pandas for final transformations
    
    # Process with Pandas API (distributed)
    city_stats = ps_df.groupby('city').agg({
        'restaurant_id': 'count',
        'average_rating': 'mean',
        'num_reviews': 'sum'
    }).reset_index()
    
    city_stats = city_stats.rename(columns={
        'restaurant_id': 'restaurant_count',
        'average_rating': 'avg_rating',
        'num_reviews': 'total_reviews'
    })
    
    # Convert to pandas for final processing
    pd_city_stats = city_stats.to_pandas()
    
    # Now we can use regular pandas
    if len(pd_city_stats) > 0:
        # Add derived columns
        pd_city_stats['reviews_per_restaurant'] = (
            pd_city_stats['total_reviews'] / pd_city_stats['restaurant_count']
        )
        
        # Sort by average rating
        pd_city_stats = pd_city_stats.sort_values('avg_rating', ascending=False)
        
        print("Top cities by rating:")
        print(pd_city_stats.head(3))
    
    # Clean up
    spark.stop()
```

## 5. Ideal Use Cases for Pandas API on Spark

```python
# app/ideal_use_cases.py
import pandas as pd
import pyspark.pandas as ps
from pyspark.sql import SparkSession
import time

def demonstrate_ideal_use_cases():
    """Show ideal scenarios for using Pandas API on Spark"""
    spark = SparkSession.builder \
        .appName("PandasAPI-IdealUseCases") \
        .getOrCreate()
    
    print("Ideal Use Case 1: Pandas code migration to big data")
    
    # Original pandas code (simplified)
    def pandas_workflow(data_path):
        # This is what data scientists might already have
        pd_df = pd.read_json(data_path, lines=True)
        
        # Typical pandas operations
        pd_df['rating_category'] = pd_df['average_rating'].apply(
            lambda x: 'High' if x >= 4.0 else 'Medium' if x >= 3.0 else 'Low'
        )
        
        cuisine_stats = pd_df.groupby('cuisine_type').agg({
            'restaurant_id': 'count',
            'average_rating': 'mean'
        }).reset_index()
        
        cuisine_stats.columns = ['cuisine_type', 'restaurant_count', 'avg_rating']
        
        top_cuisines = cuisine_stats.sort_values('restaurant_count', ascending=False).head(5)
        
        return top_cuisines
    
    # Migrated to Pandas API for scale
    def pandas_api_workflow(data_path):
        # Almost identical code, but scales to big data
        ps_df = ps.read_json(data_path, lines=True)
        
        # Same operations
        ps_df['rating_category'] = ps_df['average_rating'].apply(
            lambda x: 'High' if x >= 4.0 else 'Medium' if x >= 3.0 else 'Low'
        )
        
        cuisine_stats = ps_df.groupby('cuisine_type').agg({
            'restaurant_id': 'count',
            'average_rating': 'mean'
        }).reset_index()
        
        cuisine_stats.columns = ['cuisine_type', 'restaurant_count', 'avg_rating']
        
        top_cuisines = cuisine_stats.sort_values('restaurant_count', ascending=False).head(5)
        
        return top_cuisines
    
    # Run both workflows
    data_path = "./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl"
    
    print("Running with pandas:")
    start = time.time()
    pd_result = pandas_workflow(data_path)
    pd_duration = time.time() - start
    print(f"Pandas duration: {pd_duration:.2f} seconds")
    print(pd_result)
    
    print("\nRunning with Pandas API on Spark:")
    start = time.time()
    ps_result = pandas_api_workflow(data_path)
    ps_duration = time.time() - start
    print(f"Pandas API duration: {ps_duration:.2f} seconds")
    print(ps_result.to_pandas())
    
    print("\nIdeal Use Case 2: Data transformation pipeline for large datasets")
    
    # Pipeline that would be impractical with pandas alone
    def large_data_pipeline():
        # In a real scenario, this could be TB of data
        ps_df = ps.read_json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl", lines=True)
        ps_users = ps.read_json("./storage/mongodb/users/01JS4W5A7WWZBQ6Y1C465EYR76.jsonl", lines=True)
        ps_orders = ps.read_json("./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl", lines=True)
        
        # Data cleaning
        ps_df = ps_df.fillna({
            'cuisine_type': 'Unknown',
            'average_rating': 0.0,
            'num_reviews': 0
        })
        
        # Feature engineering
        ps_df['is_highly_rated'] = ps_df['average_rating'] >= 4.0
        ps_df['has_many_reviews'] = ps_df['num_reviews'] > 100
        
        # Preprocessing for machine learning
        features = ps_df[['average_rating', 'num_reviews', 'is_highly_rated', 'has_many_reviews']]
        
        # Show it works with familiar pandas operations
        print(f"Processed {len(ps_df)} restaurants")
        print(f"Created {len(features.columns)} features")
        
        return features
    
    large_data_pipeline()
    
    print("\nIdeal Use Case 3: Combined pandas and Spark ML workflow")
    
    def ml_workflow():
        # Prepare data with Pandas API
        ps_df = ps.read_json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl", lines=True)
        
        # Feature engineering
        ps_df['reviews_per_rating'] = ps_df['num_reviews'] / ps_df['average_rating'].clip(lower=0.1)
        
        # Switch to Spark for ML
        spark_df = ps_df.to_spark()
        
        # Use Spark ML library
        from pyspark.ml.feature import VectorAssembler
        from pyspark.ml.regression import LinearRegression
        
        # Prepare features for ML
        feature_cols = ['average_rating', 'num_reviews', 'reviews_per_rating']
        assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
        vector_df = assembler.transform(spark_df)
        
        print(f"Prepared ML features using Pandas API and Spark ML")
        
        return vector_df
    
    ml_workflow()
    
    # Clean up
    spark.stop()
```

## 6. Debugging and Performance Analysis

```python
# app/debugging.py
import pyspark.pandas as ps
from pyspark.sql import SparkSession
import time

def demonstrate_debugging():
    """Show debugging and performance analysis for Pandas API"""
    spark = SparkSession.builder \
        .appName("PandasAPI-Debugging") \
        .getOrCreate()
    
    # Enable Spark UI for detailed performance monitoring
    # Access at http://localhost:4040 when running
    
    # Load data
    ps_df = ps.read_json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl", lines=True)
    
    # 1. Basic debugging technique: print and describe
    print("1. Basic inspection:")
    print(f"DataFrame shape: {ps_df.shape}")
    print("DataFrame info:")
    print(ps_df.dtypes)
    
    # 2. Understanding computation with explain
    print("\n2. Explaining the computation plan:")
    # Convert to Spark to use explain
    ps_df.to_spark().explain()
    
    # More complex operation
    result = ps_df.groupby('cuisine_type').agg({'average_rating': 'mean'})
    print("\nGroupBy operation plan:")
    result.to_spark().explain()
    
    # 3. Find performance bottlenecks
    print("\n3. Identifying performance bottlenecks:")
    
    def measure_time(operation_name, operation_func):
        start = time.time()
        result = operation_func()
        duration = time.time() - start
        print(f"{operation_name}: {duration:.2f} seconds")
        return result
    
    # Check various operations
    measure_time("Filtering", lambda: ps_df[ps_df['average_rating'] > 4.0])
    measure_time("GroupBy", lambda: ps_df.groupby('cuisine_type').agg({'average_rating': 'mean'}))
    measure_time("Sorting", lambda: ps_df.sort_values('average_rating', ascending=False))
    
    # 4. Memory usage analysis
    print("\n4. Memory usage analysis:")
    
    # Convert to Spark to check partitioning
    spark_df = ps_df.to_spark()
    partitions = spark_df.rdd.getNumPartitions()
    print(f"Number of partitions: {partitions}")
    
    # Estimated size
    size_bytes = spark_df.rdd.map(lambda x: len(str(x))).sum()
    size_mb = size_bytes / (1024 * 1024)
    print(f"Estimated data size: {size_mb:.2f} MB")
    
    # 5. Debugging with Spark UI
    print("\n5. Detailed debugging with Spark UI:")
    print("Launch the Spark UI at http://localhost:4040 when running locally")
    
    # Trigger some operations for the Spark UI to show
    result = measure_time("Complex operation", lambda: 
        ps_df.join(
            ps_df.groupby('cuisine_type').agg({'average_rating': 'mean'}).reset_index(),
            on='cuisine_type'
        )
    )
    
    # Keep the application running to examine Spark UI
    print("\nCheck Spark UI now for detailed metrics...")
    
    # Clean up
    spark.stop()
```

## 7. Key Takeaways and Best Practices

### When to Use Pandas API on Spark

1. **Migrating existing pandas code** to handle larger datasets
2. **Data scientists transitioning** to big data workflows
3. **ETL pipelines** that need familiar pandas syntax but with Spark's scale
4. **Exploratory data analysis** on large datasets

### When to Avoid Pandas API on Spark

1. **Row-by-row operations** that don't translate well to distributed computing
2. **Heavy index operations** which can be inefficient
3. **Very small datasets** where pandas alone would be faster
4. **Operations requiring exact pandas behavior** with nuanced differences

### Best Practices Summary

1. **Understand the Translation**: Know how your pandas code translates to Spark operations
2. **Strategic Switching**: Convert to Spark DataFrames for complex operations
3. **Avoid collect() on Large Data**: Never convert large datasets to pandas
4. **Caching**: Cache intermediate results that are used multiple times
5. **Optimize Partitioning**: Adjust partition count based on data size
6. **Vectorize Operations**: Use built-in functions over row-wise operations
7. **Understand Laziness**: Be aware of when operations actually execute
8. **Monitor Performance**: Use the Spark UI to identify bottlenecks

## 8. Conclusion

The Pandas API on Spark (formerly Koalas) bridges the gap between pandas simplicity and Spark's scalability. By following these best practices, you can write pandas-like code that efficiently processes large datasets while avoiding common performance pitfalls.

For the best experience, combine the strengths of both worlds: use Pandas API for its familiar syntax and easy transition, but don't hesitate to drop down to native Spark operations when you need maximum performance or functionality not available in the Pandas API.
