# Pandas API on Spark Foundations - Part 5: Data Delivery

## Introduction

After processing data with Pandas API on Spark, the final crucial step is delivering the processed data for downstream analysis or integration with other systems. This lesson covers how to export data to various formats, convert to native pandas for visualization, and implement strategies for large datasets.

## Setting Up Our Environment

Let's set up our environment and load our processed UberEats datasets:

```python
# Import libraries
import pyspark.pandas as ps
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import time

# For regular PySpark
from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder \
    .appName("Pandas API Data Delivery") \
    .master("local[*]") \
    .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
    .getOrCreate()

# Load our datasets
restaurants_ps = ps.read_json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
orders_ps = ps.read_json("./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl")

# Prepare some processed data for delivery examples
# Process restaurants data to create a simple analytics dataset
restaurant_analytics = restaurants_ps[["name", "cuisine_type", "city", "country", "average_rating", "num_reviews"]]
restaurant_analytics["popularity_score"] = (restaurant_analytics["average_rating"] * 
                                          np.sqrt(restaurant_analytics["num_reviews"] / 100))

# Process orders data
orders_analytics = orders_ps.rename(columns={"restaurant_key": "cnpj"})
orders_analytics = orders_analytics.merge(
    restaurants_ps[["cnpj", "name", "cuisine_type"]],
    on="cnpj",
    how="inner"
)

# Show sample data
print("Restaurant Analytics Data Sample:")
print(restaurant_analytics.head(3))
print("\nOrders Analytics Data Sample:")
print(orders_analytics.head(3))
```

## 1. Exporting to Different Formats

Pandas API on Spark supports exporting data to various formats, similar to native pandas but with distributed capabilities.

### Basic File Formats

```python
# Define output directory
output_dir = "./storage/output"
os.makedirs(output_dir, exist_ok=True)

# Export to CSV
restaurant_analytics.to_csv(f"{output_dir}/restaurant_analytics.csv", index=False)
print(f"Data exported to CSV: {output_dir}/restaurant_analytics.csv")

# Export to JSON (orient='records' for line-delimited JSON)
restaurant_analytics.to_json(f"{output_dir}/restaurant_analytics.json", orient="records", lines=True)
print(f"Data exported to JSON: {output_dir}/restaurant_analytics.json")

# Export to Parquet
restaurant_analytics.to_parquet(f"{output_dir}/restaurant_analytics.parquet")
print(f"Data exported to Parquet: {output_dir}/restaurant_analytics.parquet")

# Export to Excel (requires openpyxl)
# For large datasets, this is best done with a small sample
restaurant_sample = restaurant_analytics.head(100)
restaurant_sample.to_pandas().to_excel(f"{output_dir}/restaurant_analytics_sample.xlsx", index=False)
print(f"Sample data exported to Excel: {output_dir}/restaurant_analytics_sample.xlsx")
```

### Controlling Output Size and Format

```python
# Export with compression
restaurant_analytics.to_csv(
    f"{output_dir}/restaurant_analytics_compressed.csv.gz", 
    compression="gzip",
    index=False
)
print(f"Compressed CSV exported: {output_dir}/restaurant_analytics_compressed.csv.gz")

# Export with specific partitioning (useful for large datasets)
restaurant_analytics.to_parquet(
    f"{output_dir}/restaurant_analytics_partitioned", 
    partition_cols=["cuisine_type"]
)
print(f"Partitioned Parquet exported: {output_dir}/restaurant_analytics_partitioned")

# Export a subset of columns
restaurant_analytics[["name", "cuisine_type", "average_rating", "popularity_score"]].to_csv(
    f"{output_dir}/restaurant_ratings.csv",
    index=False
)
print(f"Subset CSV exported: {output_dir}/restaurant_ratings.csv")
```

## 2. Converting to Native Pandas for Analysis

For certain analyses, especially visualizations, converting to native pandas is often necessary.

### Converting Small Datasets

```python
# Convert a small dataset to native pandas
# IMPORTANT: Only do this for reasonably sized data that fits in memory
top_restaurants = restaurant_analytics.nlargest(20, "popularity_score")
top_restaurants_pd = top_restaurants.to_pandas()

print(f"\nConverted to native pandas: {type(top_restaurants_pd)}")
print(f"Memory usage: {top_restaurants_pd.memory_usage().sum() / 1024:.2f} KB")

# Now you can use native pandas features
# For example, advanced indexing
indexed_restaurants = top_restaurants_pd.set_index("name")
print("\nIndexed by restaurant name:")
print(indexed_restaurants.head(3))
```

### Safely Handling Larger Datasets

```python
# For larger datasets, process in chunks
def process_in_chunks(ps_df, chunk_size=1000):
    """Process a large Pandas API on Spark DataFrame in smaller chunks."""
    total_rows = len(ps_df)
    chunks_needed = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division
    
    results = []
    
    for i in range(chunks_needed):
        start_idx = i * chunk_size
        end_idx = min((i + 1) * chunk_size, total_rows)
        
        print(f"Processing chunk {i+1}/{chunks_needed} (rows {start_idx}-{end_idx})")
        
        # Get chunk as native pandas
        chunk = ps_df.iloc[start_idx:end_idx].to_pandas()
        
        # Process the chunk (example: calculate statistics)
        chunk_result = {
            'chunk_id': i,
            'row_count': len(chunk),
            'avg_rating': chunk['average_rating'].mean() if 'average_rating' in chunk.columns else None,
            'max_score': chunk['popularity_score'].max() if 'popularity_score' in chunk.columns else None
        }
        
        results.append(chunk_result)
    
    # Combine results
    return pd.DataFrame(results)

# Demonstrate with our dataset (small enough for demonstration)
chunk_analysis = process_in_chunks(restaurant_analytics, chunk_size=10)
print("\nChunk Analysis Results:")
print(chunk_analysis)
```

## 3. Integration with Visualization Tools

Pandas API on Spark can be integrated with visualization libraries, typically by converting to native pandas.

### Basic Visualizations with Matplotlib/Seaborn

```python
# For demonstration, we'll use a small sample
# In real applications, you'd typically aggregate first, then visualize

# Get top cuisines by average rating
cuisine_stats = restaurant_analytics.groupby("cuisine_type").agg({
    "average_rating": "mean",
    "num_reviews": "sum",
    "popularity_score": "mean"
}).reset_index()

# Convert to pandas for visualization
cuisine_stats_pd = cuisine_stats.to_pandas()

# Sort by popularity
cuisine_stats_pd = cuisine_stats_pd.sort_values("popularity_score", ascending=False).head(10)

# Create a simple bar chart
plt.figure(figsize=(10, 6))
sns.barplot(x="cuisine_type", y="popularity_score", data=cuisine_stats_pd)
plt.title("Top Cuisines by Popularity Score")
plt.xlabel("Cuisine Type")
plt.ylabel("Popularity Score")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()

# Save to file instead of displaying (for non-interactive environments)
plt.savefig(f"{output_dir}/cuisine_popularity.png")
print(f"\nVisualization saved to: {output_dir}/cuisine_popularity.png")

# Another example: Rating distribution
rating_counts = restaurant_analytics.groupby("average_rating").size().reset_index(name="count")
rating_counts_pd = rating_counts.to_pandas()

plt.figure(figsize=(10, 6))
sns.histplot(data=restaurant_analytics.to_pandas(), x="average_rating", bins=20)
plt.title("Distribution of Restaurant Ratings")
plt.xlabel("Average Rating")
plt.ylabel("Count")
plt.savefig(f"{output_dir}/rating_distribution.png")
print(f"Visualization saved to: {output_dir}/rating_distribution.png")
```

### Interactive Dashboards (Conceptual)

```python
# Note: This is a conceptual example for interactive dashboards
# In a Jupyter environment, you would use libraries like Plotly, Bokeh, or Dash

def create_interactive_dashboard(data_ps, output_path):
    """Create an interactive HTML dashboard using Plotly (conceptual)."""
    # Convert to pandas after aggregating
    # Step 1: Aggregate by cuisine type
    cuisine_data = data_ps.groupby("cuisine_type").agg({
        "average_rating": "mean",
        "num_reviews": "sum"
    }).reset_index()
    
    # Step 2: Get top restaurants
    top_restaurants = data_ps.nlargest(20, "popularity_score")
    
    # Step 3: Convert to pandas for visualization
    cuisine_pd = cuisine_data.to_pandas()
    restaurants_pd = top_restaurants.to_pandas()
    
    # Step 4: In a real application, create interactive plots
    # For example with Plotly:
    #
    # import plotly.express as px
    # fig1 = px.bar(cuisine_pd, x="cuisine_type", y="average_rating", 
    #              title="Average Rating by Cuisine Type")
    # fig2 = px.scatter(restaurants_pd, x="average_rating", y="num_reviews",
    #                  size="popularity_score", color="cuisine_type", 
    #                  hover_name="name", title="Restaurant Rating vs Popularity")
    # 
    # Step 5: Combine into a dashboard
    # from dash import Dash, html, dcc
    # app = Dash(__name__)
    # app.layout = html.Div([
    #     html.H1("UberEats Restaurant Analytics"),
    #     dcc.Graph(figure=fig1),
    #     dcc.Graph(figure=fig2)
    # ])
    #
    # Step 6: Save to HTML
    # app.write_html(f"{output_path}/dashboard.html")
    
    # For this example, we'll just note the concept
    print(f"\nConcept: Interactive dashboard would be saved to {output_path}/dashboard.html")
    print("In a real application, you would use Plotly, Bokeh, or a similar library for interactive visualizations.")

# Demonstrate the concept
create_interactive_dashboard(restaurant_analytics, output_dir)
```

## 4. Strategies for Large Datasets

When dealing with large datasets, you need specific strategies to efficiently deliver the data.

### Partitioning by Key Columns

```python
# Partitioning data by key columns for efficient downstream processing
def export_partitioned_data(df_ps, output_path, partition_cols):
    """Export data with appropriate partitioning for large datasets."""
    # Make sure the directory exists
    os.makedirs(output_path, exist_ok=True)
    
    # Check if partitioning is reasonable
    for col in partition_cols:
        if col not in df_ps.columns:
            raise ValueError(f"Partition column '{col}' not found in DataFrame")
        
        # Check cardinality (number of unique values)
        unique_count = df_ps[col].nunique()
        print(f"Column '{col}' has {unique_count} unique values")
        
        if unique_count > 100:
            print(f"Warning: Column '{col}' has high cardinality ({unique_count} values)!")
            print("This may result in too many small partitions.")
    
    # Export with partitioning
    print(f"Exporting data to {output_path}, partitioned by {partition_cols}...")
    df_ps.to_parquet(output_path, partition_cols=partition_cols)
    
    # Report success
    print(f"Data successfully exported to {output_path}")
    
    # Return information about the export
    return {
        "path": output_path,
        "partition_columns": partition_cols,
        "total_rows": len(df_ps)
    }

# Demonstrate with restaurant analytics data
export_info = export_partitioned_data(
    restaurant_analytics,
    f"{output_dir}/restaurant_analytics_by_location",
    partition_cols=["country", "cuisine_type"]
)

print("\nExport Information:")
print(export_info)
```

### Incremental Data Delivery

```python
# For time-series data, incremental export by date ranges
def export_incremental_data(df_ps, output_path, date_column, mode="overwrite"):
    """Export data incrementally by date ranges."""
    # Ensure the date column exists
    if date_column not in df_ps.columns:
        raise ValueError(f"Date column '{date_column}' not found in DataFrame")
    
    # Try to convert to datetime if not already
    if not pd.api.types.is_datetime64_dtype(df_ps[date_column]):
        try:
            df_ps[date_column] = pd.to_datetime(df_ps[date_column])
        except:
            raise ValueError(f"Could not convert '{date_column}' to datetime")
    
    # Extract date components
    df_ps = df_ps.assign(
        year=df_ps[date_column].dt.year,
        month=df_ps[date_column].dt.month,
        day=df_ps[date_column].dt.day
    )
    
    # Export with partitioning by date components
    df_ps.to_parquet(
        output_path,
        partition_cols=["year", "month", "day"],
        mode=mode
    )
    
    print(f"Data incrementally exported to {output_path}")
    
    # In a real application, you'd record the last exported date
    max_date = df_ps[date_column].max()
    return {
        "path": output_path,
        "last_exported_date": max_date,
        "total_rows": len(df_ps)
    }

# Convert order_date to datetime for demonstration
if 'order_date' in orders_analytics.columns:
    orders_analytics['order_date'] = pd.to_datetime(orders_analytics['order_date'])
    
    # Demonstrate incremental export
    incremental_info = export_incremental_data(
        orders_analytics,
        f"{output_dir}/orders_incremental",
        date_column="order_date"
    )
    
    print("\nIncremental Export Information:")
    print(incremental_info)
else:
    print("\nSkipping incremental export demo (order_date column not found)")
```

### Optimizing for Specific Consumers

```python
# Export optimized for different downstream consumers
def export_for_consumers(df_ps, output_base_path):
    """Export data optimized for different consumers."""
    results = {}
    
    # 1. Data scientists: Parquet with full detail
    data_science_path = f"{output_base_path}/data_science"
    df_ps.to_parquet(data_science_path)
    results["data_science"] = data_science_path
    
    # 2. Business analysts: Aggregated CSV
    business_path = f"{output_base_path}/business"
    os.makedirs(business_path, exist_ok=True)
    
    # Create aggregated view
    if "cuisine_type" in df_ps.columns and "average_rating" in df_ps.columns:
        # Aggregate by cuisine
        agg_by_cuisine = df_ps.groupby("cuisine_type").agg({
            "average_rating": "mean",
            "num_reviews": "sum",
            "popularity_score": "mean",
            "name": "count"
        }).reset_index()
        
        agg_by_cuisine.columns = [
            "cuisine_type", "avg_rating", "total_reviews", 
            "avg_popularity", "restaurant_count"
        ]
        
        # Export as CSV for easy access
        agg_by_cuisine.to_csv(f"{business_path}/cuisine_summary.csv", index=False)
        results["business_cuisine"] = f"{business_path}/cuisine_summary.csv"
    
    # 3. Reporting: Filtered Excel with key metrics
    report_path = f"{output_base_path}/reporting"
    os.makedirs(report_path, exist_ok=True)
    
    # Get top performers in each cuisine
    if "cuisine_type" in df_ps.columns and "popularity_score" in df_ps.columns:
        # Get top 5 in each cuisine
        top_by_cuisine = df_ps.sort_values("popularity_score", ascending=False).groupby("cuisine_type").head(5)
        
        # Create executive summary
        summary = top_by_cuisine[["name", "cuisine_type", "average_rating", "num_reviews", "popularity_score"]]
        
        # Convert to pandas for Excel export
        summary.to_pandas().to_excel(f"{report_path}/top_restaurants.xlsx", index=False)
        results["reporting"] = f"{report_path}/top_restaurants.xlsx"
    
    return results

# Execute the function
consumer_paths = export_for_consumers(restaurant_analytics, f"{output_dir}/by_consumer")
print("\nExports for Different Consumers:")
for consumer, path in consumer_paths.items():
    print(f"- {consumer}: {path}")
```

## 5. Practical Application: Complete Data Delivery Pipeline

Let's build a comprehensive data delivery pipeline that combines the techniques we've learned:

```python
def complete_delivery_pipeline(restaurants_ps, orders_ps, output_dir):
    """
    End-to-end data delivery pipeline for UberEats analytics
    """
    # Create output directory structure
    base_dir = f"{output_dir}/ubereats_analytics"
    os.makedirs(base_dir, exist_ok=True)
    
    # Step 1: Prepare Analytics Datasets
    print("Step 1: Preparing analytics datasets...")
    
    # Restaurant analytics with popularity scores
    restaurant_analytics = restaurants_ps.copy()
    restaurant_analytics["popularity_score"] = (
        restaurant_analytics["average_rating"] * 
        np.sqrt(restaurant_analytics["num_reviews"] / 100)
    )
    
    # Join orders with restaurant data
    orders_analysis = orders_ps.rename(columns={"restaurant_key": "cnpj"})
    orders_analysis = orders_analysis.merge(
        restaurants_ps[["cnpj", "name", "cuisine_type"]],
        on="cnpj",
        how="inner"
    )
    
    # Convert order_date to datetime if needed
    if "order_date" in orders_analysis.columns and not pd.api.types.is_datetime64_dtype(orders_analysis["order_date"]):
        orders_analysis["order_date"] = pd.to_datetime(orders_analysis["order_date"])
    
    # Step 2: Create Summary Analytics
    print("Step 2: Creating summary analytics...")
    
    # Restaurant summary by cuisine
    cuisine_summary = restaurant_analytics.groupby("cuisine_type").agg({
        "average_rating": "mean",
        "num_reviews": "sum",
        "popularity_score": "mean",
        "restaurant_id": "count"
    }).reset_index()
    
    cuisine_summary.columns = [
        "cuisine_type", "avg_rating", "total_reviews", 
        "avg_popularity", "restaurant_count"
    ]
    
    # Order summary by cuisine
    if "order_date" in orders_analysis.columns:
        # Extract date components
        orders_analysis = orders_analysis.assign(
            year=orders_analysis["order_date"].dt.year,
            month=orders_analysis["order_date"].dt.month
        )
        
        # Summarize by cuisine and month
        order_summary = orders_analysis.groupby(["cuisine_type", "year", "month"]).agg({
            "order_id": "count",
            "total_amount": "sum"
        }).reset_index()
        
        order_summary.columns = [
            "cuisine_type", "year", "month", "order_count", "total_revenue"
        ]
    
    # Step 3: Export Data in Various Formats
    print("Step 3: Exporting data in various formats...")
    
    # Restaurant data
    export_paths = {}
    
    # Full datasets as Parquet
    restaurant_analytics.to_parquet(
        f"{base_dir}/restaurants",
        partition_cols=["cuisine_type"]
    )
    export_paths["restaurant_full"] = f"{base_dir}/restaurants"
    
    # Orders data partitioned by time
    if "order_date" in orders_analysis.columns:
        orders_analysis.to_parquet(
            f"{base_dir}/orders",
            partition_cols=["year", "month"]
        )
        export_paths["orders_full"] = f"{base_dir}/orders"
    
    # Summaries as CSV for easy access
    cuisine_summary.to_csv(f"{base_dir}/cuisine_summary.csv", index=False)
    export_paths["cuisine_summary"] = f"{base_dir}/cuisine_summary.csv"
    
    if "order_date" in orders_analysis.columns:
        order_summary.to_csv(f"{base_dir}/order_summary.csv", index=False)
        export_paths["order_summary"] = f"{base_dir}/order_summary.csv"
    
    # Step 4: Create Visualizations for Key Insights
    print("Step 4: Creating visualizations...")
    
    viz_dir = f"{base_dir}/visualizations"
    os.makedirs(viz_dir, exist_ok=True)
    
    # Top cuisines bar chart
    cuisine_summary_pd = cuisine_summary.sort_values("avg_popularity", ascending=False).head(10).to_pandas()
    
    plt.figure(figsize=(10, 6))
    sns.barplot(x="cuisine_type", y="avg_popularity", data=cuisine_summary_pd)
    plt.title("Top Cuisines by Popularity Score")
    plt.xlabel("Cuisine Type")
    plt.ylabel("Popularity Score")
    plt.xticks(rotation=45, ha="right")
    plt.tight_layout()
    plt.savefig(f"{viz_dir}/top_cuisines.png")
    export_paths["visualization_top_cuisines"] = f"{viz_dir}/top_cuisines.png"
    
    # Step 5: Export Samples for Quick Review
    print("Step 5: Exporting samples for quick review...")
    
    # Small samples as Excel for easy viewing
    samples_dir = f"{base_dir}/samples"
    os.makedirs(samples_dir, exist_ok=True)
    
    # Top restaurants
    top_restaurants = restaurant_analytics.nlargest(20, "popularity_score")
    top_restaurants.to_pandas().to_excel(f"{samples_dir}/top_restaurants.xlsx", index=False)
    export_paths["sample_top_restaurants"] = f"{samples_dir}/top_restaurants.xlsx"
    
    # Return the export paths
    print("\nData delivery pipeline completed successfully!")
    return export_paths

# Execute the pipeline
delivery_results = complete_delivery_pipeline(restaurants_ps, orders_ps, output_dir)

print("\nDelivery Pipeline Results:")
for dataset, path in delivery_results.items():
    print(f"- {dataset}: {path}")
```

## Conclusion and Best Practices

In this lesson, we've covered the essential aspects of data delivery with Pandas API on Spark:

1. **Exporting to Different Formats**: Writing data to CSV, JSON, Parquet, and more
2. **Converting to Native Pandas**: Safely working with subsets for visualization and detailed analysis
3. **Integration with Visualization Tools**: Creating charts and dashboards from processed data
4. **Strategies for Large Datasets**: Partitioning, incremental delivery, and consumer-specific optimizations
5. **Complete Delivery Pipeline**: Building an end-to-end process for UberEats analytics

**Best Practices:**

- **Choose the right format for each use case**:
  - Parquet for analytical workloads (columnar, efficient)
  - CSV for compatibility with other tools
  - JSON for API integrations

- **Handle large datasets appropriately**:
  - Use partitioning by commonly filtered columns
  - Process in chunks when converting to native pandas
  - Consider incremental delivery for time-series data

- **Optimize for different consumers**:
  - Data scientists: detailed, optimized formats
  - Analysts: aggregated, easy-to-use formats
  - Executives: visualizations and summaries

- **Balance performance and usability**:
  - Compress data when appropriate
  - Provide both detailed and summarized views
  - Create sample datasets for quick exploration

This completes our Pandas API on Spark Foundations series! You now have a solid understanding of how to ingest, transform, analyze, and deliver data using the familiar Pandas syntax with the power of distributed processing.

## Exercise

1. Create a complete delivery pipeline for the UberEats driver data that:
   - Exports driver performance metrics in multiple formats
   - Creates visualizations of driver performance by vehicle type
   - Implements partitioning by appropriate columns
   - Prepares both detailed and summary views for different audiences

2. Build a data delivery workflow that:
   - Integrates with a custom visualization or dashboard
   - Handles incremental updates efficiently
   - Provides data in multiple formats for different downstream systems
   - Includes validation steps to ensure data quality

## Resources

- [Pandas API on Spark IO Operations](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/io.html)
- [Parquet Format Documentation](https://parquet.apache.org/)
- [Matplotlib Visualization Library](https://matplotlib.org/)
- [Seaborn Statistical Visualization](https://seaborn.pydata.org/)
