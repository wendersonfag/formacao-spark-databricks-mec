# Spark SQL Foundations - Part 3: Advanced Transformation Operations

## Introduction

Building on our foundation of basic SQL operations, we now explore more advanced transformation techniques in Spark SQL. These include complex joins, subqueries, advanced aggregations, and Common Table Expressions (CTEs), which enable sophisticated data analysis.

## Setting Up Our Environment

Let's start by setting up our Spark session and loading the UberEats datasets:

```python
from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder \
    .appName("Spark SQL Advanced Transformations") \
    .master("local[*]") \
    .getOrCreate()

# Read our UberEats datasets and create temporary views
restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
drivers_df = spark.read.json("./storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl")
orders_df = spark.read.json("./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl")
ratings_df = spark.read.json("./storage/mysql/ratings/01JS4W5A7YWTYRQKDA7F7N95VZ.jsonl")

# Create temporary views
restaurants_df.createOrReplaceTempView("restaurants")
drivers_df.createOrReplaceTempView("drivers")
orders_df.createOrReplaceTempView("orders")
ratings_df.createOrReplaceTempView("ratings")

# Display table row counts
print("Dataset sizes:")
for table in ["restaurants", "drivers", "orders", "ratings"]:
    count = spark.sql(f"SELECT COUNT(*) AS count FROM {table}").collect()[0]["count"]
    print(f"- {table}: {count} rows")
```

## 1. Complex JOIN Operations

Joins combine data from multiple tables. Let's explore different join types and complex join scenarios.

### INNER JOIN

```python
# Basic inner join between orders and restaurants
orders_with_restaurants = spark.sql("""
    SELECT 
        o.order_id,
        r.name AS restaurant_name,
        r.cuisine_type,
        o.total_amount
    FROM orders o
    INNER JOIN restaurants r ON o.restaurant_key = r.cnpj
    LIMIT 10
""")

print("Orders with Restaurant Details (INNER JOIN):")
orders_with_restaurants.show()
```

### LEFT, RIGHT, and FULL OUTER JOINs

```python
# LEFT JOIN to keep all restaurants, even those without orders
restaurants_with_orders = spark.sql("""
    SELECT 
        r.name AS restaurant_name,
        r.cuisine_type,
        COUNT(o.order_id) AS order_count,
        COALESCE(SUM(o.total_amount), 0) AS total_revenue
    FROM restaurants r
    LEFT JOIN orders o ON r.cnpj = o.restaurant_key
    GROUP BY r.name, r.cuisine_type
    ORDER BY order_count DESC, total_revenue DESC
    LIMIT 10
""")

print("\nRestaurants with Order Counts (LEFT JOIN):")
restaurants_with_orders.show()

# RIGHT JOIN (keeping all orders, even if no matching restaurant)
all_orders_with_restaurants = spark.sql("""
    SELECT 
        o.order_id,
        o.total_amount,
        r.name AS restaurant_name,
        r.cuisine_type
    FROM restaurants r
    RIGHT JOIN orders o ON r.cnpj = o.restaurant_key
    LIMIT 10
""")

print("\nAll Orders with Restaurant Details (RIGHT JOIN):")
all_orders_with_restaurants.show()

# FULL OUTER JOIN (keeping all records from both tables)
complete_join = spark.sql("""
    SELECT 
        r.name AS restaurant_name,
        o.order_id,
        o.total_amount
    FROM restaurants r
    FULL OUTER JOIN orders o ON r.cnpj = o.restaurant_key
    LIMIT 10
""")

print("\nComplete Join (FULL OUTER JOIN):")
complete_join.show()
```

### Multi-Table JOINs

```python
# Join across more than two tables
full_order_details = spark.sql("""
    SELECT 
        o.order_id,
        r.name AS restaurant_name,
        r.cuisine_type,
        d.first_name || ' ' || d.last_name AS driver_name,
        d.vehicle_type,
        o.total_amount
    FROM orders o
    JOIN restaurants r ON o.restaurant_key = r.cnpj
    JOIN drivers d ON o.driver_key = d.license_number
    ORDER BY o.total_amount DESC
    LIMIT 10
""")

print("\nFull Order Details (Multi-Table JOIN):")
full_order_details.show()
```

### Self JOINs

```python
# Self join example - comparing restaurants of the same cuisine type
# First, create a view with row numbers for the self join
spark.sql("""
    CREATE OR REPLACE TEMPORARY VIEW numbered_restaurants AS
    SELECT 
        ROW_NUMBER() OVER (ORDER BY name) AS id,
        name,
        cuisine_type,
        city,
        average_rating,
        num_reviews
    FROM restaurants
""")

# Self join to find restaurants of the same cuisine with different ratings
similar_restaurants = spark.sql("""
    SELECT 
        a.name AS restaurant_a,
        b.name AS restaurant_b,
        a.cuisine_type,
        a.city,
        a.average_rating AS rating_a,
        b.average_rating AS rating_b,
        ABS(a.average_rating - b.average_rating) AS rating_difference
    FROM numbered_restaurants a
    JOIN numbered_restaurants b ON 
        a.cuisine_type = b.cuisine_type AND
        a.city = b.city AND
        a.id < b.id
    WHERE ABS(a.average_rating - b.average_rating) > 1.0
    ORDER BY rating_difference DESC
    LIMIT 10
""")

print("\nSimilar Restaurants with Different Ratings (SELF JOIN):")
similar_restaurants.show()
```

## 2. Working with Subqueries

Subqueries allow you to nest one query inside another, enabling complex data transformations.

### Subqueries in SELECT

```python
# Using subquery in SELECT clause to calculate ranking
restaurant_rankings = spark.sql("""
    SELECT
        name,
        cuisine_type,
        average_rating,
        num_reviews,
        (SELECT COUNT(*) + 1 
         FROM restaurants r2 
         WHERE r2.average_rating > r1.average_rating) AS rating_rank
    FROM restaurants r1
    ORDER BY rating_rank
    LIMIT 10
""")

print("\nRestaurant Rankings (Subquery in SELECT):")
restaurant_rankings.show()
```

### Subqueries in WHERE

```python
# Using subquery in WHERE clause to filter restaurants
above_average_restaurants = spark.sql("""
    SELECT
        name,
        cuisine_type,
        average_rating
    FROM restaurants
    WHERE average_rating > (
        SELECT AVG(average_rating) 
        FROM restaurants
    )
    ORDER BY average_rating DESC
    LIMIT 10
""")

print("\nAbove Average Restaurants (Subquery in WHERE):")
above_average_restaurants.show()

# More complex filtering with subquery
top_cuisine_restaurants = spark.sql("""
    SELECT
        name,
        cuisine_type,
        city,
        average_rating
    FROM restaurants r
    WHERE (cuisine_type, average_rating) IN (
        SELECT 
            cuisine_type,
            MAX(average_rating) AS max_rating
        FROM restaurants
        GROUP BY cuisine_type
    )
    ORDER BY average_rating DESC
    LIMIT 10
""")

print("\nTop-Rated Restaurant per Cuisine (Subquery with IN):")
top_cuisine_restaurants.show()
```

### Subqueries in FROM

```python
# Using a subquery in FROM clause as a derived table
cuisine_stats = spark.sql("""
    SELECT
        cs.cuisine_type,
        cs.restaurant_count,
        cs.avg_rating,
        COUNT(o.order_id) AS order_count,
        ROUND(SUM(o.total_amount), 2) AS total_revenue
    FROM (
        SELECT
            cuisine_type,
            COUNT(*) AS restaurant_count,
            AVG(average_rating) AS avg_rating,
            COLLECT_LIST(cnpj) AS restaurant_keys
        FROM restaurants
        GROUP BY cuisine_type
    ) cs
    LEFT JOIN orders o ON o.restaurant_key IN (SELECT key FROM UNNEST(cs.restaurant_keys) AS key)
    GROUP BY cs.cuisine_type, cs.restaurant_count, cs.avg_rating
    ORDER BY total_revenue DESC
    LIMIT 10
""")

# Note: The above query might not work directly due to limitations with UNNEST in some Spark versions
# Let's use a simpler approach:

cuisine_stats = spark.sql("""
    SELECT
        cs.cuisine_type,
        cs.restaurant_count,
        cs.avg_rating,
        COUNT(o.order_id) AS order_count,
        ROUND(SUM(COALESCE(o.total_amount, 0)), 2) AS total_revenue
    FROM (
        SELECT
            cuisine_type,
            COUNT(*) AS restaurant_count,
            ROUND(AVG(average_rating), 2) AS avg_rating
        FROM restaurants
        GROUP BY cuisine_type
    ) cs
    LEFT JOIN restaurants r ON r.cuisine_type = cs.cuisine_type
    LEFT JOIN orders o ON o.restaurant_key = r.cnpj
    GROUP BY cs.cuisine_type, cs.restaurant_count, cs.avg_rating
    ORDER BY total_revenue DESC
    LIMIT 10
""")

print("\nCuisine Statistics with Orders (Subquery in FROM):")
cuisine_stats.show()
```

## 3. Advanced Aggregate Functions

Spark SQL provides powerful aggregate functions beyond basic COUNT, SUM, and AVG.

### Statistical Aggregations

```python
# Statistical aggregations
rating_statistics = spark.sql("""
    SELECT
        cuisine_type,
        COUNT(*) AS restaurant_count,
        ROUND(AVG(average_rating), 2) AS mean_rating,
        ROUND(STDDEV(average_rating), 2) AS stddev_rating,
        ROUND(VARIANCE(average_rating), 2) AS variance_rating,
        PERCENTILE(average_rating, 0.5) AS median_rating,
        MIN(average_rating) AS min_rating,
        MAX(average_rating) AS max_rating
    FROM restaurants
    GROUP BY cuisine_type
    ORDER BY restaurant_count DESC
    LIMIT 10
""")

print("\nRating Statistics by Cuisine (Advanced Aggregations):")
rating_statistics.show()
```

### Conditional Aggregations

```python
# Conditional aggregations
rating_breakdown = spark.sql("""
    SELECT
        cuisine_type,
        COUNT(*) AS total_restaurants,
        SUM(CASE WHEN average_rating >= 4.5 THEN 1 ELSE 0 END) AS excellent_count,
        SUM(CASE WHEN average_rating >= 4.0 AND average_rating < 4.5 THEN 1 ELSE 0 END) AS very_good_count,
        SUM(CASE WHEN average_rating >= 3.5 AND average_rating < 4.0 THEN 1 ELSE 0 END) AS good_count,
        SUM(CASE WHEN average_rating >= 3.0 AND average_rating < 3.5 THEN 1 ELSE 0 END) AS average_count,
        SUM(CASE WHEN average_rating < 3.0 THEN 1 ELSE 0 END) AS below_average_count,
        ROUND(AVG(CASE WHEN average_rating >= 4.0 THEN 1.0 ELSE 0.0 END), 2) AS high_rating_percentage
    FROM restaurants
    GROUP BY cuisine_type
    ORDER BY total_restaurants DESC
    LIMIT 10
""")

print("\nRating Breakdown by Cuisine (Conditional Aggregations):")
rating_breakdown.show()
```

### Window Functions

```python
# Window functions for analytics
# Top 3 restaurants per cuisine by rating
top_per_cuisine = spark.sql("""
    SELECT *
    FROM (
        SELECT
            name,
            cuisine_type,
            city,
            average_rating,
            ROW_NUMBER() OVER (PARTITION BY cuisine_type ORDER BY average_rating DESC) AS rank
        FROM restaurants
    ) ranked
    WHERE rank <= 3
    ORDER BY cuisine_type, rank
""")

print("\nTop 3 Restaurants per Cuisine (Window Functions):")
top_per_cuisine.show()

# Calculating running totals
city_running_totals = spark.sql("""
    SELECT
        city,
        COUNT(*) AS restaurant_count,
        SUM(COUNT(*)) OVER (ORDER BY COUNT(*) DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS running_total,
        ROUND(SUM(COUNT(*)) OVER (ORDER BY COUNT(*) DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) / 
              SUM(COUNT(*)) OVER () * 100, 2) AS cumulative_percentage
    FROM restaurants
    GROUP BY city
    ORDER BY restaurant_count DESC
    LIMIT 10
""")

print("\nCity Restaurant Count with Running Totals:")
city_running_totals.show()
```

## 4. Common Table Expressions (CTEs)

CTEs provide a way to create temporary result sets that can be referenced within a query, making complex queries more readable.

### Basic CTE

```python
# Simple CTE example
popular_restaurants_cte = spark.sql("""
    WITH PopularRestaurants AS (
        SELECT
            name,
            cuisine_type,
            city,
            average_rating,
            num_reviews,
            average_rating * SQRT(num_reviews / 1000) AS popularity_score
        FROM restaurants
        WHERE num_reviews > 1000
    )
    SELECT *
    FROM PopularRestaurants
    ORDER BY popularity_score DESC
    LIMIT 10
""")

print("\nPopular Restaurants (Simple CTE):")
popular_restaurants_cte.show()
```

### Multiple CTEs

```python
# Multiple CTEs
multi_cte_analysis = spark.sql("""
    WITH 
    RestaurantMetrics AS (
        SELECT
            r.cnpj,
            r.name,
            r.cuisine_type,
            r.city,
            r.average_rating,
            r.num_reviews,
            r.average_rating * SQRT(r.num_reviews / 1000) AS popularity_score
        FROM restaurants r
    ),
    OrderMetrics AS (
        SELECT
            restaurant_key,
            COUNT(*) AS order_count,
            SUM(total_amount) AS total_revenue,
            AVG(total_amount) AS avg_order_value
        FROM orders
        GROUP BY restaurant_key
    ),
    CombinedMetrics AS (
        SELECT
            rm.name,
            rm.cuisine_type,
            rm.city,
            rm.average_rating,
            rm.popularity_score,
            COALESCE(om.order_count, 0) AS order_count,
            COALESCE(om.total_revenue, 0) AS total_revenue,
            COALESCE(om.avg_order_value, 0) AS avg_order_value
        FROM RestaurantMetrics rm
        LEFT JOIN OrderMetrics om ON rm.cnpj = om.restaurant_key
    )
    SELECT
        cuisine_type,
        COUNT(*) AS restaurant_count,
        ROUND(AVG(average_rating), 2) AS avg_rating,
        ROUND(AVG(popularity_score), 2) AS avg_popularity,
        SUM(order_count) AS total_orders,
        ROUND(SUM(total_revenue), 2) AS total_revenue,
        ROUND(AVG(avg_order_value), 2) AS avg_order_value
    FROM CombinedMetrics
    GROUP BY cuisine_type
    ORDER BY total_revenue DESC
    LIMIT 10
""")

print("\nCuisine Analysis with Multiple CTEs:")
multi_cte_analysis.show()
```

### Recursive CTEs

```python
# Note: Recursive CTEs are supported in recent versions of Spark SQL
# Here's a simple example of a recursive CTE (may not work in all versions)
try:
    # Creating a sequence of numbers for demonstration
    number_sequence = spark.sql("""
        WITH RECURSIVE NumberSequence AS (
            SELECT 1 AS n
            UNION ALL
            SELECT n + 1
            FROM NumberSequence
            WHERE n < 10
        )
        SELECT *
        FROM NumberSequence
    """)
    
    print("\nNumber Sequence (Recursive CTE):")
    number_sequence.show()
except:
    print("\nRecursive CTEs may not be supported in this version of Spark SQL.")
    
    # Alternative approach
    print("Using an alternative approach to generate a sequence:")
    
    # Create a temporary sequence table
    spark.sql("""
        CREATE OR REPLACE TEMPORARY VIEW number_sequence AS
        SELECT id AS n
        FROM (
            SELECT explode(array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)) AS id
        )
    """)
    
    spark.sql("SELECT * FROM number_sequence").show()
```

## 5. Practical Application: Comprehensive UberEats Analysis

Let's combine these advanced techniques to build a comprehensive analysis of our UberEats data:

```python
def comprehensive_ubereats_analysis(spark):
    """
    Perform a comprehensive analysis of UberEats data using advanced SQL techniques.
    Returns a dictionary of DataFrames with various analyses.
    """
    results = {}
    
    # 1. Restaurant Performance Analysis with CTEs and Window Functions
    restaurant_performance = spark.sql("""
        WITH 
        RestaurantOrders AS (
            SELECT
                r.cnpj,
                r.name,
                r.cuisine_type,
                r.city,
                r.average_rating,
                r.num_reviews,
                COUNT(o.order_id) AS order_count,
                COALESCE(SUM(o.total_amount), 0) AS total_revenue,
                COALESCE(AVG(o.total_amount), 0) AS avg_order_value
            FROM restaurants r
            LEFT JOIN orders o ON r.cnpj = o.restaurant_key
            GROUP BY r.cnpj, r.name, r.cuisine_type, r.city, r.average_rating, r.num_reviews
        ),
        RestaurantMetrics AS (
            SELECT
                *,
                average_rating * SQRT(num_reviews / 1000) AS popularity_score,
                total_revenue / NULLIF(order_count, 0) * SQRT(num_reviews / 1000) AS performance_score
            FROM RestaurantOrders
        ),
        RankedRestaurants AS (
            SELECT
                *,
                DENSE_RANK() OVER (PARTITION BY cuisine_type ORDER BY performance_score DESC) AS cuisine_rank,
                DENSE_RANK() OVER (PARTITION BY city ORDER BY performance_score DESC) AS city_rank,
                DENSE_RANK() OVER (ORDER BY performance_score DESC) AS overall_rank
            FROM RestaurantMetrics
        )
        SELECT
            name,
            cuisine_type,
            city,
            average_rating,
            num_reviews,
            order_count,
            ROUND(total_revenue, 2) AS total_revenue,
            ROUND(avg_order_value, 2) AS avg_order_value,
            ROUND(popularity_score, 2) AS popularity_score,
            ROUND(performance_score, 2) AS performance_score,
            cuisine_rank,
            city_rank,
            overall_rank
        FROM RankedRestaurants
        ORDER BY overall_rank
        LIMIT 20
    """)
    results["restaurant_performance"] = restaurant_performance
    
    # 2. Cuisine Analysis with Advanced Aggregations
    cuisine_analysis = spark.sql("""
        WITH OrderData AS (
            SELECT
                r.cuisine_type,
                COUNT(DISTINCT r.restaurant_id) AS restaurant_count,
                COUNT(o.order_id) AS order_count,
                SUM(o.total_amount) AS total_revenue
            FROM restaurants r
            LEFT JOIN orders o ON r.cnpj = o.restaurant_key
            GROUP BY r.cuisine_type
        )
        SELECT
            od.cuisine_type,
            od.restaurant_count,
            ROUND(AVG(r.average_rating), 2) AS avg_rating,
            ROUND(STDDEV(r.average_rating), 2) AS rating_stddev,
            PERCENTILE(r.average_rating, 0.5) AS median_rating,
            SUM(CASE WHEN r.average_rating >= 4.0 THEN 1 ELSE 0 END) AS high_rated_count,
            ROUND(AVG(CASE WHEN r.average_rating >= 4.0 THEN 1.0 ELSE 0.0 END), 2) AS high_rated_percentage,
            od.order_count,
            ROUND(od.total_revenue, 2) AS total_revenue,
            ROUND(od.total_revenue / NULLIF(od.restaurant_count, 0), 2) AS revenue_per_restaurant,
            ROUND(od.order_count / NULLIF(od.restaurant_count, 0), 2) AS orders_per_restaurant
        FROM restaurants r
        JOIN OrderData od ON r.cuisine_type = od.cuisine_type
        GROUP BY od.cuisine_type, od.restaurant_count, od.order_count, od.total_revenue
        ORDER BY total_revenue DESC
    """)
    results["cuisine_analysis"] = cuisine_analysis
    
    # 3. Driver Performance Analysis with Subqueries and Joins
    driver_performance = spark.sql("""
        SELECT
            d.driver_id,
            d.first_name || ' ' || d.last_name AS driver_name,
            d.vehicle_type,
            d.city,
            COUNT(o.order_id) AS delivery_count,
            ROUND(SUM(o.total_amount), 2) AS total_delivery_value,
            ROUND(AVG(o.total_amount), 2) AS avg_delivery_value,
            (
                SELECT COUNT(*) + 1
                FROM (
                    SELECT driver_key, SUM(total_amount) AS driver_total
                    FROM orders
                    GROUP BY driver_key
                ) driver_totals
                WHERE driver_totals.driver_total > SUM(o.total_amount)
            ) AS earnings_rank
        FROM drivers d
        JOIN orders o ON d.license_number = o.driver_key
        GROUP BY d.driver_id, d.first_name, d.last_name, d.vehicle_type, d.city
        ORDER BY total_delivery_value DESC
        LIMIT 20
    """)
    results["driver_performance"] = driver_performance
    
    # 4. City Analysis with Window Functions
    city_analysis = spark.sql("""
        WITH 
        CityMetrics AS (
            SELECT
                r.city,
                COUNT(DISTINCT r.restaurant_id) AS restaurant_count,
                ROUND(AVG(r.average_rating), 2) AS avg_rating,
                COUNT(o.order_id) AS order_count,
                ROUND(SUM(o.total_amount), 2) AS total_revenue,
                COUNT(DISTINCT d.driver_id) AS driver_count
            FROM restaurants r
            LEFT JOIN orders o ON r.cnpj = o.restaurant_key
            LEFT JOIN drivers d ON d.license_number = o.driver_key
            GROUP BY r.city
        ),
        CityRanks AS (
            SELECT
                *,
                RANK() OVER (ORDER BY restaurant_count DESC) AS restaurant_rank,
                RANK() OVER (ORDER BY avg_rating DESC) AS rating_rank,
                RANK() OVER (ORDER BY total_revenue DESC) AS revenue_rank,
                RANK() OVER (ORDER BY driver_count DESC) AS driver_rank
            FROM CityMetrics
        )
        SELECT
            city,
            restaurant_count,
            avg_rating,
            order_count,
            total_revenue,
            driver_count,
            restaurant_rank,
            rating_rank,
            revenue_rank,
            driver_rank,
            (restaurant_rank + rating_rank + revenue_rank + driver_rank) / 4.0 AS overall_performance_score
        FROM CityRanks
        ORDER BY overall_performance_score
        LIMIT 20
    """)
    results["city_analysis"] = city_analysis
    
    return results

# Execute the analysis
analysis = comprehensive_ubereats_analysis(spark)

# Display the results
print("\n=== Comprehensive UberEats Analysis ===\n")

print("Restaurant Performance:")
analysis["restaurant_performance"].show(5)

print("\nCuisine Analysis:")
analysis["cuisine_analysis"].show(5)

print("\nDriver Performance:")
analysis["driver_performance"].show(5)

print("\nCity Analysis:")
analysis["city_analysis"].show(5)
```

## Conclusion and Best Practices

In this lesson, we've explored advanced transformation operations in Spark SQL:

1. **Complex JOINs**: Combining data from multiple tables with different join types
2. **Subqueries**: Using nested queries in SELECT, WHERE, and FROM clauses
3. **Advanced Aggregations**: Leveraging statistical and conditional aggregations
4. **Window Functions**: Performing calculations across related rows
5. **Common Table Expressions (CTEs)**: Improving query readability with temporary result sets

**Best Practices:**

- Choose the appropriate JOIN type based on your data and analysis needs
- Use CTEs to make complex queries more readable and maintainable
- Break down complex logic into subqueries or CTEs for better organization
- Leverage window functions for analytical calculations rather than multiple self-joins
- Optimize performance by filtering early in subqueries and CTEs
- Document complex queries with comments to explain the logic
- Consider query execution plan when working with very complex joins and subqueries
- Test advanced queries on smaller datasets before running on full data

In the next lesson, we'll explore advanced techniques in Spark SQL, including performance optimization and working with complex data types.

## Exercise

1. Create a comprehensive analysis of restaurant ratings:
   - Compare actual ratings with the expected rating based on cuisine type
   - Calculate a "surprise factor" for restaurants that significantly outperform their cuisine average
   - Identify trends in high-performing restaurants across different cuisines and cities
   - Create a ranking system that considers multiple factors (rating, reviews, orders)

2. Build a driver efficiency dashboard:
   - Analyze driver performance based on vehicle type and city
   - Calculate revenue per hour/delivery for different driver segments
   - Identify the most efficient drivers using multiple metrics
   - Compare performance between vehicle types while controlling for other factors

## Resources

- [Spark SQL Joins Documentation](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-join.html)
- [Spark SQL Subquery Documentation](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-subqueries.html)
- [Spark SQL Aggregation Functions](https://spark.apache.org/docs/latest/api/sql/index.html#agg_funcs)
- [Spark SQL Window Functions](https://spark.apache.org/docs/latest/api/sql/index.html#window_funcs)
