# Deep Dive & Best Practices with Spark SQL

This guide explores advanced techniques for optimizing Spark SQL queries, understanding join strategies, and implementing best practices for complex data processing problems.

## 1. Understanding Spark SQL Execution

To optimize queries, first understand how Spark executes them.

```python
# app/sql_execution.py
from pyspark.sql import SparkSession

def explore_execution_plans():
    """Demonstrate how Spark SQL execution plans work"""
    spark = SparkSession.builder.appName("SparkSQL-Execution").getOrCreate()
    
    # Load restaurant data
    restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
    
    # Register view for SQL queries
    restaurants_df.createOrReplaceTempView("restaurants")
    
    # Simple query
    query = """
    SELECT cuisine_type, COUNT(*) as restaurant_count, ROUND(AVG(average_rating), 2) as avg_rating
    FROM restaurants
    GROUP BY cuisine_type
    ORDER BY restaurant_count DESC
    """
    
    print("SQL Query:")
    print(query)
    
    # Examine logical plan
    print("\nLogical Plan:")
    spark.sql(query).explain(mode="formatted")
    
    # Examine physical plan
    print("\nPhysical Plan (including codegen stages):")
    spark.sql(query).explain(mode="codegen")
    
    # Get query metrics using Spark UI
    print("\nExecute query to view metrics in Spark UI (http://localhost:4040):")
    result = spark.sql(query)
    result.show(5)
    
    spark.stop()
```

### Key Components of Execution Plans

1. **Logical Plan**: Represents what operations to perform
   - Unresolved Logical Plan: Initial representation
   - Resolved Logical Plan: After schema resolution
   - Optimized Logical Plan: After optimization rules

2. **Physical Plan**: Represents how to execute the operations
   - Multiple strategies for each logical operation
   - Cost-based optimization for selecting strategies

3. **Execution**: Final compiled code that runs on the cluster

## 2. Query Optimization Techniques

Let's explore key optimization techniques for Spark SQL queries.

```python
# app/sql_optimization.py
from pyspark.sql import SparkSession
import time

def demonstrate_query_optimization():
    """Show key query optimization techniques"""
    spark = SparkSession.builder.appName("SparkSQL-Optimization").getOrCreate()
    
    # Load data
    restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
    users_df = spark.read.json("./storage/mongodb/users/01JS4W5A7WWZBQ6Y1C465EYR76.jsonl")
    orders_df = spark.read.json("./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl")
    
    # Register views
    restaurants_df.createOrReplaceTempView("restaurants")
    users_df.createOrReplaceTempView("users")
    orders_df.createOrReplaceTempView("orders")
    
    # Technique 1: Predicate Pushdown
    print("Technique 1: Predicate Pushdown")
    
    # Inefficient: Late filtering
    def inefficient_filtering():
        start = time.time()
        query = """
        SELECT r.*
        FROM restaurants r
        WHERE r.average_rating > 4.0
        """
        result = spark.sql(query)
        count = result.count()
        duration = time.time() - start
        print(f"Query result: {count} rows in {duration:.2f} seconds")
        # Check the execution plan
        result.explain()
        return result
    
    # Efficient: Push predicates to data source
    def efficient_filtering():
        start = time.time()
        # Enable predicate pushdown
        spark.conf.set("spark.sql.parquet.filterPushdown", "true")
        spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning", "true")
        
        # Same query, but Spark will push the filter to the data source
        query = """
        SELECT r.*
        FROM restaurants r
        WHERE r.average_rating > 4.0
        """
        result = spark.sql(query)
        count = result.count()
        duration = time.time() - start
        print(f"Query result: {count} rows in {duration:.2f} seconds")
        # Check the execution plan
        result.explain()
        return result
    
    inefficient_filtering()
    efficient_filtering()
    
    # Technique 2: Join Optimization
    print("\nTechnique 2: Join Optimization")
    
    # Enable adaptive query execution
    spark.conf.set("spark.sql.adaptive.enabled", "true")
    
    # Example: Order data with user city
    query = """
    -- Create a city dimension for analysis
    WITH city_stats AS (
        SELECT 
            city,
            COUNT(DISTINCT user_id) as user_count
        FROM users
        GROUP BY city
    )
    
    -- Join to get order data by city
    SELECT 
        c.city,
        c.user_count,
        COUNT(o.order_id) as order_count,
        SUM(o.total_amount) as total_amount
    FROM city_stats c
    JOIN users u ON c.city = u.city
    JOIN orders o ON u.user_id = o.user_key
    GROUP BY c.city, c.user_count
    ORDER BY order_count DESC
    """
    
    print(f"Complex join query execution:")
    start = time.time()
    result = spark.sql(query)
    # Trigger execution
    count = result.count()
    duration = time.time() - start
    print(f"Query result: {count} rows in {duration:.2f} seconds")
    # Check the execution plan
    result.explain()
    
    # Technique 3: Rewriting Subqueries
    print("\nTechnique 3: Rewriting Subqueries")
    
    # Inefficient: Correlated subquery
    def inefficient_subquery():
        start = time.time()
        query = """
        SELECT 
            r.cuisine_type,
            r.name,
            r.average_rating,
            (SELECT COUNT(*) FROM restaurants r2 
             WHERE r2.cuisine_type = r.cuisine_type) as cuisine_count
        FROM restaurants r
        WHERE r.average_rating > 4.0
        ORDER BY r.average_rating DESC
        """
        result = spark.sql(query)
        count = result.count()
        duration = time.time() - start
        print(f"Correlated subquery: {count} rows in {duration:.2f} seconds")
        # Check the execution plan
        result.explain()
        return result
    
    # Efficient: Using window functions
    def efficient_window_function():
        start = time.time()
        query = """
        SELECT 
            r.cuisine_type,
            r.name,
            r.average_rating,
            COUNT(*) OVER (PARTITION BY r.cuisine_type) as cuisine_count
        FROM restaurants r
        WHERE r.average_rating > 4.0
        ORDER BY r.average_rating DESC
        """
        result = spark.sql(query)
        count = result.count()
        duration = time.time() - start
        print(f"Window function: {count} rows in {duration:.2f} seconds")
        # Check the execution plan
        result.explain()
        return result
    
    inefficient_subquery()
    efficient_window_function()
    
    # Technique 4: Caching Intermediate Results
    print("\nTechnique 4: Caching Intermediate Results")
    
    # Create and cache a frequently used view
    spark.sql("""
    CREATE OR REPLACE TEMPORARY VIEW high_rated_restaurants AS
    SELECT * FROM restaurants WHERE average_rating > 4.0
    """)
    
    # Cache the view
    spark.sql("CACHE TABLE high_rated_restaurants")
    
    # First query - will cache the data
    print("First query (populates cache):")
    start = time.time()
    spark.sql("SELECT COUNT(*) FROM high_rated_restaurants").show()
    duration1 = time.time() - start
    print(f"Duration: {duration1:.2f} seconds")
    
    # Second query - should use cached data
    print("Second query (uses cache):")
    start = time.time()
    spark.sql("SELECT COUNT(*) FROM high_rated_restaurants").show()
    duration2 = time.time() - start
    print(f"Duration: {duration2:.2f} seconds")
    print(f"Speedup: {duration1/duration2:.2f}x")
    
    # Clean up
    spark.sql("UNCACHE TABLE high_rated_restaurants")
    spark.stop()
```

## 3. Join Strategies Deep Dive

Understanding different join strategies is crucial for optimizing complex queries.

```python
# app/join_strategies.py
from pyspark.sql import SparkSession
import time

def explore_join_strategies():
    """Deep dive into Spark SQL join strategies"""
    spark = SparkSession.builder.appName("SparkSQL-JoinStrategies").getOrCreate()
    
    # Load data
    restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
    users_df = spark.read.json("./storage/mongodb/users/01JS4W5A7WWZBQ6Y1C465EYR76.jsonl")
    
    # Register views
    restaurants_df.createOrReplaceTempView("restaurants")
    users_df.createOrReplaceTempView("users")
    
    print("Exploring different join strategies:")
    
    # 1. Broadcast Join (aka Map-side Join)
    def broadcast_join():
        print("\n1. Broadcast Join Strategy")
        # Explicitly set broadcast threshold to force broadcast
        spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "10m")  # 10MB
        
        query = """
        SELECT 
            u.city,
            COUNT(DISTINCT r.restaurant_id) as restaurant_count,
            COUNT(DISTINCT u.user_id) as user_count
        FROM users u
        JOIN restaurants r ON u.city = r.city
        GROUP BY u.city
        """
        
        start = time.time()
        result = spark.sql(query)
        result.explain()
        count = result.count()
        duration = time.time() - start
        print(f"Broadcast join: {count} rows in {duration:.2f} seconds")
        
        # Reset to default behavior
        spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "10m")  # 10MB
        
        return result
    
    # 2. Shuffle Hash Join
    def shuffle_hash_join():
        print("\n2. Shuffle Hash Join Strategy")
        # Disable broadcast to force shuffle hash join
        spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "-1")
        
        # Force shuffle hash join
        spark.conf.set("spark.sql.join.preferSortMergeJoin", "false")
        
        query = """
        SELECT 
            u.city,
            COUNT(DISTINCT r.restaurant_id) as restaurant_count,
            COUNT(DISTINCT u.user_id) as user_count
        FROM users u
        JOIN restaurants r ON u.city = r.city
        GROUP BY u.city
        """
        
        start = time.time()
        result = spark.sql(query)
        result.explain()
        count = result.count()
        duration = time.time() - start
        print(f"Shuffle hash join: {count} rows in {duration:.2f} seconds")
        
        return result
    
    # 3. Sort Merge Join
    def sort_merge_join():
        print("\n3. Sort Merge Join Strategy")
        # Disable broadcast to force sort merge join
        spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "-1")
        
        # Prefer sort merge join
        spark.conf.set("spark.sql.join.preferSortMergeJoin", "true")
        
        query = """
        SELECT 
            u.city,
            COUNT(DISTINCT r.restaurant_id) as restaurant_count,
            COUNT(DISTINCT u.user_id) as user_count
        FROM users u
        JOIN restaurants r ON u.city = r.city
        GROUP BY u.city
        """
        
        start = time.time()
        result = spark.sql(query)
        result.explain()
        count = result.count()
        duration = time.time() - start
        print(f"Sort merge join: {count} rows in {duration:.2f} seconds")
        
        return result
    
    # 4. Adaptive Join Selection (Spark 3.0+)
    def adaptive_join():
        print("\n4. Adaptive Join Strategy")
        # Reset config to defaults
        spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "10m")
        spark.conf.set("spark.sql.join.preferSortMergeJoin", "true")
        
        # Enable adaptive query execution
        spark.conf.set("spark.sql.adaptive.enabled", "true")
        
        query = """
        SELECT 
            u.city,
            COUNT(DISTINCT r.restaurant_id) as restaurant_count,
            COUNT(DISTINCT u.user_id) as user_count
        FROM users u
        JOIN restaurants r ON u.city = r.city
        GROUP BY u.city
        """
        
        start = time.time()
        result = spark.sql(query)
        result.explain()
        count = result.count()
        duration = time.time() - start
        print(f"Adaptive join: {count} rows in {duration:.2f} seconds")
        
        return result
    
    # Run all strategies for comparison
    broadcast_result = broadcast_join()
    shuffle_hash_result = shuffle_hash_join()
    sort_merge_result = sort_merge_join()
    adaptive_result = adaptive_join()
    
    print("\nJoin Strategy Guidelines:")
    print("1. Broadcast Join: Best when one table is small (<10MB by default)")
    print("2. Shuffle Hash Join: Good for medium-sized tables when columns have high cardinality")
    print("3. Sort Merge Join: Best for large tables, especially with pre-sorted data")
    print("4. Adaptive Join: Let Spark decide based on runtime statistics (recommended)")
    
    # Clean up
    spark.stop()
```

## 4. Complex SQL vs. Programmatic Code

Let's compare SQL and programmatic approaches for complex data transformations.

```python
# app/sql_vs_code.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, avg, round as spark_round
import time

def compare_approaches():
    """Compare SQL and programmatic approaches for the same task"""
    spark = SparkSession.builder.appName("SparkSQL-vs-Code").getOrCreate()
    
    # Load restaurant data
    restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
    users_df = spark.read.json("./storage/mongodb/users/01JS4W5A7WWZBQ6Y1C465EYR76.jsonl")
    
    # Register views for SQL
    restaurants_df.createOrReplaceTempView("restaurants")
    users_df.createOrReplaceTempView("users")
    
    # Task: Get restaurant statistics by cuisine type, with city distribution
    
    # Approach 1: SQL
    def sql_approach():
        print("\nApproach 1: Using SQL")
        start = time.time()
        
        query = """
        WITH cuisine_stats AS (
            SELECT 
                cuisine_type,
                COUNT(*) as restaurant_count,
                ROUND(AVG(average_rating), 2) as avg_rating,
                SUM(num_reviews) as total_reviews
            FROM restaurants
            GROUP BY cuisine_type
        ),
        
        cuisine_cities AS (
            SELECT 
                cuisine_type,
                city,
                COUNT(*) as city_count
            FROM restaurants
            GROUP BY cuisine_type, city
        ),
        
        top_cities AS (
            SELECT 
                cc.cuisine_type,
                COLLECT_LIST(cc.city) as top_cities
            FROM cuisine_cities cc
            JOIN (
                SELECT 
                    cuisine_type,
                    city,
                    DENSE_RANK() OVER (PARTITION BY cuisine_type ORDER BY city_count DESC) as city_rank
                FROM cuisine_cities
            ) ranked ON cc.cuisine_type = ranked.cuisine_type AND cc.city = ranked.city
            WHERE ranked.city_rank <= 3
            GROUP BY cc.cuisine_type
        )
        
        SELECT 
            cs.cuisine_type,
            cs.restaurant_count,
            cs.avg_rating,
            cs.total_reviews,
            tc.top_cities
        FROM cuisine_stats cs
        JOIN top_cities tc ON cs.cuisine_type = tc.cuisine_type
        ORDER BY cs.restaurant_count DESC
        """
        
        result = spark.sql(query)
        result.show(truncate=False)
        
        duration = time.time() - start
        print(f"SQL approach: {duration:.2f} seconds")
        
        return result
    
    # Approach 2: DataFrames API
    def dataframe_approach():
        print("\nApproach 2: Using DataFrame API")
        start = time.time()
        
        # Step 1: Cuisine statistics
        cuisine_stats = restaurants_df.groupBy("cuisine_type").agg(
            count("*").alias("restaurant_count"),
            spark_round(avg("average_rating"), 2).alias("avg_rating"),
            sum("num_reviews").alias("total_reviews")
        )
        
        # Step 2: City counts per cuisine
        from pyspark.sql.functions import dense_rank, desc, collect_list
        from pyspark.sql.window import Window
        
        # Count restaurants by cuisine and city
        city_counts = restaurants_df.groupBy("cuisine_type", "city").count()
        
        # Rank cities within each cuisine
        window_spec = Window.partitionBy("cuisine_type").orderBy(desc("count"))
        ranked_cities = city_counts.withColumn("rank", dense_rank().over(window_spec))
        
        # Keep only top 3 cities per cuisine
        top_cities = ranked_cities.filter(col("rank") <= 3)
        
        # Aggregate cities into lists
        city_lists = top_cities.groupBy("cuisine_type").agg(
            collect_list("city").alias("top_cities")
        )
        
        # Step 3: Join the results
        result = cuisine_stats.join(city_lists, "cuisine_type").orderBy(desc("restaurant_count"))
        
        result.show(truncate=False)
        
        duration = time.time() - start
        print(f"DataFrame approach: {duration:.2f} seconds")
        
        return result
    
    # Run both approaches for comparison
    sql_result = sql_approach()
    df_result = dataframe_approach()
    
    print("\nCriteria for choosing between SQL and programmatic approaches:")
    print("1. Complexity: SQL for simple to moderately complex queries, DataFrames for highly complex logic")
    print("2. Team expertise: SQL for SQL experts, DataFrames for Python developers")
    print("3. Maintainability: SQL for self-documenting analysis, DataFrames for complex ETL")
    print("4. Testability: DataFrames are easier to unit test")
    
    # Clean up
    spark.stop()
```

## 5. Advanced SQL Techniques

Let's explore some advanced SQL techniques in Spark SQL.

```python
# app/advanced_sql.py
from pyspark.sql import SparkSession

def demonstrate_advanced_sql():
    """Show advanced SQL techniques in Spark SQL"""
    spark = SparkSession.builder.appName("SparkSQL-Advanced").getOrCreate()
    
    # Load data
    restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
    users_df = spark.read.json("./storage/mongodb/users/01JS4W5A7WWZBQ6Y1C465EYR76.jsonl")
    orders_df = spark.read.json("./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl")
    
    # Register views
    restaurants_df.createOrReplaceTempView("restaurants")
    users_df.createOrReplaceTempView("users")
    orders_df.createOrReplaceTempView("orders")
    
    # 1. Window Functions
    print("1. Window Functions")
    
    query = """
    SELECT 
        cuisine_type,
        name,
        average_rating,
        RANK() OVER (PARTITION BY cuisine_type ORDER BY average_rating DESC) as rating_rank,
        AVG(average_rating) OVER (PARTITION BY cuisine_type) as cuisine_avg_rating,
        average_rating - AVG(average_rating) OVER (PARTITION BY cuisine_type) as rating_diff
    FROM restaurants
    WHERE num_reviews > 10  -- Filter out restaurants with too few reviews
    ORDER BY cuisine_type, rating_rank
    """
    
    spark.sql(query).show(10)
    
    # 2. Common Table Expressions (CTEs)
    print("\n2. Common Table Expressions (CTEs)")
    
    query = """
    -- Calculate restaurant metrics
    WITH restaurant_metrics AS (
        SELECT 
            cuisine_type,
            COUNT(*) as restaurant_count,
            AVG(average_rating) as avg_rating,
            AVG(num_reviews) as avg_reviews
        FROM restaurants
        GROUP BY cuisine_type
    ),
    
    -- Calculate user metrics
    user_city_metrics AS (
        SELECT 
            city,
            COUNT(*) as user_count
        FROM users
        GROUP BY city
    ),
    
    -- Join restaurants with cities
    restaurant_cities AS (
        SELECT 
            r.cuisine_type,
            r.city,
            COUNT(*) as restaurants_in_city
        FROM restaurants r
        GROUP BY cuisine_type, city
    )
    
    -- Combine all metrics
    SELECT 
        rm.cuisine_type,
        rm.restaurant_count,
        rm.avg_rating,
        rc.city,
        rc.restaurants_in_city,
        ucm.user_count as users_in_city
    FROM restaurant_metrics rm
    JOIN restaurant_cities rc ON rm.cuisine_type = rc.cuisine_type
    JOIN user_city_metrics ucm ON rc.city = ucm.city
    ORDER BY rm.restaurant_count DESC, rc.restaurants_in_city DESC
    """
    
    spark.sql(query).show(10)
    
    # 3. Complex Aggregations and Arrays
    print("\n3. Complex Aggregations and Arrays")
    
    query = """
    SELECT 
        cuisine_type,
        COLLECT_LIST(name) as restaurant_names,
        COLLECT_SET(city) as unique_cities,
        SIZE(COLLECT_SET(city)) as num_cities
    FROM restaurants
    GROUP BY cuisine_type
    ORDER BY num_cities DESC
    """
    
    spark.sql(query).show(truncate=False)
    
    # 4. User-Defined Functions in SQL
    print("\n4. User-Defined Functions in SQL")
    
    # Register a UDF
    spark.udf.register("rating_category", 
                      lambda rating: "Excellent" if rating >= 4.5 else
                                    "Very Good" if rating >= 4.0 else
                                    "Good" if rating >= 3.5 else
                                    "Average" if rating >= 3.0 else
                                    "Below Average")
    
    query = """
    SELECT 
        name,
        cuisine_type,
        average_rating,
        rating_category(average_rating) as category
    FROM restaurants
    ORDER BY average_rating DESC
    """
    
    spark.sql(query).show()
    
    # 5. CUBE and ROLLUP
    print("\n5. CUBE and ROLLUP")
    
    query = """
    -- CUBE: All possible grouping combinations
    SELECT 
        COALESCE(cuisine_type, 'All Cuisines') as cuisine_type,
        COALESCE(city, 'All Cities') as city,
        COUNT(*) as restaurant_count,
        ROUND(AVG(average_rating), 2) as avg_rating
    FROM restaurants
    GROUP BY CUBE(cuisine_type, city)
    ORDER BY restaurant_count DESC
    """
    
    spark.sql(query).show()
    
    query = """
    -- ROLLUP: Hierarchical grouping (cuisine -> city)
    SELECT 
        COALESCE(cuisine_type, 'All Cuisines') as cuisine_type,
        COALESCE(city, 'All Cities') as city,
        COUNT(*) as restaurant_count,
        ROUND(AVG(average_rating), 2) as avg_rating
    FROM restaurants
    GROUP BY ROLLUP(cuisine_type, city)
    ORDER BY cuisine_type, city
    """
    
    spark.sql(query).show()
    
    # Clean up
    spark.stop()
```

## 6. Optimizing Complex Analytic Queries

Let's focus on optimizing a real-world complex analytics pipeline.

```python
# app/optimize_analytics.py
from pyspark.sql import SparkSession
import time

def optimize_analytics_pipeline():
    """Optimize a complex analytics pipeline"""
    spark = SparkSession.builder.appName("SparkSQL-Analytics").getOrCreate()
    
    # Enable adaptive query execution
    spark.conf.set("spark.sql.adaptive.enabled", "true")
    
    # Load data
    print("Loading data...")
    restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
    users_df = spark.read.json("./storage/mongodb/users/01JS4W5A7WWZBQ6Y1C465EYR76.jsonl")
    orders_df = spark.read.json("./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl")
    
    # Register views
    restaurants_df.createOrReplaceTempView("restaurants_raw")
    users_df.createOrReplaceTempView("users_raw")
    orders_df.createOrReplaceTempView("orders_raw")
    
    # Version 1: Original analytics pipeline (suboptimal)
    def original_pipeline():
        print("\nOriginal Pipeline (without optimization)")
        start = time.time()
        
        query = """
        -- Extract restaurant info with details
        WITH restaurant_details AS (
            SELECT 
                restaurant_id,
                name,
                cuisine_type,
                city,
                average_rating,
                num_reviews
            FROM restaurants_raw
        ),
        
        -- Extract user info
        user_details AS (
            SELECT 
                user_id,
                city,
                email
            FROM users_raw
        ),
        
        -- Extract order info
        order_details AS (
            SELECT 
                order_id,
                user_key as user_id,
                restaurant_key as restaurant_id,
                total_amount,
                order_date
            FROM orders_raw
        ),
        
        -- Join users and restaurants by city
        user_restaurant_matches AS (
            SELECT 
                u.user_id,
                r.restaurant_id,
                r.name as restaurant_name,
                r.cuisine_type,
                r.average_rating
            FROM user_details u
            JOIN restaurant_details r ON u.city = r.city
        ),
        
        -- Join with orders
        order_analytics AS (
            SELECT 
                o.order_id,
                o.user_id,
                o.restaurant_id,
                o.total_amount,
                o.order_date,
                urm.restaurant_name,
                urm.cuisine_type,
                urm.average_rating
            FROM order_details o
            LEFT JOIN user_restaurant_matches urm ON 
                o.user_id = urm.user_id AND 
                o.restaurant_id = urm.restaurant_id
        )
        
        -- Final analytics
        SELECT 
            cuisine_type,
            COUNT(DISTINCT restaurant_id) as restaurant_count,
            COUNT(DISTINCT user_id) as user_count,
            COUNT(DISTINCT order_id) as order_count,
            ROUND(AVG(total_amount), 2) as avg_order_amount,
            ROUND(AVG(average_rating), 2) as avg_restaurant_rating
        FROM order_analytics
        GROUP BY cuisine_type
        ORDER BY order_count DESC
        """
        
        result = spark.sql(query)
        result.explain()
        result.show()
        
        duration = time.time() - start
        print(f"Original pipeline duration: {duration:.2f} seconds")
    
    # Version 2: Optimized analytics pipeline
    def optimized_pipeline():
        print("\nOptimized Pipeline")
        start = time.time()
        
        # 1. Define clean views with only needed columns
        spark.sql("""
        CREATE OR REPLACE TEMPORARY VIEW restaurants AS
        SELECT 
            restaurant_id,
            name,
            cuisine_type,
            city,
            average_rating
        FROM restaurants_raw
        """)
        
        spark.sql("""
        CREATE OR REPLACE TEMPORARY VIEW users AS
        SELECT 
            user_id,
            city
        FROM users_raw
        """)
        
        spark.sql("""
        CREATE OR REPLACE TEMPORARY VIEW orders AS
        SELECT 
            order_id,
            user_key as user_id,
            restaurant_key as restaurant_id,
            total_amount
        FROM orders_raw
        """)
        
        # 2. Cache the smaller tables for multiple joins
        spark.sql("CACHE TABLE restaurants")
        spark.sql("CACHE TABLE users")
        
        # 3. Optimized query with better join order
        optimized_query = """
        -- First join smaller tables (restaurant city mapping)
        WITH restaurant_city_mapping AS (
            SELECT 
                restaurant_id,
                name,
                cuisine_type,
                city,
                average_rating
            FROM restaurants
        ),
        
        -- Pre-aggregate restaurant stats by cuisine
        cuisine_stats AS (
            SELECT 
                cuisine_type,
                COUNT(DISTINCT restaurant_id) as restaurant_count,
                ROUND(AVG(average_rating), 2) as avg_rating
            FROM restaurant_city_mapping
            GROUP BY cuisine_type
        ),
        
        -- Join users with their cities (much smaller result than restaurant-city)
        user_city_mapping AS (
            SELECT 
                user_id,
                city
            FROM users
        )
        
        -- Final analytics with better join order
        SELECT 
            cs.cuisine_type,
            cs.restaurant_count,
            COUNT(DISTINCT u.user_id) as user_count,
            COUNT(DISTINCT o.order_id) as order_count,
            ROUND(AVG(o.total_amount), 2) as avg_order_amount,
            cs.avg_rating
        FROM cuisine_stats cs
        JOIN restaurant_city_mapping r ON cs.cuisine_type = r.cuisine_type
        JOIN user_city_mapping u ON r.city = u.city
        LEFT JOIN orders o ON u.user_id = o.user_id AND r.restaurant_id = o.restaurant_id
        GROUP BY cs.cuisine_type, cs.restaurant_count, cs.avg_rating
        ORDER BY order_count DESC
        """
        
        result = spark.sql(optimized_query)
        result.explain()
        result.show()
        
        duration = time.time() - start
        print(f"Optimized pipeline duration: {duration:.2f} seconds")
        
        # Clean up
        spark.sql("UNCACHE TABLE restaurants")
        spark.sql("UNCACHE TABLE users")
    
    # Run both versions for comparison
    original_pipeline()
    optimized_pipeline()
    
    # Key optimizations applied:
    print("\nKey Optimizations Applied:")
    print("1. Column Pruning: Selected only needed columns in views")
    print("2. Strategic Caching: Cached smaller tables used in multiple joins")
    print("3. Join Ordering: Joined smaller tables first")
    print("4. Aggregation Pushdown: Pre-aggregated restaurant stats by cuisine")
    print("5. Query Simplification: Removed unnecessary subqueries")
    
    # Clean up
    spark.stop()
```

## 7. Best Practices for Production SQL

```python
# app/production_sql.py
from pyspark.sql import SparkSession

def demonstrate_production_sql():
    """Show best practices for production SQL in Spark"""
    spark = SparkSession.builder.appName("SparkSQL-Production").getOrCreate()
    
    # Load data
    restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
    
    # Register view
    restaurants_df.createOrReplaceTempView("restaurants")
    
    # 1. Modularize SQL with functions
    def get_sql_query(filter_clause=None, group_by=None, limit=None):
        """Generate a SQL query with customizable components"""
        base_query = """
        SELECT 
            cuisine_type,
            COUNT(*) as restaurant_count,
            ROUND(AVG(average_rating), 2) as avg_rating
        FROM restaurants
        """
        
        # Add filter clause if provided
        if filter_clause:
            base_query += f"\nWHERE {filter_clause}"
        
        # Add group by clause if provided
        if group_by:
            base_query += f"\nGROUP BY {group_by}"
        else:
            base_query += "\nGROUP BY cuisine_type"
        
        # Add order and limit
        base_query += "\nORDER BY restaurant_count DESC"
        
        if limit:
            base_query += f"\nLIMIT {limit}"
        
        return base_query
    
    # Example usage
    query1 = get_sql_query(
        filter_clause="average_rating > 3.5",
        limit=5
    )
    
    print("Dynamically generated query:")
    print(query1)
    
    result = spark.sql(query1)
    result.show()
    
    # 2. Create a configuration layer for SQL parameters
    class SQLConfig:
        """Configuration for SQL queries"""
        # Filters
        MIN_RATING = 3.5
        MIN_REVIEWS = 10
        
        # Grouping
        GROUP_BY_CUISINE = "cuisine_type"
        GROUP_BY_CITY = "city"
        GROUP_BY_BOTH = "cuisine_type, city"
        
        # Limits
        PREVIEW_LIMIT = 10
        EXPORT_LIMIT = 1000
    
    # Use configuration in queries
    query2 = f"""
    SELECT 
        {SQLConfig.GROUP_BY_CUISINE},
        COUNT(*) as restaurant_count,
        ROUND(AVG(average_rating), 2) as avg_rating
    FROM restaurants
    WHERE average_rating >= {SQLConfig.MIN_RATING}
      AND num_reviews >= {SQLConfig.MIN_REVIEWS}
    GROUP BY {SQLConfig.GROUP_BY_CUISINE}
    ORDER BY restaurant_count DESC
    LIMIT {SQLConfig.PREVIEW_LIMIT}
    """
    
    print("\nConfigurable query:")
    print(query2)
    
    result = spark.sql(query2)
    result.show()
    
    # 3. Document SQL with block comments
    query3 = """
    /**
     * Restaurant Analytics Query
     * Purpose: Analyze restaurant distribution by cuisine type
     * 
     * Outputs:
     * - cuisine_type: Type of cuisine
     * - restaurant_count: Number of restaurants of this cuisine
     * - avg_rating: Average rating for this cuisine
     * - city_count: Number of cities where this cuisine is available
     * 
     * Filters:
     * - average_rating >= 3.5: Only include well-rated restaurants
     * - num_reviews >= 10: Ensure rating is based on sufficient data
     */
    
    -- Get all restaurants by cuisine with rating info
    WITH rated_restaurants AS (
        SELECT 
            cuisine_type,
            city,
            average_rating
        FROM restaurants
        WHERE average_rating >= 3.5
          AND num_reviews >= 10
    ),
    
    -- Get unique cities by cuisine
    cuisine_cities AS (
        SELECT 
            cuisine_type,
            COUNT(DISTINCT city) as city_count
        FROM rated_restaurants
        GROUP BY cuisine_type
    )
    
    -- Final analytics with city distribution
    SELECT 
        r.cuisine_type,
        COUNT(*) as restaurant_count,
        ROUND(AVG(r.average_rating), 2) as avg_rating,
        c.city_count
    FROM rated_restaurants r
    JOIN cuisine_cities c ON r.cuisine_type = c.cuisine_type
    GROUP BY r.cuisine_type, c.city_count
    ORDER BY restaurant_count DESC
    LIMIT 10
    """
    
    print("\nWell-documented query:")
    print(query3)
    
    result = spark.sql(query3)
    result.show()
    
    # 4. Error handling in SQL
    def safe_execute_sql(query, default_value=None):
        """Execute SQL with error handling"""
        try:
            return spark.sql(query)
        except Exception as e:
            print(f"Error executing query: {str(e)}")
            if default_value is not None:
                print(f"Returning default value")
                return default_value
            raise
    
    # Example usage
    valid_query = "SELECT COUNT(*) FROM restaurants"
    invalid_query = "SELECT * FROM nonexistent_table"
    
    print("\nSafe SQL execution:")
    result1 = safe_execute_sql(valid_query)
    result1.show()
    
    # Create an empty DataFrame as default
    from pyspark.sql.types import StructType, StructField, StringType, IntegerType
    empty_schema = StructType([
        StructField("cuisine_type", StringType(), True),
        StructField("count", IntegerType(), True)
    ])
    default_df = spark.createDataFrame([], empty_schema)
    
    result2 = safe_execute_sql(invalid_query, default_df)
    if result2 is not default_df:
        result2.show()
    else:
        print("Returned default empty DataFrame for invalid query")
    
    # Clean up
    spark.stop()
```

## 8. Key Takeaways

1. **Query Optimization Priorities**:
   - Push filters early (predicate pushdown)
   - Optimize join strategies based on table sizes
   - Use window functions instead of self-joins or correlated subqueries
   - Cache frequently used tables or views

2. **Join Strategy Selection**:
   - Broadcast Join: For small tables (<10MB by default)
   - Shuffle Hash Join: For medium-sized tables with high-cardinality keys
   - Sort Merge Join: For large tables with similar sizes
   - Enable adaptive query execution to let Spark choose optimally

3. **SQL vs. Programmatic Code**:
   - SQL: More readable, often more optimizable
   - DataFrame API: More testable, better for complex logic
   - Consider team skills and maintenance needs when choosing

4. **Complex Analytics Guidelines**:
   - Modularize with CTEs
   - Pre-aggregate data when possible
   - Consider join order (smallest to largest)
   - Document complex queries thoroughly

5. **Production Best Practices**:
   - Parameterize SQL for reusability
   - Use column pruning aggressively
   - Document purpose and outputs
   - Implement error handling

By applying these techniques, you can write highly optimized Spark SQL queries that process large datasets efficiently while remaining maintainable and understandable.
