# Pandas API on Spark Foundations - Part 4: Advanced Techniques

## Introduction

While Pandas API on Spark provides a familiar interface for distributed data processing, there are situations where you need to integrate native pandas code, create custom functions, or work around limitations. This lesson explores advanced techniques to maximize the benefits of both pandas and Spark.

## Setting Up Our Environment

Let's set up our environment and load the UberEats datasets:

```python
# Import libraries
import pyspark.pandas as ps
import pandas as pd
import numpy as np
import time

# For regular PySpark
from pyspark.sql import SparkSession
from pyspark.sql.functions import pandas_udf, PandasUDFType
from pyspark.sql.types import DoubleType, StringType, StructType, StructField

# Create a Spark session
spark = SparkSession.builder \
    .appName("Pandas API Advanced Techniques") \
    .master("local[*]") \
    .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
    .getOrCreate()

# Load our datasets
restaurants_ps = ps.read_json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
drivers_ps = ps.read_json("./storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl")
orders_ps = ps.read_json("./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl")

# Show dataset info
print(f"Restaurants: {len(restaurants_ps)} records")
print(f"Drivers: {len(drivers_ps)} records")
print(f"Orders: {len(orders_ps)} records")
```

## 1. Integrating Native Pandas Code

There are several ways to use native pandas code with Pandas API on Spark:

### Converting Small DataFrames to Pandas

For smaller datasets or operations that require the full pandas functionality:

```python
# Select a small subset (this is important for memory considerations)
top_restaurants = restaurants_ps.nlargest(10, 'average_rating')

# Convert to native pandas
top_restaurants_pd = top_restaurants.to_pandas()

print("Converted to native pandas:")
print(type(top_restaurants_pd))

# Perform native pandas operations 
# For example, use a pandas-only feature like style
styled_df = top_restaurants_pd.style.background_gradient(subset=['average_rating'], cmap='viridis')

# Note: the styled output won't show in this environment, but would work in Jupyter notebooks

# Perform another pandas-only operation
rolling_mean = top_restaurants_pd.sort_values('average_rating')['average_rating'].rolling(window=3).mean()
print("\nRolling mean of ratings (pandas-only operation):")
print(rolling_mean)

# Convert back to Pandas API on Spark if needed
# top_restaurants_ps = ps.from_pandas(top_restaurants_pd)
```

### Applying Pandas Functions to Partitions

Use `map_partitions` to apply pandas functions to each partition:

```python
# Define a function that works with pandas DataFrame
def enhance_restaurant_data(pdf):
    """Apply pandas-only operations to a partition"""
    # This is a regular pandas DataFrame
    pdf = pdf.copy()
    
    # Example pandas operations:
    # 1. Create new columns using pandas-specific methods
    pdf['name_length'] = pdf['name'].str.len()
    
    # 2. Use pandas-specific calculations
    if len(pdf) > 1:
        pdf['rating_rolling_mean'] = pdf.sort_values('average_rating')['average_rating'].rolling(window=2, min_periods=1).mean()
    else:
        pdf['rating_rolling_mean'] = pdf['average_rating']
    
    # 3. Apply pandas-only string manipulations
    pdf['name_clean'] = pdf['name'].str.replace(r' Restaurant$', '', regex=True)
    
    return pdf

# Apply the function to each partition
enhanced_restaurants = restaurants_ps.map_partitions(enhance_restaurant_data)

print("\nEnhanced restaurant data:")
print(enhanced_restaurants[['name', 'name_clean', 'name_length', 'average_rating', 'rating_rolling_mean']].head())
```

## 2. Using Pandas UDFs

Pandas UDFs (User-Defined Functions) provide an efficient way to apply custom pandas logic with Arrow-based optimization:

### Scalar Pandas UDFs

Scalar UDFs apply a function to a column and return a transformed column:

```python
# Convert Pandas API on Spark to PySpark DataFrame for UDF demonstration
restaurants_spark = restaurants_ps.to_spark()

# Define a scalar pandas UDF
@pandas_udf(DoubleType())
def calculate_score(ratings: pd.Series, reviews: pd.Series) -> pd.Series:
    """Calculate a custom score using pandas operations"""
    # This runs on pandas Series objects
    return (ratings ** 2) * np.log1p(reviews) / 10

# Apply the UDF to the PySpark DataFrame
restaurants_spark = restaurants_spark.withColumn(
    "custom_score", 
    calculate_score("average_rating", "num_reviews")
)

# Convert back to Pandas API on Spark
restaurants_with_score = ps.DataFrame(restaurants_spark)

print("\nRestaurants with custom score:")
print(restaurants_with_score[['name', 'average_rating', 'num_reviews', 'custom_score']].sort_values('custom_score', ascending=False).head())
```

### Grouped Aggregate Pandas UDFs

These UDFs perform aggregations within groups:

```python
# Define a grouped aggregate pandas UDF
@pandas_udf(DoubleType())
def weighted_average(ratings: pd.Series, weights: pd.Series) -> float:
    """Calculate weighted average using pandas operations"""
    return (ratings * weights).sum() / weights.sum() if weights.sum() > 0 else 0.0

# Apply to PySpark DataFrame with groupBy
weighted_ratings = restaurants_spark.groupBy("cuisine_type").agg(
    weighted_average("average_rating", "num_reviews").alias("weighted_rating")
)

# Convert to Pandas API on Spark
weighted_ratings_ps = ps.DataFrame(weighted_ratings)

print("\nWeighted average ratings by cuisine:")
print(weighted_ratings_ps.sort_values('weighted_rating', ascending=False))
```

## 3. Optimizations for Pandas API on Spark

Let's explore some optimizations specific to Pandas API on Spark:

### Using Proper Index Types

```python
# Check default index type
print(f"\nDefault index type: {ps.get_option('compute.default_index_type')}")

# Change index type to distributed (better for large datasets)
ps.set_option('compute.default_index_type', 'distributed')
print(f"New index type: {ps.get_option('compute.default_index_type')}")

# Reset index after operations that might affect it
sorted_restaurants = restaurants_ps.sort_values('average_rating', ascending=False).reset_index(drop=True)
print("\nSorted restaurants with reset index:")
print(sorted_restaurants.head())
```

### Controlling Partition Count

```python
# Check current number of partitions
current_partitions = restaurants_ps.to_spark().rdd.getNumPartitions()
print(f"\nCurrent number of partitions: {current_partitions}")

# Set a specific number of partitions for better parallelism
restaurants_repartitioned = restaurants_ps.repartition(4)
new_partitions = restaurants_repartitioned.to_spark().rdd.getNumPartitions()
print(f"New number of partitions: {new_partitions}")
```

### Eager vs. Lazy Execution Control

```python
# By default, many operations are lazy
filtered_data = restaurants_ps[restaurants_ps.average_rating > 4.0]
# Nothing computed yet

# Force computation with compute() or actions like len()
start_time = time.time()
result_count = len(filtered_data)
computation_time = time.time() - start_time
print(f"\nComputation time for filtering: {computation_time:.4f} seconds")
print(f"Result count: {result_count}")

# Use compute() to control when computation happens
start_time = time.time()
restaurants_ps.compute()
computation_time = time.time() - start_time
print(f"Computation time for full dataset: {computation_time:.4f} seconds")
```

## 4. Limitations and Workarounds

Let's address some common limitations of Pandas API on Spark and how to work around them:

### Handling Unsupported Operations

```python
# Some pandas operations aren't supported or behave differently
# For example, .loc with boolean indexing behaves differently

try:
    # This might not work as expected in Pandas API on Spark
    result = restaurants_ps.loc[restaurants_ps.average_rating > 4.0, ['name', 'cuisine_type']]
    print("\nUsing .loc with boolean indexing:")
    print(result.head())
except Exception as e:
    print(f"\nError with .loc: {str(e)}")
    
    # Workaround: Use alternative syntax
    result = restaurants_ps[restaurants_ps.average_rating > 4.0][['name', 'cuisine_type']]
    print("\nWorkaround using alternative syntax:")
    print(result.head())
```

### Managing Memory for Large Operations

```python
# For memory-intensive operations on large datasets, 
# convert to Spark DataFrame temporarily

# Define a complex operation
def complex_analysis(df):
    """A memory-intensive operation"""
    # Convert to PySpark
    spark_df = df.to_spark()
    
    # Use Spark operations
    result = spark_df.groupBy("cuisine_type").agg(
        {"average_rating": "mean", "num_reviews": "sum"}
    )
    
    # Cache if used multiple times
    result.cache()
    
    # Convert back to Pandas API on Spark
    return ps.DataFrame(result)

# Run the analysis
cuisine_analysis = complex_analysis(restaurants_ps)
print("\nCuisine analysis using Spark optimization:")
print(cuisine_analysis)
```

### Working with Time Series Data

Time series operations can be challenging in distributed environments:

```python
# Convert order_date to datetime
orders_ps['order_timestamp'] = pd.to_datetime(orders_ps['order_date'])

# Extract date components
orders_ps['order_year'] = orders_ps['order_timestamp'].dt.year
orders_ps['order_month'] = orders_ps['order_timestamp'].dt.month
orders_ps['order_day'] = orders_ps['order_timestamp'].dt.day

# For time series analysis, often better to group by time period first
monthly_orders = orders_ps.groupby(['order_year', 'order_month']).agg({
    'order_id': 'count',
    'total_amount': 'sum'
}).reset_index()

print("\nMonthly Order Analysis:")
print(monthly_orders.sort_values(['order_year', 'order_month']))
```

## 5. Practical Application: Advanced UberEats Analysis

Let's combine these advanced techniques in a comprehensive analysis:

```python
def advanced_ubereats_analysis(restaurants_ps, orders_ps, drivers_ps):
    """
    Perform advanced analysis using a combination of Pandas API on Spark, 
    native pandas, and Spark optimizations
    """
    # Step 1: Data Preparation with Pandas API on Spark
    # Convert orders dates to datetime
    orders_ps['order_timestamp'] = pd.to_datetime(orders_ps['order_date'])
    
    # Step 2: Join datasets (using more efficient PySpark operations)
    orders_spark = orders_ps.to_spark()
    restaurants_spark = restaurants_ps.to_spark()
    drivers_spark = drivers_ps.to_spark()
    
    # Rename for join
    orders_spark = orders_spark.withColumnRenamed("restaurant_key", "cnpj")
    orders_spark = orders_spark.withColumnRenamed("driver_key", "license_number")
    
    # Join operations
    joined_data = orders_spark.join(
        restaurants_spark, 
        on="cnpj", 
        how="inner"
    ).join(
        drivers_spark, 
        on="license_number", 
        how="inner"
    )
    
    # Step 3: Define a Pandas UDF for advanced calculations
    @pandas_udf(DoubleType())
    def calculate_efficiency_score(amounts: pd.Series, vehicle_year: pd.Series) -> pd.Series:
        """Calculate driver efficiency based on order amount and vehicle age"""
        vehicle_age = 2025 - vehicle_year  # Current year = 2025
        return amounts / (1 + 0.05 * vehicle_age)  # Adjust for vehicle age
    
    # Apply UDF
    driver_performance = joined_data.groupBy("driver_id", "first_name", "last_name", "vehicle_type", "vehicle_year") \
        .agg(
            {"order_id": "count", "total_amount": "sum"}
        ) \
        .withColumnRenamed("count(order_id)", "order_count") \
        .withColumnRenamed("sum(total_amount)", "total_amount")
    
    # Add efficiency score
    driver_performance = driver_performance.withColumn(
        "efficiency_score",
        calculate_efficiency_score("total_amount", "vehicle_year")
    )
    
    # Step 4: Convert back to Pandas API on Spark for further analysis
    driver_performance_ps = ps.DataFrame(driver_performance)
    
    # Step 5: Use native pandas for specialized visualization on a small sample
    top_drivers = driver_performance_ps.nlargest(10, 'efficiency_score')
    top_drivers_pd = top_drivers.to_pandas()
    
    # Step 6: Restaurant analysis with window functions
    restaurant_performance = ps.DataFrame(
        joined_data.groupBy("restaurant_id", "name", "cuisine_type", "city") \
        .agg(
            {"order_id": "count", "total_amount": "sum", "average_rating": "first"}
        ) \
        .withColumnRenamed("count(order_id)", "order_count") \
        .withColumnRenamed("sum(total_amount)", "total_revenue") \
        .withColumnRenamed("first(average_rating)", "rating")
    )
    
    # Calculate ranks within cuisine types
    restaurant_performance['revenue_rank'] = restaurant_performance \
        .groupby('cuisine_type')['total_revenue'] \
        .rank(method='dense', ascending=False)
    
    # Step 7: Time-based analysis using native pandas technique but on Spark
    # First aggregate by date in Spark (more efficient)
    time_analysis_spark = orders_spark.groupBy(
        orders_spark.order_timestamp.cast("date").alias("order_date")
    ).agg(
        {"order_id": "count", "total_amount": "sum"}
    ).orderBy("order_date")
    
    # Convert to Pandas API on Spark
    time_analysis = ps.DataFrame(time_analysis_spark)
    
    # Calculate 7-day rolling average
    # For this, we'll use a small pandas sample (assuming reasonably few dates)
    if len(time_analysis) < 100:  # Small enough for pandas
        time_analysis_pd = time_analysis.to_pandas()
        time_analysis_pd['rolling_avg'] = time_analysis_pd['sum(total_amount)'].rolling(7, min_periods=1).mean()
        time_analysis = ps.from_pandas(time_analysis_pd)
    
    return {
        'driver_performance': driver_performance_ps,
        'top_drivers_pd': top_drivers_pd,  # Native pandas for visualizations
        'restaurant_performance': restaurant_performance,
        'time_analysis': time_analysis
    }

# Run the analysis
results = advanced_ubereats_analysis(restaurants_ps, orders_ps, drivers_ps)

# Display results
print("\nTop Performing Drivers:")
print(results['driver_performance'].sort_values('efficiency_score', ascending=False).head())

print("\nTop Restaurant Performance:")
print(results['restaurant_performance'].sort_values('total_revenue', ascending=False).head())

print("\nOrder Time Analysis:")
print(results['time_analysis'].head())
```

## Conclusion and Best Practices

In this lesson, we've explored advanced techniques with Pandas API on Spark:

1. **Integrating Native Pandas Code**: Converting between Pandas API on Spark and native pandas
2. **Pandas UDFs**: Creating efficient custom functions with Arrow optimization
3. **Optimizations**: Tuning performance with indexing, partitioning, and execution control
4. **Limitations and Workarounds**: Addressing common challenges with alternative approaches
5. **Practical Application**: Combining techniques for sophisticated analysis

**Best Practices:**

- Use native pandas only for small datasets that fit in memory
- Leverage Pandas UDFs for custom logic with better performance
- Choose the right index type for your workload (distributed for large datasets)
- Convert to PySpark DataFrame temporarily for operations that aren't efficient in Pandas API
- Control partitioning to balance parallelism and overhead
- Be aware of execution model differences (lazy vs. eager)
- Consider fallback strategies for unsupported operations

In the next lesson, we'll explore how to deliver processed data to various destinations using Pandas API on Spark.

## Exercise

1. Create a custom Pandas UDF that:
   - Calculates a restaurant score based on ratings, reviews, and other factors
   - Incorporates time-aware logic (e.g., gives more weight to recent activity)
   - Handles edge cases and missing data

2. Build an analysis pipeline that:
   - Combines Pandas API, native pandas, and PySpark operations for optimal performance
   - Creates custom aggregations not available in standard functions
   - Works around Pandas API limitations using the techniques learned

## Resources

- [Pandas UDFs Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark/sql/functions.html#pandas-function-apis)
- [Pandas API on Spark Configuration](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/options.html)
