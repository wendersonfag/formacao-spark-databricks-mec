# Building an End-to-End Pipeline with Spark SQL

This guide demonstrates how to build a complete data pipeline using Spark SQL for the UberEats case study, focusing on declarative development, SQL query structuring, and modularization.

## 1. Project Structure

Let's organize our SQL-based pipeline with a clean structure:

```
src/
  app/
    main.py                 # Pipeline entry point
    config.py               # Configuration settings
    sql_pipeline.py         # Main pipeline orchestration
    sql/
      create_views.sql      # SQL for creating views
      restaurants.sql       # Restaurant analysis queries
      users.sql             # User analysis queries
      orders.sql            # Order analysis queries
      delivery.sql          # Delivery analysis queries
```

## 2. Configuration Setup

```python
# app/config.py
import os

class SQLPipelineConfig:
    """Configuration for the SQL-based pipeline"""
    
    # Input paths
    RESTAURANTS_PATH = "./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl"
    USERS_PATH = "./storage/mongodb/users/01JS4W5A7WWZBQ6Y1C465EYR76.jsonl"
    ORDERS_PATH = "./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl"
    DRIVERS_PATH = "./storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl"
    
    # SQL files
    SQL_DIR = os.path.join(os.path.dirname(__file__), "sql")
    CREATE_VIEWS_SQL = os.path.join(SQL_DIR, "create_views.sql")
    RESTAURANTS_SQL = os.path.join(SQL_DIR, "restaurants.sql")
    USERS_SQL = os.path.join(SQL_DIR, "users.sql")
    ORDERS_SQL = os.path.join(SQL_DIR, "orders.sql")
    DELIVERY_SQL = os.path.join(SQL_DIR, "delivery.sql")
    
    # Output paths
    OUTPUT_DIR = "./output/spark_sql"
    RESTAURANT_ANALYSIS_OUTPUT = os.path.join(OUTPUT_DIR, "restaurant_analysis")
    USER_ANALYSIS_OUTPUT = os.path.join(OUTPUT_DIR, "user_analysis")
    ORDER_ANALYSIS_OUTPUT = os.path.join(OUTPUT_DIR, "order_analysis")
    DELIVERY_ANALYSIS_OUTPUT = os.path.join(OUTPUT_DIR, "delivery_analysis")
    
    # Spark configuration
    SPARK_CONF = {
        "spark.sql.adaptive.enabled": "true",
        "spark.sql.adaptive.coalescePartitions.enabled": "true",
        "spark.sql.adaptive.skewJoin.enabled": "true",
        "spark.sql.shuffle.partitions": "10"
    }
```

## 3. SQL Script: Creating Views

```sql
-- app/sql/create_views.sql

-- Clean and standardize restaurant data
CREATE OR REPLACE TEMPORARY VIEW restaurants_raw AS
SELECT * FROM json.`./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl`;

CREATE OR REPLACE TEMPORARY VIEW restaurants AS
SELECT
  restaurant_id,
  name,
  city,
  COALESCE(cuisine_type, 'Unknown') AS cuisine_type,
  COALESCE(average_rating, 0) AS average_rating,
  COALESCE(num_reviews, 0) AS num_reviews,
  REGEXP_REPLACE(phone_number, '[\\(\\)\\s-]', '') AS phone_number,
  uuid,
  address,
  opening_time,
  closing_time,
  country,
  cnpj
FROM restaurants_raw
WHERE restaurant_id IS NOT NULL;

-- Clean and standardize user data
CREATE OR REPLACE TEMPORARY VIEW users_raw AS
SELECT * FROM json.`./storage/mongodb/users/01JS4W5A7WWZBQ6Y1C465EYR76.jsonl`;

CREATE OR REPLACE TEMPORARY VIEW users AS
SELECT
  user_id,
  COALESCE(city, '') AS city,
  country,
  REGEXP_REPLACE(phone_number, '[\\(\\)\\s-]', '') AS phone_number,
  email,
  uuid,
  delivery_address,
  COALESCE(cpf, '') AS cpf
FROM users_raw
WHERE user_id IS NOT NULL;

-- Clean and standardize order data
CREATE OR REPLACE TEMPORARY VIEW orders_raw AS
SELECT * FROM json.`./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl`;

CREATE OR REPLACE TEMPORARY VIEW orders AS
SELECT
  order_id,
  user_key,
  restaurant_key,
  driver_key,
  TO_TIMESTAMP(order_date) AS order_date,
  CAST(COALESCE(total_amount, 0) AS DECIMAL(10,2)) AS total_amount,
  payment_key,
  rating_key
FROM orders_raw
WHERE order_id IS NOT NULL;

-- Clean and standardize driver data
CREATE OR REPLACE TEMPORARY VIEW drivers_raw AS
SELECT * FROM json.`./storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl`;

CREATE OR REPLACE TEMPORARY VIEW drivers AS
SELECT
  driver_id,
  first_name,
  last_name,
  CONCAT(first_name, ' ', last_name) AS full_name,
  city,
  country,
  REGEXP_REPLACE(phone_number, '[\\(\\)\\s-]', '') AS phone_number,
  vehicle_type,
  vehicle_make,
  vehicle_model,
  uuid,
  license_number
FROM drivers_raw
WHERE driver_id IS NOT NULL;
```

## 4. SQL Script: Restaurant Analysis

```sql
-- app/sql/restaurants.sql

-- Restaurant statistics by cuisine type
CREATE OR REPLACE TEMPORARY VIEW restaurant_cuisine_stats AS
SELECT
  cuisine_type,
  COUNT(*) AS restaurant_count,
  ROUND(AVG(average_rating), 2) AS avg_rating,
  SUM(num_reviews) AS total_reviews,
  MIN(average_rating) AS min_rating,
  MAX(average_rating) AS max_rating
FROM restaurants
GROUP BY cuisine_type
ORDER BY restaurant_count DESC;

-- Analyze restaurant ratings distribution
CREATE OR REPLACE TEMPORARY VIEW restaurant_rating_distribution AS
WITH rating_categories AS (
  SELECT
    restaurant_id,
    name,
    cuisine_type,
    average_rating,
    CASE
      WHEN average_rating >= 4.5 THEN 'Excellent (4.5-5.0)'
      WHEN average_rating >= 4.0 THEN 'Very Good (4.0-4.4)'
      WHEN average_rating >= 3.5 THEN 'Good (3.5-3.9)'
      WHEN average_rating >= 3.0 THEN 'Average (3.0-3.4)'
      WHEN average_rating >= 2.0 THEN 'Below Average (2.0-2.9)'
      ELSE 'Poor (0-1.9)'
    END AS rating_category
  FROM restaurants
)
SELECT
  rating_category,
  COUNT(*) AS restaurant_count,
  ROUND(100.0 * COUNT(*) / (SELECT COUNT(*) FROM restaurants), 2) AS percentage
FROM rating_categories
GROUP BY rating_category
ORDER BY restaurant_count DESC;

-- Top restaurants by cuisine
CREATE OR REPLACE TEMPORARY VIEW top_restaurants_by_cuisine AS
WITH ranked_restaurants AS (
  SELECT
    cuisine_type,
    restaurant_id,
    name,
    city,
    average_rating,
    num_reviews,
    RANK() OVER (PARTITION BY cuisine_type ORDER BY average_rating DESC, num_reviews DESC) AS rank
  FROM restaurants
  WHERE num_reviews >= 100  -- Minimum number of reviews for reliability
)
SELECT
  cuisine_type,
  restaurant_id,
  name,
  city,
  average_rating,
  num_reviews
FROM ranked_restaurants
WHERE rank <= 3  -- Top 3 per cuisine
ORDER BY cuisine_type, rank;
```

## 5. SQL Script: User Analysis

```sql
-- app/sql/users.sql

-- User distribution by city
CREATE OR REPLACE TEMPORARY VIEW user_city_distribution AS
SELECT
  city,
  COUNT(*) AS user_count,
  ROUND(100.0 * COUNT(*) / (SELECT COUNT(*) FROM users), 2) AS percentage
FROM users
WHERE city IS NOT NULL AND city != ''
GROUP BY city
ORDER BY user_count DESC;

-- User-restaurant matches by city
CREATE OR REPLACE TEMPORARY VIEW user_restaurant_city_matches AS
SELECT
  u.city,
  COUNT(DISTINCT u.user_id) AS user_count,
  COUNT(DISTINCT r.restaurant_id) AS restaurant_count,
  ROUND(COUNT(DISTINCT r.restaurant_id) * 1.0 / COUNT(DISTINCT u.user_id), 2) AS restaurants_per_user,
  ROUND(AVG(r.average_rating), 2) AS avg_restaurant_rating
FROM users u
JOIN restaurants r ON u.city = r.city
GROUP BY u.city
HAVING COUNT(DISTINCT u.user_id) > 0
ORDER BY user_count DESC;

-- User email domain analysis
CREATE OR REPLACE TEMPORARY VIEW user_email_domains AS
SELECT
  LOWER(SUBSTRING(email, LOCATE('@', email) + 1)) AS email_domain,
  COUNT(*) AS user_count,
  ROUND(100.0 * COUNT(*) / (SELECT COUNT(*) FROM users), 2) AS percentage
FROM users
WHERE email IS NOT NULL
GROUP BY email_domain
ORDER BY user_count DESC;
```

## 6. SQL Script: Order Analysis

```sql
-- app/sql/orders.sql

-- Extract time components from order_date
CREATE OR REPLACE TEMPORARY VIEW orders_with_time AS
SELECT
  order_id,
  order_date,
  YEAR(order_date) AS year,
  MONTH(order_date) AS month,
  DAY(order_date) AS day,
  HOUR(order_date) AS hour,
  DAYOFWEEK(order_date) AS day_of_week,
  total_amount,
  user_key,
  restaurant_key,
  driver_key
FROM orders;

-- Orders by month analysis
CREATE OR REPLACE TEMPORARY VIEW monthly_orders AS
SELECT
  year,
  month,
  COUNT(*) AS order_count,
  ROUND(AVG(total_amount), 2) AS avg_order_amount,
  ROUND(SUM(total_amount), 2) AS total_revenue,
  MIN(total_amount) AS min_order_amount,
  MAX(total_amount) AS max_order_amount
FROM orders_with_time
GROUP BY year, month
ORDER BY year, month;

-- Orders by hour of day
CREATE OR REPLACE TEMPORARY VIEW hourly_orders AS
SELECT
  hour,
  COUNT(*) AS order_count,
  ROUND(AVG(total_amount), 2) AS avg_order_amount,
  ROUND(100.0 * COUNT(*) / (SELECT COUNT(*) FROM orders_with_time), 2) AS percentage
FROM orders_with_time
GROUP BY hour
ORDER BY hour;

-- Orders by day of week
CREATE OR REPLACE TEMPORARY VIEW weekly_orders AS
SELECT
  day_of_week,
  CASE day_of_week
    WHEN 1 THEN 'Sunday'
    WHEN 2 THEN 'Monday'
    WHEN 3 THEN 'Tuesday'
    WHEN 4 THEN 'Wednesday'
    WHEN 5 THEN 'Thursday'
    WHEN 6 THEN 'Friday'
    WHEN 7 THEN 'Saturday'
  END AS day_name,
  COUNT(*) AS order_count,
  ROUND(AVG(total_amount), 2) AS avg_order_amount
FROM orders_with_time
GROUP BY day_of_week, day_name
ORDER BY day_of_week;
```

## 7. SQL Script: Delivery Analysis

```sql
-- app/sql/delivery.sql

-- Join orders with drivers and restaurants
-- Note: In a real scenario, you'd have proper foreign keys
-- Here we're using a simplified approach
CREATE OR REPLACE TEMPORARY VIEW delivery_analysis AS
SELECT
  o.order_id,
  o.order_date,
  o.total_amount,
  d.driver_id,
  d.full_name AS driver_name,
  d.vehicle_type,
  d.city AS driver_city,
  r.restaurant_id,
  r.name AS restaurant_name,
  r.city AS restaurant_city,
  r.cuisine_type
FROM orders o
LEFT JOIN drivers d ON o.driver_key = d.license_number  -- Simplified join
LEFT JOIN restaurants r ON r.cnpj = o.restaurant_key    -- Simplified join
WHERE o.order_date IS NOT NULL;

-- Driver performance analysis
CREATE OR REPLACE TEMPORARY VIEW driver_performance AS
SELECT
  driver_id,
  driver_name,
  vehicle_type,
  COUNT(order_id) AS delivery_count,
  driver_city,
  MIN(order_date) AS first_delivery,
  MAX(order_date) AS last_delivery
FROM delivery_analysis
WHERE driver_id IS NOT NULL
GROUP BY driver_id, driver_name, vehicle_type, driver_city
ORDER BY delivery_count DESC;

-- City delivery analysis
CREATE OR REPLACE TEMPORARY VIEW city_delivery_stats AS
SELECT
  restaurant_city,
  COUNT(order_id) AS order_count,
  COUNT(DISTINCT driver_id) AS unique_drivers,
  COUNT(DISTINCT restaurant_id) AS unique_restaurants,
  ROUND(AVG(total_amount), 2) AS avg_order_amount,
  ROUND(SUM(total_amount), 2) AS total_revenue
FROM delivery_analysis
WHERE restaurant_city IS NOT NULL
GROUP BY restaurant_city
ORDER BY order_count DESC;
```

## 8. Main Python Script for Orchestration

```python
# app/sql_pipeline.py
from pyspark.sql import SparkSession
import os
from app.config import SQLPipelineConfig

class SparkSQLPipeline:
    """Pipeline for running SQL-based analysis on UberEats data"""
    
    def __init__(self):
        """Initialize the pipeline with a Spark session"""
        self.config = SQLPipelineConfig()
        self.spark = self._create_spark_session()
    
    def _create_spark_session(self):
        """Create a Spark session configured for SQL workloads"""
        builder = SparkSession.builder.appName("UberEats-SQL-Pipeline")
        
        # Apply configurations from config
        for key, value in self.config.SPARK_CONF.items():
            builder = builder.config(key, value)
        
        return builder.getOrCreate()
    
    def _read_sql_file(self, file_path):
        """Read SQL from a file and return as a string"""
        with open(file_path, 'r') as file:
            return file.read()
    
    def _execute_sql_script(self, file_path):
        """Execute a SQL script file with multiple statements"""
        sql_content = self._read_sql_file(file_path)
        
        # Split the SQL content into individual statements
        statements = [stmt.strip() for stmt in sql_content.split(';') if stmt.strip()]
        
        # Execute each statement
        for statement in statements:
            self.spark.sql(statement)
    
    def create_views(self):
        """Create and register temporary views for the data sources"""
        print("Creating views...")
        self._execute_sql_script(self.config.CREATE_VIEWS_SQL)
        
        # Verify views were created
        print("Registered views:")
        for view in self.spark.catalog.listTables():
            print(f"- {view.name}")
    
    def analyze_restaurants(self):
        """Run restaurant analysis SQL queries"""
        print("Analyzing restaurants...")
        self._execute_sql_script(self.config.RESTAURANTS_SQL)
        
        # Save results
        os.makedirs(self.config.RESTAURANT_ANALYSIS_OUTPUT, exist_ok=True)
        
        # Save each analysis view
        self.spark.table("restaurant_cuisine_stats").write.mode("overwrite") \
            .parquet(os.path.join(self.config.RESTAURANT_ANALYSIS_OUTPUT, "cuisine_stats"))
        
        self.spark.table("restaurant_rating_distribution").write.mode("overwrite") \
            .parquet(os.path.join(self.config.RESTAURANT_ANALYSIS_OUTPUT, "rating_distribution"))
        
        self.spark.table("top_restaurants_by_cuisine").write.mode("overwrite") \
            .parquet(os.path.join(self.config.RESTAURANT_ANALYSIS_OUTPUT, "top_by_cuisine"))
    
    def analyze_users(self):
        """Run user analysis SQL queries"""
        print("Analyzing users...")
        self._execute_sql_script(self.config.USERS_SQL)
        
        # Save results
        os.makedirs(self.config.USER_ANALYSIS_OUTPUT, exist_ok=True)
        
        # Save each analysis view
        self.spark.table("user_city_distribution").write.mode("overwrite") \
            .parquet(os.path.join(self.config.USER_ANALYSIS_OUTPUT, "city_distribution"))
        
        self.spark.table("user_restaurant_city_matches").write.mode("overwrite") \
            .parquet(os.path.join(self.config.USER_ANALYSIS_OUTPUT, "restaurant_matches"))
        
        self.spark.table("user_email_domains").write.mode("overwrite") \
            .parquet(os.path.join(self.config.USER_ANALYSIS_OUTPUT, "email_domains"))
    
    def analyze_orders(self):
        """Run order analysis SQL queries"""
        print("Analyzing orders...")
        self._execute_sql_script(self.config.ORDERS_SQL)
        
        # Save results
        os.makedirs(self.config.ORDER_ANALYSIS_OUTPUT, exist_ok=True)
        
        # Save each analysis view
        self.spark.table("monthly_orders").write.mode("overwrite") \
            .parquet(os.path.join(self.config.ORDER_ANALYSIS_OUTPUT, "monthly"))
        
        self.spark.table("hourly_orders").write.mode("overwrite") \
            .parquet(os.path.join(self.config.ORDER_ANALYSIS_OUTPUT, "hourly"))
        
        self.spark.table("weekly_orders").write.mode("overwrite") \
            .parquet(os.path.join(self.config.ORDER_ANALYSIS_OUTPUT, "weekly"))
    
    def analyze_delivery(self):
        """Run delivery analysis SQL queries"""
        print("Analyzing delivery...")
        self._execute_sql_script(self.config.DELIVERY_SQL)
        
        # Save results
        os.makedirs(self.config.DELIVERY_ANALYSIS_OUTPUT, exist_ok=True)
        
        # Save each analysis view
        self.spark.table("driver_performance").write.mode("overwrite") \
            .parquet(os.path.join(self.config.DELIVERY_ANALYSIS_OUTPUT, "driver_performance"))
        
        self.spark.table("city_delivery_stats").write.mode("overwrite") \
            .parquet(os.path.join(self.config.DELIVERY_ANALYSIS_OUTPUT, "city_stats"))
    
    def show_sample_results(self):
        """Display sample results from each analysis"""
        analyses = [
            "restaurant_cuisine_stats",
            "user_city_distribution",
            "monthly_orders",
            "driver_performance"
        ]
        
        print("\nSample results:")
        for analysis in analyses:
            print(f"\n{analysis.replace('_', ' ').title()}:")
            self.spark.table(analysis).show(5, truncate=False)
    
    def run_pipeline(self):
        """Run the complete pipeline"""
        try:
            print("Starting UberEats SQL pipeline...")
            
            # Create views from the data sources
            self.create_views()
            
            # Run analyses
            self.analyze_restaurants()
            self.analyze_users()
            self.analyze_orders()
            self.analyze_delivery()
            
            # Display sample results
            self.show_sample_results()
            
            print("\nPipeline completed successfully.")
            
        except Exception as e:
            print(f"Pipeline failed: {str(e)}")
            raise
        finally:
            # Clean up resources
            self.spark.stop()
```

## 9. Pipeline Entry Point

```python
# app/main.py
import argparse
import time
from app.sql_pipeline import SparkSQLPipeline

def main():
    """Main entry point for the UberEats SQL pipeline"""
    parser = argparse.ArgumentParser(description="UberEats SQL Pipeline")
    parser.add_argument("--show-results", action="store_true", help="Show sample results")
    args = parser.parse_args()
    
    start_time = time.time()
    
    # Create and run the pipeline
    pipeline = SparkSQLPipeline()
    pipeline.run_pipeline()
    
    execution_time = time.time() - start_time
    print(f"Pipeline execution completed in {execution_time:.2f} seconds")

if __name__ == "__main__":
    main()
```

## 10. Running the Pipeline

```bash
# From the src directory
mkdir -p app/sql
# Create the SQL files in app/sql/ directory
python -m app.main
```

## 11. Key SQL Best Practices Demonstrated

1. **Modular SQL Organization**:
   - Breaking queries into logical files
   - Creating views for intermediate results
   - Separating data preparation from analysis

2. **Declarative Development**:
   - Using SQL for what it does best: data transformation
   - Letting the optimizer handle execution details
   - Focusing on the "what," not the "how"

3. **Complex Query Techniques**:
   - Using Common Table Expressions (CTEs) for readability
   - Window functions for analytical queries
   - CASE statements for categorization

4. **Performance Optimization**:
   - Views for query reuse
   - Predicate pushdown (filtering early)
   - Schema consistency in views

5. **Maintenance and Evolution**:
   - Centralized configuration for file paths
   - Parameterized SQL execution
   - Clear documentation within SQL

## 12. Advantages of SQL-Based Pipelines

1. **Simplicity**: SQL is concise and expressive for data transformations
2. **Readability**: Even non-developers can understand the analysis
3. **Optimization**: Spark's Catalyst optimizer can better optimize SQL
4. **Maintainability**: Easier to update and extend
5. **Accessibility**: Lower barrier for data analysts to contribute

## 13. Next Steps

1. **Parameterization**: Add parameters to SQL queries for dynamic analysis
2. **Scheduling**: Integrate with orchestration tools for regular execution
3. **Data Quality**: Add SQL-based data quality checks
4. **Documentation**: Add data dictionaries and query documentation
5. **Performance Tuning**: Optimize query plans for larger datasets

This SQL-centric approach provides a clean, maintainable, and efficient way to implement a complete data pipeline, demonstrating the power of declarative programming for data analysis.
