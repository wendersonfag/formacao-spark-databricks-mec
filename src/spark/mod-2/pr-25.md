# First Steps with Spark SQL

This guide introduces declarative development with Spark SQL, showing how to use SQL as your primary tool for data processing in Apache Spark.

## 1. Introduction to Spark SQL

Spark SQL lets you write SQL queries instead of imperative code, which can be:
- More concise and readable
- Easier to maintain and document
- More accessible to data analysts familiar with SQL
- Automatically optimized by Spark's Catalyst optimizer

## 2. Setting Up Your Environment

```python
# app/spark_sql_setup.py

from pyspark.sql import SparkSession

def create_spark_session():
    """Create a Spark Session with SQL configurations"""
    return SparkSession.builder \
        .appName("UberEats-SparkSQL") \
        .config("spark.sql.shuffle.partitions", "10") \
        .config("spark.sql.adaptive.enabled", "true") \
        .getOrCreate()

if __name__ == "__main__":
    # Create Spark session
    spark = create_spark_session()
    
    # Test basic SQL query
    spark.sql("SELECT 'Hello, Spark SQL!'").show()
    
    spark.stop()
```

## 3. Reading Data and Creating Views

```python
# app/spark_sql_views.py

from pyspark.sql import SparkSession

def create_views():
    """Load data and create temporary views for SQL queries"""
    spark = SparkSession.builder.appName("UberEats-Views").getOrCreate()
    
    try:
        # Load data
        users_df = spark.read.json("./storage/mongodb/users/01JS4W5A7WWZBQ6Y1C465EYR76.jsonl")
        restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
        orders_df = spark.read.json("./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl")
        drivers_df = spark.read.json("./storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl")
        
        # Register temporary views
        users_df.createOrReplaceTempView("users")
        restaurants_df.createOrReplaceTempView("restaurants")
        orders_df.createOrReplaceTempView("orders")
        drivers_df.createOrReplaceTempView("drivers")
        
        # Show registered views
        print("Registered temporary views:")
        for view in spark.catalog.listTables():
            print(f"- {view.name}")
        
        # Inspect schema of a view
        print("\nSchema for 'restaurants':")
        spark.sql("DESCRIBE restaurants").show()
        
    finally:
        # Don't stop Spark here if using this function with other code
        pass
        
    return spark

if __name__ == "__main__":
    spark = create_views()
    
    # Run a simple query to verify views
    print("\nSample data from restaurants:")
    spark.sql("SELECT name, cuisine_type, average_rating FROM restaurants LIMIT 3").show(truncate=False)
    
    spark.stop()
```

## 4. Building a SQL-Focused Pipeline

```python
# app/spark_sql_pipeline.py

from pyspark.sql import SparkSession
import os

def run_sql_pipeline():
    """Build an end-to-end pipeline using Spark SQL"""
    spark = SparkSession.builder.appName("UberEats-SQLPipeline").getOrCreate()
    
    # Step 1: Load data and create views
    users_df = spark.read.json("./storage/mongodb/users/01JS4W5A7WWZBQ6Y1C465EYR76.jsonl")
    restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
    orders_df = spark.read.json("./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl")
    
    users_df.createOrReplaceTempView("users")
    restaurants_df.createOrReplaceTempView("restaurants")
    orders_df.createOrReplaceTempView("orders")
    
    # Step 2: Data cleaning with SQL
    # Create clean view with standardized phone numbers
    spark.sql("""
    CREATE OR REPLACE TEMPORARY VIEW clean_restaurants AS
    SELECT 
        restaurant_id,
        name,
        cuisine_type,
        city,
        average_rating,
        num_reviews,
        REGEXP_REPLACE(phone_number, '[\\(\\)\\s-]', '') AS clean_phone
    FROM restaurants
    """)
    
    # Step 3: Analysis with SQL queries
    
    # Cuisine distribution
    cuisine_counts = spark.sql("""
    SELECT 
        cuisine_type,
        COUNT(*) AS restaurant_count,
        ROUND(AVG(average_rating), 2) AS avg_rating
    FROM clean_restaurants
    GROUP BY cuisine_type
    ORDER BY restaurant_count DESC
    """)
    
    # City analysis
    city_stats = spark.sql("""
    SELECT
        r.city,
        COUNT(DISTINCT r.restaurant_id) AS restaurant_count,
        COUNT(DISTINCT u.user_id) AS user_count,
        ROUND(AVG(r.average_rating), 2) AS avg_restaurant_rating
    FROM clean_restaurants r
    LEFT JOIN users u ON r.city = u.city
    GROUP BY r.city
    HAVING COUNT(DISTINCT u.user_id) > 0
    ORDER BY restaurant_count DESC
    """)
    
    # Step 4: Save results
    output_dir = "./output/sql_results"
    os.makedirs(output_dir, exist_ok=True)
    
    cuisine_counts.write.mode("overwrite").parquet(f"{output_dir}/cuisine_analysis")
    city_stats.write.mode("overwrite").parquet(f"{output_dir}/city_analysis")
    
    # Show results
    print("Cuisine Analysis:")
    cuisine_counts.show()
    
    print("\nCity Analysis:")
    city_stats.show()
    
    spark.stop()
    
if __name__ == "__main__":
    run_sql_pipeline()
```

## 5. Advanced SQL Techniques

```python
# app/spark_sql_advanced.py

from pyspark.sql import SparkSession

def demonstrate_advanced_sql():
    """Show advanced SQL techniques in Spark"""
    spark = SparkSession.builder.appName("UberEats-AdvancedSQL").getOrCreate()
    
    # Load data and create views
    restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
    orders_df = spark.read.json("./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl")
    
    restaurants_df.createOrReplaceTempView("restaurants")
    orders_df.createOrReplaceTempView("orders")
    
    # Example 1: Window Functions
    print("Restaurant Rankings by Cuisine Type:")
    spark.sql("""
    WITH rankings AS (
        SELECT
            name,
            cuisine_type,
            average_rating,
            num_reviews,
            RANK() OVER (PARTITION BY cuisine_type ORDER BY average_rating DESC) as rank
        FROM restaurants
    )
    SELECT * FROM rankings WHERE rank <= 2
    """).show()
    
    # Example 2: Common Table Expressions (CTEs)
    print("\nRestaurants with Above Average Ratings:")
    spark.sql("""
    WITH stats AS (
        SELECT 
            cuisine_type,
            AVG(average_rating) AS cuisine_avg_rating
        FROM restaurants
        GROUP BY cuisine_type
    )
    SELECT 
        r.name,
        r.cuisine_type,
        r.average_rating,
        s.cuisine_avg_rating,
        r.average_rating - s.cuisine_avg_rating AS rating_difference
    FROM restaurants r
    JOIN stats s ON r.cuisine_type = s.cuisine_type
    WHERE r.average_rating > s.cuisine_avg_rating
    ORDER BY rating_difference DESC
    """).show()
    
    # Example 3: SQL-based UDFs
    print("\nCategory Labels Based on Ratings:")
    spark.sql("""
    SELECT
        name,
        cuisine_type,
        average_rating,
        CASE
            WHEN average_rating >= 4.5 THEN 'Excellent'
            WHEN average_rating >= 4.0 THEN 'Very Good'
            WHEN average_rating >= 3.5 THEN 'Good'
            WHEN average_rating >= 3.0 THEN 'Average'
            ELSE 'Below Average'
        END AS rating_category
    FROM restaurants
    ORDER BY average_rating DESC
    """).show()
    
    spark.stop()

if __name__ == "__main__":
    demonstrate_advanced_sql()
```

## 6. Integrating SQL and Python

```python
# app/spark_sql_hybrid.py

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, expr

def hybrid_sql_python():
    """Demonstrate combining SQL and Python APIs"""
    spark = SparkSession.builder.appName("UberEats-HybridSQL").getOrCreate()
    
    # Load data
    restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
    orders_df = spark.read.json("./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl")
    
    # Register temporary view
    restaurants_df.createOrReplaceTempView("restaurants")
    
    # Approach 1: Start with SQL, then use Python API
    top_restaurants = spark.sql("""
    SELECT
        cuisine_type,
        name,
        average_rating,
        num_reviews
    FROM restaurants
    WHERE average_rating > 4.0
    """)
    
    # Continue processing with Python API
    result1 = top_restaurants \
        .groupBy("cuisine_type") \
        .agg({"average_rating": "avg", "num_reviews": "sum"}) \
        .withColumnRenamed("avg(average_rating)", "avg_rating") \
        .withColumnRenamed("sum(num_reviews)", "total_reviews") \
        .orderBy(col("total_reviews").desc())
    
    print("Approach 1 - SQL to Python API:")
    result1.show()
    
    # Approach 2: Start with Python API, then use SQL
    # Filter and select with Python API
    filtered_df = restaurants_df \
        .filter(col("average_rating") > 3.5) \
        .select("restaurant_id", "name", "cuisine_type", "average_rating", "city")
    
    # Register as view and continue with SQL
    filtered_df.createOrReplaceTempView("filtered_restaurants")
    
    result2 = spark.sql("""
    SELECT
        city,
        COUNT(*) AS restaurant_count,
        ROUND(AVG(average_rating), 2) AS avg_rating
    FROM filtered_restaurants
    GROUP BY city
    HAVING COUNT(*) > 1
    ORDER BY restaurant_count DESC
    """)
    
    print("\nApproach 2 - Python API to SQL:")
    result2.show()
    
    # Approach 3: SQL expressions within Python API
    result3 = restaurants_df \
        .groupBy("cuisine_type") \
        .agg(
            expr("ROUND(AVG(average_rating), 2) AS avg_rating"),
            expr("SUM(num_reviews) AS total_reviews")
        ) \
        .orderBy(col("avg_rating").desc())
    
    print("\nApproach 3 - SQL Expressions in Python API:")
    result3.show()
    
    spark.stop()

if __name__ == "__main__":
    hybrid_sql_python()
```

## 7. Documentation and Maintenance

When using SQL as your primary development tool, documentation becomes crucial. Here's a small example of a well-documented SQL pipeline:

```python
# app/spark_sql_documented.py

from pyspark.sql import SparkSession
import os

"""
UberEats Data Analysis Pipeline
===============================

This module performs analysis on the UberEats dataset using primarily SQL queries.
The pipeline processes restaurant, user, and order data to generate insights.

Key operations:
1. Data cleaning and preparation
2. Restaurant analysis by cuisine and location
3. Order value analysis
4. Customer-restaurant matching

Each step is implemented as a separate SQL query for modularity and readability.
"""

def create_spark_session():
    """Create and configure a Spark session optimized for SQL workloads"""
    return SparkSession.builder \
        .appName("UberEats-DocumentedSQL") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.sql.adaptive.skewJoin.enabled", "true") \
        .getOrCreate()

def load_data(spark):
    """
    Load data from JSON files and create temporary views
    
    Parameters
    ----------
    spark : SparkSession
        Active Spark session
        
    Returns
    -------
    None
        Data is registered as temporary views
    """
    # Load restaurant data
    restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
    restaurants_df.createOrReplaceTempView("restaurants_raw")
    
    # Load user data
    users_df = spark.read.json("./storage/mongodb/users/01JS4W5A7WWZBQ6Y1C465EYR76.jsonl")
    users_df.createOrReplaceTempView("users_raw")
    
    # Load order data
    orders_df = spark.read.json("./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl")
    orders_df.createOrReplaceTempView("orders_raw")
    
    print(f"Loaded restaurants: {restaurants_df.count()}")
    print(f"Loaded users: {users_df.count()}")
    print(f"Loaded orders: {orders_df.count()}")

def clean_data(spark):
    """
    Clean and prepare data for analysis
    
    Creates cleaned views with consistent formats and handles missing values
    """
    # Clean restaurant data
    spark.sql("""
    -- Clean restaurant data
    -- Standardize phone numbers, handle missing fields, and filter valid records
    CREATE OR REPLACE TEMPORARY VIEW restaurants AS
    SELECT
        restaurant_id,
        name,
        COALESCE(cuisine_type, 'Unknown') AS cuisine_type,
        city,
        COALESCE(average_rating, 0) AS average_rating,
        COALESCE(num_reviews, 0) AS num_reviews,
        REGEXP_REPLACE(phone_number, '[\\(\\)\\s-]', '') AS phone_number
    FROM restaurants_raw
    WHERE restaurant_id IS NOT NULL
    """)
    
    # Clean user data
    spark.sql("""
    -- Clean user data
    -- Standardize phone numbers and emails
    CREATE OR REPLACE TEMPORARY VIEW users AS
    SELECT
        user_id,
        LOWER(email) AS email,
        city,
        REGEXP_REPLACE(phone_number, '[\\(\\)\\s-]', '') AS phone_number
    FROM users_raw
    WHERE user_id IS NOT NULL
    """)
    
    # Clean order data
    spark.sql("""
    -- Clean order data
    CREATE OR REPLACE TEMPORARY VIEW orders AS
    SELECT
        order_id,
        user_key,
        restaurant_key,
        driver_key,
        CAST(COALESCE(total_amount, 0) AS DECIMAL(10,2)) AS total_amount,
        TO_TIMESTAMP(order_date) AS order_date
    FROM orders_raw
    WHERE order_id IS NOT NULL
    """)

def analyze_restaurants(spark):
    """Analyze restaurants by cuisine type and ratings"""
    return spark.sql("""
    -- Restaurant analysis by cuisine type
    -- Calculates average ratings, review counts, and distribution
    SELECT
        cuisine_type,
        COUNT(*) AS restaurant_count,
        ROUND(AVG(average_rating), 2) AS avg_rating,
        SUM(num_reviews) AS total_reviews,
        MIN(average_rating) AS min_rating,
        MAX(average_rating) AS max_rating
    FROM restaurants
    GROUP BY cuisine_type
    ORDER BY restaurant_count DESC
    """)

def analyze_cities(spark):
    """Analyze restaurant and user distribution by city"""
    return spark.sql("""
    -- City distribution analysis
    -- Matches restaurants and users by location
    SELECT
        r.city,
        COUNT(DISTINCT r.restaurant_id) AS restaurant_count,
        COUNT(DISTINCT u.user_id) AS user_count,
        ROUND(AVG(r.average_rating), 2) AS avg_restaurant_rating
    FROM restaurants r
    LEFT JOIN users u ON r.city = u.city
    GROUP BY r.city
    HAVING COUNT(DISTINCT u.user_id) > 0
    ORDER BY restaurant_count DESC
    """)

def run_pipeline():
    """Run the full analysis pipeline"""
    spark = create_spark_session()
    
    try:
        # Load data
        load_data(spark)
        
        # Clean and prepare data
        clean_data(spark)
        
        # Run analyses
        restaurant_analysis = analyze_restaurants(spark)
        city_analysis = analyze_cities(spark)
        
        # Output results
        print("\nRestaurant Analysis by Cuisine:")
        restaurant_analysis.show()
        
        print("\nCity Analysis:")
        city_analysis.show()
        
        # Save results
        output_dir = "./output/documented_sql"
        os.makedirs(output_dir, exist_ok=True)
        
        restaurant_analysis.write.mode("overwrite").parquet(f"{output_dir}/restaurant_analysis")
        city_analysis.write.mode("overwrite").parquet(f"{output_dir}/city_analysis")
        
    finally:
        spark.stop()

if __name__ == "__main__":
    run_pipeline()
```

## 8. Best Practices for Spark SQL Development

1. **Use Views Strategically**
   - Temporary views for intermediate results
   - Global temporary views for cross-session sharing
   - Permanent views for production pipelines

2. **Document SQL Queries**
   - Add descriptive comments to explain complex logic
   - Document table schemas and relationships
   - Explain any non-obvious transformations

3. **Optimize for Performance**
   - Use appropriate joins (e.g., broadcast joins for small tables)
   - Filter early to reduce data volume
   - Use window functions instead of self-joins when possible

4. **Maintain Readability**
   - Format queries consistently
   - Use CTEs instead of nested subqueries
   - Break complex queries into smaller, reusable views

5. **Version Control SQL Code**
   - Store queries in version-controlled files
   - Use parameterized queries when possible
   - Test queries with different data volumes

## 9. Conclusion

Spark SQL provides a powerful, declarative approach to data processing that combines the familiarity of SQL with the distributed computing power of Spark. By using SQL as your primary development tool, you can:

- Write more concise and readable code
- Leverage Spark's automatic query optimization
- Make your data pipelines accessible to a wider audience
- Easily document and maintain your analytics workflows

Whether you're coming from a traditional database background or are new to SQL, Spark SQL offers a productive and efficient way to process large-scale data.
