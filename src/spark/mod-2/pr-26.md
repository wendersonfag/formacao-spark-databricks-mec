# Building an End-to-End PySpark Pipeline for UberEats

This guide walks through developing a complete data pipeline for the UberEats case study using PySpark. We'll focus on modular code structure, error handling, and monitoring.

## 1. Project Structure

Let's organize our code in a modular, maintainable way:

```
src/
  app/
    main.py                 # Pipeline entry point
    config.py               # Configuration settings
    extractors.py           # Data extraction components
    transformers.py         # Data transformation components
    loaders.py              # Data loading components
    utils/
      error_handlers.py     # Error handling utilities
      validators.py         # Data validation utilities
      logging_utils.py      # Logging utilities
```

## 2. Configuration Setup (config.py)

```python
# app/config.py
import os
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType

class PipelineConfig:
    # Input paths
    RESTAURANTS_PATH = "./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl"
    USERS_PATH = "./storage/mongodb/users/01JS4W5A7WWZBQ6Y1C465EYR76.jsonl"
    DRIVERS_PATH = "./storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl"
    ORDERS_PATH = "./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl"
    
    # Output paths
    OUTPUT_BASE = "./output/ubereats"
    RESTAURANTS_OUTPUT = f"{OUTPUT_BASE}/restaurants_analysis"
    USERS_OUTPUT = f"{OUTPUT_BASE}/users_analysis"
    ORDERS_OUTPUT = f"{OUTPUT_BASE}/orders_analysis"
    DELIVERY_OUTPUT = f"{OUTPUT_BASE}/delivery_analysis"
    
    # Spark configuration
    SPARK_CONF = {
        "spark.sql.shuffle.partitions": "20",
        "spark.driver.memory": "4g",
        "spark.executor.memory": "4g",
        "spark.sql.adaptive.enabled": "true"
    }
    
    # Schema definitions for validation
    RESTAURANT_SCHEMA = StructType([
        StructField("restaurant_id", IntegerType(), True),
        StructField("name", StringType(), True),
        StructField("city", StringType(), True),
        StructField("cuisine_type", StringType(), True),
        StructField("average_rating", DoubleType(), True),
        StructField("num_reviews", IntegerType(), True),
        # Additional fields as needed
    ])
    
    # Thresholds for data quality checks
    MIN_RESTAURANTS = 1
    MIN_USERS = 1
    MIN_DRIVERS = 1
    MIN_ORDERS = 1
    
    # Logging configuration
    LOG_LEVEL = "INFO"
    LOG_FORMAT = "%(asctime)s - %(levelname)s - %(message)s"
    LOG_FILE = "./logs/ubereats_pipeline.log"
```

## 3. Logging Setup (utils/logging_utils.py)

```python
# app/utils/logging_utils.py
import logging
import os
from datetime import datetime
from app.config import PipelineConfig

class PipelineLogger:
    def __init__(self, name="UberEatsPipeline"):
        self.name = name
        self._setup_logger()
        
    def _setup_logger(self):
        # Create logs directory if it doesn't exist
        log_dir = os.path.dirname(PipelineConfig.LOG_FILE)
        os.makedirs(log_dir, exist_ok=True)
        
        # Configure logger
        self.logger = logging.getLogger(self.name)
        self.logger.setLevel(getattr(logging, PipelineConfig.LOG_LEVEL))
        
        # Create file handler
        file_handler = logging.FileHandler(PipelineConfig.LOG_FILE)
        file_formatter = logging.Formatter(PipelineConfig.LOG_FORMAT)
        file_handler.setFormatter(file_formatter)
        
        # Create console handler
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(file_formatter)
        
        # Add handlers
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
    
    def log_job_start(self, job_name):
        """Log the start of a job with a timestamp"""
        self.logger.info(f"Starting job: {job_name} at {datetime.now()}")
        
    def log_job_end(self, job_name, rows_processed=None):
        """Log the end of a job with a timestamp and optionally rows processed"""
        if rows_processed:
            self.logger.info(f"Completed job: {job_name} at {datetime.now()}. Processed {rows_processed} rows")
        else:
            self.logger.info(f"Completed job: {job_name} at {datetime.now()}")
            
    def log_checkpoint(self, message):
        """Log a checkpoint in the pipeline"""
        self.logger.info(f"CHECKPOINT: {message}")
        
    def log_error(self, message, error=None):
        """Log an error with details"""
        if error:
            self.logger.error(f"{message}: {str(error)}")
        else:
            self.logger.error(message)
    
    def get_logger(self):
        """Return the configured logger"""
        return self.logger
```

## 4. Error Handling (utils/error_handlers.py)

```python
# app/utils/error_handlers.py
from functools import wraps
import traceback
from pyspark.sql.utils import AnalysisException, ParseException
from app.utils.logging_utils import PipelineLogger

logger = PipelineLogger().get_logger()

class PipelineError(Exception):
    """Base exception for pipeline errors"""
    pass

class DataQualityError(PipelineError):
    """Exception raised for data quality issues"""
    pass

class SchemaValidationError(PipelineError):
    """Exception raised for schema validation failures"""
    pass

class ProcessingError(PipelineError):
    """Exception raised for data processing errors"""
    pass

def handle_spark_exceptions(func):
    """Decorator to handle Spark exceptions"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except AnalysisException as e:
            logger.error(f"Spark Analysis Error in {func.__name__}: {str(e)}")
            raise ProcessingError(f"Spark Analysis Error: {str(e)}")
        except ParseException as e:
            logger.error(f"Spark Parsing Error in {func.__name__}: {str(e)}")
            raise ProcessingError(f"Spark Parsing Error: {str(e)}")
        except Exception as e:
            logger.error(f"Error in {func.__name__}: {str(e)}")
            logger.error(traceback.format_exc())
            raise ProcessingError(f"Processing Error: {str(e)}")
    return wrapper

def retry_operation(max_retries=3, delay_seconds=5):
    """Decorator to retry operations with exponential backoff"""
    import time
    
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(1, max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    logger.warning(f"Attempt {attempt}/{max_retries} failed: {str(e)}")
                    if attempt == max_retries:
                        logger.error(f"All {max_retries} attempts failed")
                        raise
                    time.sleep(delay_seconds * attempt)
        return wrapper
    return decorator
```

## 5. Data Validation (utils/validators.py)

```python
# app/utils/validators.py
from pyspark.sql import DataFrame
from app.utils.error_handlers import DataQualityError, SchemaValidationError
from app.utils.logging_utils import PipelineLogger

logger = PipelineLogger().get_logger()

def validate_schema(df: DataFrame, expected_schema, name: str) -> None:
    """Validate that a DataFrame matches the expected schema"""
    logger.info(f"Validating schema for {name}")
    
    # Compare the fields in both schemas
    df_fields = set(field.name for field in df.schema.fields)
    expected_fields = set(field.name for field in expected_schema.fields)
    
    # Check for missing fields
    missing_fields = expected_fields - df_fields
    if missing_fields:
        error_msg = f"Missing required fields in {name}: {missing_fields}"
        logger.error(error_msg)
        raise SchemaValidationError(error_msg)
    
    logger.info(f"Schema validation passed for {name}")

def validate_row_count(df: DataFrame, min_rows: int, name: str) -> None:
    """Validate that a DataFrame has at least the minimum number of rows"""
    row_count = df.count()
    logger.info(f"Row count for {name}: {row_count}")
    
    if row_count < min_rows:
        error_msg = f"DataFrame {name} has {row_count} rows, expected at least {min_rows}"
        logger.error(error_msg)
        raise DataQualityError(error_msg)

def validate_no_nulls(df: DataFrame, columns: list, name: str) -> None:
    """Validate that specified columns have no null values"""
    for column in columns:
        if column not in df.columns:
            logger.warning(f"Column {column} not found in {name}, skipping null check")
            continue
            
        null_count = df.filter(df[column].isNull()).count()
        if null_count > 0:
            error_msg = f"Found {null_count} null values in {name}.{column}"
            logger.error(error_msg)
            raise DataQualityError(error_msg)
    
    logger.info(f"No-nulls validation passed for {name} on columns {columns}")

def log_data_profile(df: DataFrame, name: str) -> None:
    """Log basic profile of the DataFrame for monitoring"""
    row_count = df.count()
    column_count = len(df.columns)
    
    # Get count of nulls per column
    null_counts = {col: df.filter(df[col].isNull()).count() for col in df.columns}
    
    logger.info(f"Data profile for {name}:")
    logger.info(f"  - Row count: {row_count}")
    logger.info(f"  - Column count: {column_count}")
    logger.info(f"  - Null counts: {null_counts}")
    
    # Log a few sample rows for inspection
    logger.info(f"  - Sample data: {df.limit(2).collect()}")
```

## 6. Data Extraction (extractors.py)

```python
# app/extractors.py
from pyspark.sql import SparkSession, DataFrame
from app.config import PipelineConfig
from app.utils.error_handlers import handle_spark_exceptions, retry_operation
from app.utils.validators import validate_schema, validate_row_count, log_data_profile
from app.utils.logging_utils import PipelineLogger

logger = PipelineLogger().get_logger()

class DataExtractor:
    def __init__(self, spark: SparkSession):
        self.spark = spark
        
    @handle_spark_exceptions
    @retry_operation(max_retries=3)
    def extract_restaurants(self) -> DataFrame:
        """Extract restaurant data"""
        logger.log_job_start("extract_restaurants")
        
        # Read the data
        restaurants_df = self.spark.read.json(PipelineConfig.RESTAURANTS_PATH)
        
        # Validate the data
        validate_schema(restaurants_df, PipelineConfig.RESTAURANT_SCHEMA, "restaurants")
        validate_row_count(restaurants_df, PipelineConfig.MIN_RESTAURANTS, "restaurants")
        
        # Log data profile
        log_data_profile(restaurants_df, "restaurants")
        
        logger.log_job_end("extract_restaurants", restaurants_df.count())
        return restaurants_df
    
    @handle_spark_exceptions
    @retry_operation(max_retries=3)
    def extract_users(self) -> DataFrame:
        """Extract user data"""
        logger.log_job_start("extract_users")
        
        # Read the data
        users_df = self.spark.read.json(PipelineConfig.USERS_PATH)
        
        # Validate the data
        validate_row_count(users_df, PipelineConfig.MIN_USERS, "users")
        
        # Log data profile
        log_data_profile(users_df, "users")
        
        logger.log_job_end("extract_users", users_df.count())
        return users_df
    
    @handle_spark_exceptions
    @retry_operation(max_retries=3)
    def extract_drivers(self) -> DataFrame:
        """Extract driver data"""
        logger.log_job_start("extract_drivers")
        
        # Read the data
        drivers_df = self.spark.read.json(PipelineConfig.DRIVERS_PATH)
        
        # Validate the data
        validate_row_count(drivers_df, PipelineConfig.MIN_DRIVERS, "drivers")
        
        # Log data profile
        log_data_profile(drivers_df, "drivers")
        
        logger.log_job_end("extract_drivers", drivers_df.count())
        return drivers_df
    
    @handle_spark_exceptions
    @retry_operation(max_retries=3)
    def extract_orders(self) -> DataFrame:
        """Extract order data"""
        logger.log_job_start("extract_orders")
        
        # Read the data
        orders_df = self.spark.read.json(PipelineConfig.ORDERS_PATH)
        
        # Validate the data
        validate_row_count(orders_df, PipelineConfig.MIN_ORDERS, "orders")
        
        # Log data profile
        log_data_profile(orders_df, "orders")
        
        logger.log_job_end("extract_orders", orders_df.count())
        return orders_df
```

## 7. Data Transformation (transformers.py)

```python
# app/transformers.py
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.functions import col, regexp_replace, round, avg, count, desc, when, lit
from app.utils.error_handlers import handle_spark_exceptions
from app.utils.logging_utils import PipelineLogger

logger = PipelineLogger().get_logger()

class DataTransformer:
    def __init__(self, spark: SparkSession):
        self.spark = spark
    
    @handle_spark_exceptions
    def clean_restaurants(self, restaurants_df: DataFrame) -> DataFrame:
        """Clean and standardize restaurant data"""
        logger.log_job_start("clean_restaurants")
        
        # Clean phone numbers
        cleaned_df = restaurants_df.withColumn(
            "phone_number",
            regexp_replace(col("phone_number"), "[\\(\\)\\s-]", "")
        )
        
        # Handle missing values
        cleaned_df = cleaned_df.fillna({
            "cuisine_type": "Unknown",
            "average_rating": 0.0,
            "num_reviews": 0
        })
        
        # Add derived columns
        cleaned_df = cleaned_df.withColumn(
            "rating_category",
            when(col("average_rating") >= 4.5, "Excellent")
            .when(col("average_rating") >= 4.0, "Very Good")
            .when(col("average_rating") >= 3.5, "Good")
            .when(col("average_rating") >= 3.0, "Average")
            .otherwise("Below Average")
        )
        
        logger.log_job_end("clean_restaurants", cleaned_df.count())
        return cleaned_df
    
    @handle_spark_exceptions
    def analyze_restaurants_by_cuisine(self, restaurants_df: DataFrame) -> DataFrame:
        """Group and analyze restaurants by cuisine type"""
        logger.log_job_start("analyze_restaurants_by_cuisine")
        
        cuisine_analysis = restaurants_df.groupBy("cuisine_type") \
            .agg(
                count("restaurant_id").alias("restaurant_count"),
                round(avg("average_rating"), 2).alias("avg_rating"),
                round(avg("num_reviews"), 2).alias("avg_reviews")
            ) \
            .orderBy(desc("restaurant_count"))
        
        logger.log_job_end("analyze_restaurants_by_cuisine", cuisine_analysis.count())
        return cuisine_analysis
    
    @handle_spark_exceptions
    def clean_users(self, users_df: DataFrame) -> DataFrame:
        """Clean and standardize user data"""
        logger.log_job_start("clean_users")
        
        # Clean phone numbers
        cleaned_df = users_df.withColumn(
            "phone_number",
            regexp_replace(col("phone_number"), "[\\(\\)\\s-]", "")
        )
        
        logger.log_job_end("clean_users", cleaned_df.count())
        return cleaned_df
    
    @handle_spark_exceptions
    def match_users_with_restaurants(self, users_df: DataFrame, restaurants_df: DataFrame) -> DataFrame:
        """Match users with restaurants in the same city"""
        logger.log_job_start("match_users_with_restaurants")
        
        # Join users and restaurants on city
        city_matches = users_df.join(
            restaurants_df.select("restaurant_id", "name", "city", "cuisine_type", "average_rating"),
            on="city",
            how="inner"
        )
        
        # Add a column for distance calculation (simplified here)
        # In a real scenario, you would calculate actual distances
        city_matches = city_matches.withColumn("estimated_distance_km", lit(5.0))
        
        logger.log_job_end("match_users_with_restaurants", city_matches.count())
        return city_matches
    
    @handle_spark_exceptions
    def analyze_orders(self, orders_df: DataFrame) -> DataFrame:
        """Analyze order data for insights"""
        logger.log_job_start("analyze_orders")
        
        # Extract date components and calculate aggregates
        from pyspark.sql.functions import to_timestamp, year, month, dayofweek, hour
        
        order_analysis = orders_df \
            .withColumn("order_timestamp", to_timestamp(col("order_date"))) \
            .withColumn("year", year(col("order_timestamp"))) \
            .withColumn("month", month(col("order_timestamp"))) \
            .withColumn("day_of_week", dayofweek(col("order_timestamp"))) \
            .withColumn("hour", hour(col("order_timestamp"))) \
            .groupBy("year", "month") \
            .agg(
                count("order_id").alias("order_count"),
                round(avg("total_amount"), 2).alias("avg_order_value")
            ) \
            .orderBy("year", "month")
        
        logger.log_job_end("analyze_orders", order_analysis.count())
        return order_analysis
```

## 8. Data Loading (loaders.py)

```python
# app/loaders.py
import os
from pyspark.sql import DataFrame, SparkSession
from app.config import PipelineConfig
from app.utils.error_handlers import handle_spark_exceptions, retry_operation
from app.utils.logging_utils import PipelineLogger

logger = PipelineLogger().get_logger()

class DataLoader:
    def __init__(self, spark: SparkSession):
        self.spark = spark
        
    def _ensure_output_dir(self, path: str) -> None:
        """Ensure output directory exists"""
        os.makedirs(os.path.dirname(path), exist_ok=True)
    
    @handle_spark_exceptions
    @retry_operation(max_retries=3)
    def save_restaurants_analysis(self, df: DataFrame) -> None:
        """Save restaurant analysis results"""
        logger.log_job_start("save_restaurants_analysis")
        
        self._ensure_output_dir(PipelineConfig.RESTAURANTS_OUTPUT)
        
        # Save as parquet
        df.write.mode("overwrite").parquet(PipelineConfig.RESTAURANTS_OUTPUT)
        
        logger.log_job_end("save_restaurants_analysis")
    
    @handle_spark_exceptions
    @retry_operation(max_retries=3)
    def save_users_analysis(self, df: DataFrame) -> None:
        """Save user analysis results"""
        logger.log_job_start("save_users_analysis")
        
        self._ensure_output_dir(PipelineConfig.USERS_OUTPUT)
        
        # Save as parquet
        df.write.mode("overwrite").parquet(PipelineConfig.USERS_OUTPUT)
        
        logger.log_job_end("save_users_analysis")
    
    @handle_spark_exceptions
    @retry_operation(max_retries=3)
    def save_orders_analysis(self, df: DataFrame) -> None:
        """Save order analysis results"""
        logger.log_job_start("save_orders_analysis")
        
        self._ensure_output_dir(PipelineConfig.ORDERS_OUTPUT)
        
        # Save as parquet
        df.write.mode("overwrite").parquet(PipelineConfig.ORDERS_OUTPUT)
        
        logger.log_job_end("save_orders_analysis")
    
    @handle_spark_exceptions
    @retry_operation(max_retries=3)
    def save_delivery_analysis(self, df: DataFrame) -> None:
        """Save delivery analysis results"""
        logger.log_job_start("save_delivery_analysis")
        
        self._ensure_output_dir(PipelineConfig.DELIVERY_OUTPUT)
        
        # Save as parquet
        df.write.mode("overwrite").parquet(PipelineConfig.DELIVERY_OUTPUT)
        
        logger.log_job_end("save_delivery_analysis")
```

## 9. Pipeline Entry Point (main.py)

```python
# app/main.py
from pyspark.sql import SparkSession
from app.config import PipelineConfig
from app.extractors import DataExtractor
from app.transformers import DataTransformer
from app.loaders import DataLoader
from app.utils.logging_utils import PipelineLogger
import argparse
import time

def create_spark_session() -> SparkSession:
    """Create and configure Spark session"""
    # Create builder with configurations
    builder = SparkSession.builder.appName("UberEats-Pipeline")
    
    # Apply configurations from config
    for key, value in PipelineConfig.SPARK_CONF.items():
        builder = builder.config(key, value)
    
    return builder.getOrCreate()

def run_pipeline(env: str = "dev"):
    """Run the UberEats data pipeline"""
    # Set up logging
    logger = PipelineLogger(__name__).get_logger()
    start_time = time.time()
    
    logger.info(f"Starting UberEats pipeline in {env} environment")
    
    try:
        # Create Spark session
        spark = create_spark_session()
        logger.info(f"Created Spark session: {spark.sparkContext.appName}")
        
        # Initialize pipeline components
        extractor = DataExtractor(spark)
        transformer = DataTransformer(spark)
        loader = DataLoader(spark)
        
        # Extract data
        logger.info("Extracting data...")
        restaurants_df = extractor.extract_restaurants()
        users_df = extractor.extract_users()
        orders_df = extractor.extract_orders()
        drivers_df = extractor.extract_drivers()
        
        # Transform data
        logger.info("Transforming data...")
        clean_restaurants_df = transformer.clean_restaurants(restaurants_df)
        restaurant_cuisine_analysis = transformer.analyze_restaurants_by_cuisine(clean_restaurants_df)
        
        clean_users_df = transformer.clean_users(users_df)
        city_matches = transformer.match_users_with_restaurants(clean_users_df, clean_restaurants_df)
        
        order_analysis = transformer.analyze_orders(orders_df)
        
        # Load results
        logger.info("Loading results...")
        loader.save_restaurants_analysis(restaurant_cuisine_analysis)
        loader.save_users_analysis(clean_users_df)
        loader.save_orders_analysis(order_analysis)
        loader.save_delivery_analysis(city_matches)
        
        # Log success
        execution_time = round(time.time() - start_time, 2)
        logger.info(f"Pipeline completed successfully in {execution_time} seconds")
        
        # Stop Spark session
        spark.stop()
        
    except Exception as e:
        logger.error(f"Pipeline failed: {str(e)}")
        raise

def main():
    """Main entry point with argument parsing"""
    parser = argparse.ArgumentParser(description="UberEats Data Pipeline")
    parser.add_argument("--env", choices=["dev", "test", "prod"], default="dev", 
                        help="Environment to run the pipeline in")
    args = parser.parse_args()
    
    run_pipeline(args.env)

if __name__ == "__main__":
    main()
```

## 10. Running the Pipeline

1. **Install Dependencies**:
   ```bash
   pip install pyspark pandas
   ```

2. **Create Log Directory**:
   ```bash
   mkdir -p logs
   ```

3. **Run the Pipeline**:
   ```bash
   cd src
   python -m app.main --env dev
   ```

## 11. Key Concepts Demonstrated

This pipeline demonstrates several important concepts:

1. **Modular Architecture**: Separation of concerns with extractors, transformers, and loaders
2. **Error Handling**: Comprehensive exception handling with retry mechanisms
3. **Logging and Monitoring**: Detailed logging for operations and data quality
4. **Configuration Management**: Centralized configuration
5. **Data Validation**: Schema validation and data quality checks
6. **Best Practices**: Decorators for cross-cutting concerns

## 12. Next Steps

1. **Testing**: Add unit and integration tests
2. **Orchestration**: Integrate with Airflow or other orchestration tools
3. **Monitoring Dashboard**: Create a monitoring dashboard using logs
4. **Performance Optimization**: Fine-tune partitioning and caching
5. **Pipeline Parameters**: Add more configurable parameters

This end-to-end pipeline provides a solid foundation for real-world data processing tasks and demonstrates professional software engineering practices applied to PySpark development.
