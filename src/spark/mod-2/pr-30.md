# Deep Dive & Best Practices with PySpark

This guide explores advanced techniques for optimizing PySpark applications, complex debugging strategies, and expert-level approaches to solving real-world data engineering challenges.

## 1. Understanding Spark Execution

Before we can optimize, we need to understand how Spark executes tasks.

### Spark Execution Model

```python
# app/spark_execution.py
from pyspark.sql import SparkSession

def explain_execution():
    """Demonstrate Spark's execution model with a simple example"""
    spark = SparkSession.builder.appName("ExecutionModel").getOrCreate()
    
    # Load sample data
    restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
    
    # Create a simple transformation
    high_rated = restaurants_df.filter("average_rating > 4.0") \
                               .select("restaurant_id", "name", "cuisine_type", "average_rating")
    
    # Examine the execution plan
    print("Logical Plan:")
    high_rated.explain()
    
    print("\nPhysical Plan:")
    high_rated.explain(mode="extended")
    
    # Trigger execution and show results
    print(f"\nFound {high_rated.count()} high-rated restaurants")
    high_rated.show(5)
    
    spark.stop()
```

### Understanding DAG Visualization

Spark's execution is based on a Directed Acyclic Graph (DAG). To visualize this:

1. Enable the Spark UI: `spark.conf.set("spark.ui.enabled", "true")`
2. Navigate to `http://localhost:4040` when running a Spark job
3. Examine the "Jobs" and "Stages" tabs to understand execution

## 2. Performance Optimization Techniques

Let's explore key techniques for optimizing PySpark applications.

### The 5 S's of Spark Optimization

```python
# app/optimization_techniques.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, broadcast
import os

def demonstrate_optimization():
    """Demonstrate the 5 S's of Spark optimization"""
    spark = SparkSession.builder \
        .appName("SparkOptimization") \
        .config("spark.sql.shuffle.partitions", "10") \
        .config("spark.default.parallelism", "10") \
        .getOrCreate()
    
    # Load data
    restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
    orders_df = spark.read.json("./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl")
    
    # 1. Shuffle Optimization
    # Reduce the number of partitions for small data
    spark.conf.set("spark.sql.shuffle.partitions", "10")
    
    # 2. Skew Optimization
    # Handle skewed data by salting
    def add_salt(df, salt_column, salt_count=10):
        """Add a salt column to help distribute skewed data"""
        from pyspark.sql.functions import round, rand
        salted_df = df.withColumn("salt", (round(rand() * (salt_count - 1))).cast("int"))
        return salted_df
    
    # Example of salting for skewed JOIN
    # salted_restaurants = add_salt(restaurants_df, "cuisine_type")
    # Then join on both key and salt
    
    # 3. Storage Optimization
    # Use proper data format
    def save_with_optimal_format(df, path):
        """Save data in parquet format with optimal compression"""
        df.write.mode("overwrite").option("compression", "snappy").parquet(path)
    
    # Example: 
    # save_with_optimal_format(restaurants_df, "./output/optimized_restaurants")
    
    # 4. Serialization Optimization
    # Configure Kryo serialization
    spark.conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    # Register custom classes
    # spark.conf.set("spark.kryo.registrator", "com.example.MyRegistrator")
    
    # 5. Spill Optimization
    # Configure memory thresholds
    spark.conf.set("spark.memory.fraction", "0.8")  # % of heap used for execution/storage
    spark.conf.set("spark.memory.storageFraction", "0.3")  # % of memory.fraction used for storage
    
    # Additional optimizations
    
    # 6. Broadcast Joins
    # Use broadcast join for small tables
    small_df = spark.range(1, 1000).toDF("id")
    
    # Manual broadcast
    result = orders_df.join(broadcast(small_df), 
                            orders_df.order_id == small_df.id, 
                            "left")
    
    # 7. Predicate Pushdown
    # Filter early to reduce data volume
    filtered_restaurants = restaurants_df.filter(col("average_rating") > 3.0)
    
    # 8. Caching Strategies
    # Cache frequently used DataFrames
    filtered_restaurants.cache()
    
    # 9. Repartition vs Coalesce
    # Coalesce (no shuffle) when reducing partitions
    smaller_df = filtered_restaurants.coalesce(2)
    
    # Repartition (triggers shuffle) when increasing partitions
    bigger_df = filtered_restaurants.repartition(20)
    
    # Show execution plans for comparison
    print("Execution plan with optimization:")
    filtered_restaurants.join(broadcast(small_df), 
                             filtered_restaurants.restaurant_id == small_df.id, 
                             "left").explain()
    
    # Clean up
    smaller_df.unpersist()
    spark.stop()
```

### Memory Management Deep Dive

```python
# app/memory_management.py
from pyspark.sql import SparkSession
import gc
import os

def demonstrate_memory_management():
    """Demonstrate advanced memory management techniques"""
    spark = SparkSession.builder \
        .appName("MemoryManagement") \
        .config("spark.memory.offHeap.enabled", "true") \
        .config("spark.memory.offHeap.size", "1g") \
        .getOrCreate()
    
    # Load sample data
    restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
    
    # 1. Understanding Storage Levels
    from pyspark.storagelevel import StorageLevel
    
    # Different storage options
    restaurants_df.persist(StorageLevel.MEMORY_ONLY)  # Cache only in memory
    # restaurants_df.persist(StorageLevel.MEMORY_AND_DISK)  # Spill to disk if needed
    # restaurants_df.persist(StorageLevel.MEMORY_ONLY_SER)  # Serialize data in memory
    # restaurants_df.persist(StorageLevel.OFF_HEAP)  # Store in off-heap memory
    
    # 2. Strategic Data Unpersisting
    # After operations that need the cached data
    restaurants_df.groupBy("cuisine_type").count().show(5)
    
    # Unpersist when no longer needed
    restaurants_df.unpersist()
    
    # 3. Manual Python Garbage Collection
    gc.collect()
    
    # 4. Broadcast Variable Management
    from pyspark.sql.functions import broadcast, col
    
    # Create small lookup table
    cuisine_mapping = spark.createDataFrame(
        [("Italian", "European"), ("Chinese", "Asian"), ("Indian", "Asian")],
        ["cuisine", "region"]
    )
    
    # Broadcast and use the lookup table
    transformed = restaurants_df.join(
        broadcast(cuisine_mapping),
        restaurants_df.cuisine_type == cuisine_mapping.cuisine,
        "left"
    ).select(
        restaurants_df["*"], 
        col("region").alias("cuisine_region")
    )
    
    transformed.show(5)
    
    # 5. Managing Driver Memory
    # Collect only what you need
    top_restaurants = restaurants_df.orderBy(col("average_rating").desc()).limit(10)
    
    # Instead of collect(), consider toPandas() for small results
    top_pandas = top_restaurants.toPandas()
    print(f"Top restaurant: {top_pandas.iloc[0]['name']}")
    
    spark.stop()
```

## 3. Advanced Debugging Techniques

Debugging distributed applications requires specialized approaches.

```python
# app/advanced_debugging.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf
from pyspark.sql.types import StringType
import logging
import traceback
import sys

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("./logs/spark_debug.log"),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

def demonstrate_debugging():
    """Demonstrate advanced debugging techniques for PySpark"""
    spark = SparkSession.builder.appName("AdvancedDebugging").getOrCreate()
    
    try:
        # Load sample data
        logger.info("Loading restaurant data")
        restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
        
        # 1. Debugging Dataset Contents
        logger.info(f"Restaurant count: {restaurants_df.count()}")
        
        # Schema debugging
        logger.info("Restaurant schema:")
        restaurants_df.printSchema()
        
        # Sampling for inspection
        logger.info("Sample data:")
        sample = restaurants_df.limit(2).collect()
        for row in sample:
            logger.info(f"  {row}")
        
        # 2. Debugging UDFs
        # Problem: UDFs are a black box in Spark
        # Solution: Add explicit debugging to UDFs
        
        @udf(StringType())
        def categorize_rating(rating):
            """Categorize restaurant rating with debug info"""
            try:
                if rating is None:
                    return "Unknown"
                
                rating_val = float(rating)
                if rating_val >= 4.5:
                    return "Excellent"
                elif rating_val >= 4.0:
                    return "Very Good"
                elif rating_val >= 3.0:
                    return "Good"
                else:
                    return "Average"
            except Exception as e:
                # Log error without failing the job
                error_msg = f"Error processing rating '{rating}': {str(e)}"
                # In production, you'd want to send this to a central logging system
                return f"ERROR: {error_msg}"
        
        # Apply the UDF
        with_category = restaurants_df.withColumn(
            "rating_category", 
            categorize_rating(col("average_rating"))
        )
        
        # 3. Debugging Data Lineage
        logger.info("Execution plan for debugging:")
        with_category.explain(extended=True)
        
        # 4. Identifying and Handling Data Issues
        # Check for null values
        null_counts = {col_name: restaurants_df.filter(col(col_name).isNull()).count() 
                      for col_name in restaurants_df.columns}
        logger.info(f"Null counts: {null_counts}")
        
        # 5. Tracking Transformations
        # Create transformation tracker
        def track_transformation(df, step_name):
            """Log transformation details for debugging"""
            count = df.count()
            logger.info(f"Step: {step_name}, Row count: {count}")
            
            # Optional: Validate expected output
            if count == 0:
                logger.warning(f"Warning: Zero rows after step '{step_name}'")
            
            return df
        
        # Apply tracking to pipeline
        result = restaurants_df \
            .transform(lambda df: track_transformation(df, "initial")) \
            .filter(col("cuisine_type").isNotNull()) \
            .transform(lambda df: track_transformation(df, "filter_nulls")) \
            .withColumn("rating_category", categorize_rating(col("average_rating"))) \
            .transform(lambda df: track_transformation(df, "add_category"))
        
        result.show(5)
        
        # 6. Debugging Data Distribution Issues
        distribution = result.groupBy("cuisine_type").count().orderBy(col("count").desc())
        logger.info("Data distribution:")
        for row in distribution.limit(5).collect():
            logger.info(f"  {row.cuisine_type}: {row['count']}")
        
    except Exception as e:
        logger.error(f"Error in pipeline: {str(e)}")
        logger.error(traceback.format_exc())
    finally:
        spark.stop()
```

## 4. Serialization and Broadcast Strategies

Understand how to efficiently manage serialization in Spark.

```python
# app/serialization_strategies.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import broadcast, col, udf
from pyspark.sql.types import StringType
import json
import pickle

def demonstrate_serialization():
    """Demonstrate advanced serialization and broadcast techniques"""
    spark = SparkSession.builder \
        .appName("SerializationDemo") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .config("spark.kryo.registrationRequired", "false") \
        .getOrCreate()
    
    # Load sample data
    restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
    
    # 1. Broadcast Variables for Lookup Tables
    # Create a lookup dictionary
    cuisine_regions = {
        "Italian": "European",
        "French": "European",
        "Chinese": "Asian",
        "Japanese": "Asian",
        "Indian": "Asian",
        "Mexican": "Latin American",
        "American": "North American"
    }
    
    # Broadcast the dictionary
    broadcast_regions = spark.sparkContext.broadcast(cuisine_regions)
    
    # Use the broadcast variable in a UDF
    @udf(StringType())
    def get_region(cuisine):
        """Get region from broadcast dictionary"""
        if cuisine is None:
            return "Unknown"
        
        # Access the broadcast value
        regions = broadcast_regions.value
        return regions.get(cuisine, "Other")
    
    # Apply the UDF
    with_region = restaurants_df.withColumn("region", get_region(col("cuisine_type")))
    with_region.show(5)
    
    # 2. Handling Complex Objects
    # Create a complex object (e.g., a model)
    class RatingAnalyzer:
        def __init__(self, thresholds):
            self.thresholds = thresholds
        
        def categorize(self, rating):
            if rating is None:
                return "Unknown"
            
            for category, threshold in self.thresholds.items():
                if rating >= threshold:
                    return category
            
            return "Poor"
    
    # Create the analyzer with thresholds
    analyzer = RatingAnalyzer({
        "Excellent": 4.5,
        "Very Good": 4.0,
        "Good": 3.5,
        "Average": 3.0,
        "Below Average": 2.0
    })
    
    # Option 1: Serialize with pickle (default in Python UDFs)
    @udf(StringType())
    def categorize_with_analyzer(rating):
        """Use the complex object in a UDF"""
        if rating is None:
            return "Unknown"
        return analyzer.categorize(float(rating))
    
    # Option 2: Broadcast the serialized object explicitly
    # Pickle the analyzer
    serialized_analyzer = pickle.dumps(analyzer)
    broadcast_analyzer = spark.sparkContext.broadcast(serialized_analyzer)
    
    @udf(StringType())
    def categorize_with_broadcast(rating):
        """Use the broadcasted serialized object"""
        if rating is None:
            return "Unknown"
        
        # Unpickle the analyzer
        local_analyzer = pickle.loads(broadcast_analyzer.value)
        return local_analyzer.categorize(float(rating))
    
    # Apply both UDFs for comparison
    result = restaurants_df \
        .withColumn("category1", categorize_with_analyzer(col("average_rating"))) \
        .withColumn("category2", categorize_with_broadcast(col("average_rating")))
    
    result.select("name", "average_rating", "category1", "category2").show(5)
    
    # 3. Handling Serialization Errors
    # Example of a closure that won't serialize properly
    try:
        from datetime import datetime
        
        # This won't serialize correctly - database connections can't be pickled
        def problematic_approach():
            # Simulate a database connection
            class DBConnection:
                def __init__(self):
                    self.last_query = None
                
                def query(self, sql):
                    self.last_query = sql
                    return ["result1", "result2"]
            
            # Create a "connection"
            db = DBConnection()
            
            # Try to use it in a UDF - this will fail!
            @udf(StringType())
            def get_external_data(key):
                result = db.query(f"SELECT * FROM external_table WHERE key = {key}")
                return result[0] if result else "Not found"
            
            # Apply the UDF
            restaurants_df.withColumn("external_data", get_external_data(col("restaurant_id")))
        
        # Uncomment to see the error:
        # problematic_approach()
        
        # Better approach - create new connections inside the UDF
        @udf(StringType())
        def better_approach(key):
            # Create resources inside the UDF
            # In real code, you'd use a connection pool or similar
            db = DBConnection()
            result = db.query(f"SELECT * FROM external_table WHERE key = {key}")
            return result[0] if result else "Not found"
        
    except Exception as e:
        print(f"Expected serialization error: {str(e)}")
    
    spark.stop()
```

## 5. Real-World Case Studies

Let's look at some real-world PySpark optimization scenarios.

### Case Study 1: Processing Large Joins Efficiently

```python
# app/case_study_large_joins.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, broadcast
import time

def large_join_case_study():
    """Case study: Efficiently processing large joins"""
    spark = SparkSession.builder.appName("LargeJoinsCaseStudy").getOrCreate()
    
    # Load restaurant and order data
    restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
    orders_df = spark.read.json("./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl")
    
    # Challenge: Join large fact table (orders) with dimension table (restaurants)
    # without causing excessive shuffling
    
    # Approach 1: Naive join (may cause full shuffle)
    def naive_approach():
        start = time.time()
        
        # Extract restaurant_id from restaurant_key (simplified)
        orders_extracted = orders_df.withColumn(
            "restaurant_id_extracted", 
            col("restaurant_key").substr(1, 5).cast("int")
        )
        
        # Perform join
        result = orders_extracted.join(
            restaurants_df,
            orders_extracted.restaurant_id_extracted == restaurants_df.restaurant_id,
            "inner"
        )
        
        count = result.count()
        duration = time.time() - start
        print(f"Naive approach: {count} rows in {duration:.2f} seconds")
        
        # Check the execution plan
        result.explain()
        
        return result
    
    # Approach 2: Broadcast join (better for dimension tables)
    def broadcast_approach():
        start = time.time()
        
        # Extract restaurant_id from restaurant_key (simplified)
        orders_extracted = orders_df.withColumn(
            "restaurant_id_extracted", 
            col("restaurant_key").substr(1, 5).cast("int")
        )
        
        # Broadcast the smaller table
        result = orders_extracted.join(
            broadcast(restaurants_df),
            orders_extracted.restaurant_id_extracted == restaurants_df.restaurant_id,
            "inner"
        )
        
        count = result.count()
        duration = time.time() - start
        print(f"Broadcast approach: {count} rows in {duration:.2f} seconds")
        
        # Check the execution plan
        result.explain()
        
        return result
    
    # Approach 3: Partition-aligned join
    def partitioned_approach():
        start = time.time()
        
        # Extract restaurant_id from restaurant_key (simplified)
        orders_extracted = orders_df.withColumn(
            "restaurant_id_extracted", 
            col("restaurant_key").substr(1, 5).cast("int")
        )
        
        # Repartition both DataFrames on the join key
        orders_repartitioned = orders_extracted.repartition(5, "restaurant_id_extracted")
        restaurants_repartitioned = restaurants_df.repartition(5, "restaurant_id")
        
        # Perform join with aligned partitions
        result = orders_repartitioned.join(
            restaurants_repartitioned,
            orders_repartitioned.restaurant_id_extracted == restaurants_repartitioned.restaurant_id,
            "inner"
        )
        
        count = result.count()
        duration = time.time() - start
        print(f"Partitioned approach: {count} rows in {duration:.2f} seconds")
        
        # Check the execution plan
        result.explain()
        
        return result
    
    # Approach 4: Bucketed join (conceptual - need persistent tables)
    def bucketed_approach_concept():
        print("Bucketed approach (conceptual):")
        print("""
        # Create bucketed tables
        spark.sql('''
            CREATE TABLE bucketed_restaurants
            USING parquet
            CLUSTERED BY (restaurant_id) INTO 4 BUCKETS
            AS SELECT * FROM restaurants
        ''')
        
        spark.sql('''
            CREATE TABLE bucketed_orders
            USING parquet
            CLUSTERED BY (restaurant_id_extracted) INTO 4 BUCKETS
            AS SELECT *, substr(restaurant_key, 1, 5) as restaurant_id_extracted FROM orders
        ''')
        
        # Join bucketed tables
        result = spark.sql('''
            SELECT *
            FROM bucketed_orders o
            JOIN bucketed_restaurants r
              ON o.restaurant_id_extracted = r.restaurant_id
        ''')
        """)
    
    # Run the approaches
    print("Comparing join strategies for large datasets:")
    result1 = naive_approach()
    result2 = broadcast_approach()
    result3 = partitioned_approach()
    bucketed_approach_concept()
    
    spark.stop()
```

### Case Study 2: Handling Skewed Data

```python
# app/case_study_skewed_data.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, broadcast
import time
import random

def skewed_data_case_study():
    """Case study: Effectively handling skewed data"""
    spark = SparkSession.builder.appName("SkewedDataCaseStudy").getOrCreate()
    
    # Load restaurant data
    restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
    
    # Create a test dataset with skewed distribution
    def create_skewed_dataset():
        # Check cuisine distribution
        cuisine_counts = restaurants_df.groupBy("cuisine_type").count().orderBy(col("count").desc())
        
        print("Original cuisine distribution:")
        cuisine_counts.show(5)
        
        # Create synthetic orders with heavy skew toward certain cuisines
        cuisine_list = [row.cuisine_type for row in cuisine_counts.collect()]
        
        # Use only first 3 cuisines for skew
        skewed_cuisines = cuisine_list[:3]
        other_cuisines = cuisine_list[3:]
        
        # Create data with 90% of orders from top 3 cuisines
        data = []
        for i in range(1000):
            if i < 900:  # 90% skewed
                cuisine = random.choice(skewed_cuisines)
            else:  # 10% even
                cuisine = random.choice(other_cuisines) if other_cuisines else random.choice(skewed_cuisines)
            
            data.append((i, cuisine, random.randint(10, 100)))
        
        # Create DataFrame
        skewed_df = spark.createDataFrame(data, ["order_id", "cuisine_type", "amount"])
        
        # Show the skew
        print("Synthetic data cuisine distribution:")
        skewed_df.groupBy("cuisine_type").count().orderBy(col("count").desc()).show(5)
        
        return skewed_df
    
    # Create the skewed dataset
    skewed_orders = create_skewed_dataset()
    
    # Challenge: Join skewed orders with restaurants without performance issues
    
    # Approach 1: Standard join (will suffer from skew)
    def standard_approach():
        start = time.time()
        
        result = skewed_orders.join(
            restaurants_df,
            on="cuisine_type",
            how="inner"
        )
        
        count = result.count()
        duration = time.time() - start
        print(f"Standard join: {count} rows in {duration:.2f} seconds")
        
        # Check the execution plan
        result.explain()
        
        return result
    
    # Approach 2: Salting (adding randomness to distribute the load)
    def salting_approach():
        start = time.time()
        
        # 1. Identify skewed keys
        top_cuisines = skewed_orders.groupBy("cuisine_type") \
            .count() \
            .orderBy(col("count").desc()) \
            .limit(3) \
            .select("cuisine_type") \
            .collect()
        
        skewed_keys = [row.cuisine_type for row in top_cuisines]
        print(f"Identified skewed keys: {skewed_keys}")
        
        # 2. Add salt to skewed keys
        from pyspark.sql.functions import rand, lit
        
        SALT_COUNT = 10
        
        # Add salt column to orders
        salted_orders = skewed_orders.withColumn(
            "salt", 
            (rand() * SALT_COUNT).cast("int")
        ).where(col("cuisine_type").isin(skewed_keys))
        
        # Orders with non-skewed keys
        regular_orders = skewed_orders.where(~col("cuisine_type").isin(skewed_keys))
        
        # 3. Explode the restaurants for skewed keys
        from pyspark.sql.functions import explode, array
        
        salted_restaurants = restaurants_df \
            .where(col("cuisine_type").isin(skewed_keys)) \
            .withColumn("salt", explode(array([lit(i) for i in range(SALT_COUNT)])))
        
        # Regular restaurants (no salt)
        regular_restaurants = restaurants_df.where(~col("cuisine_type").isin(skewed_keys))
        
        # 4. Join with salt
        salted_join = salted_orders.join(
            salted_restaurants,
            (salted_orders.cuisine_type == salted_restaurants.cuisine_type) & 
            (salted_orders.salt == salted_restaurants.salt),
            "inner"
        )
        
        # 5. Join regular data
        regular_join = regular_orders.join(
            regular_restaurants,
            on="cuisine_type",
            how="inner"
        )
        
        # 6. Union the results
        result = salted_join.union(regular_join)
        
        count = result.count()
        duration = time.time() - start
        print(f"Salted join: {count} rows in {duration:.2f} seconds")
        
        return result
    
    # Run the approaches
    print("Comparing strategies for skewed data:")
    result1 = standard_approach()
    result2 = salting_approach()
    
    spark.stop()
```

## 6. Common Pitfalls and How to Avoid Them

```python
# app/common_pitfalls.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf, collect_list, explode
from pyspark.sql.types import StringType, ArrayType
import time

def demonstrate_pitfalls():
    """Demonstrate common PySpark pitfalls and their solutions"""
    spark = SparkSession.builder.appName("PySpark Pitfalls").getOrCreate()
    
    # Load sample data
    restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
    
    # Pitfall 1: collect() on Large Dataset
    print("Pitfall 1: collect() on large dataset")
    
    def bad_approach_collect():
        # WARNING: This could cause OOM errors with large datasets
        print("Bad approach:")
        # Collecting all data to driver - bad for large datasets
        all_restaurants = restaurants_df.collect()
        print(f"Collected {len(all_restaurants)} restaurants to driver")
    
    def better_approach_collect():
        print("Better approach:")
        # Use take/limit for small samples
        sample = restaurants_df.limit(5).collect()
        print(f"Collected sample of {len(sample)} restaurants")
        
        # For aggregations, use specific actions
        count = restaurants_df.count()
        avg_rating = restaurants_df.agg({"average_rating": "mean"}).collect()[0][0]
        print(f"Count: {count}, Avg Rating: {avg_rating}")
    
    bad_approach_collect()
    better_approach_collect()
    
    # Pitfall 2: Inefficient UDFs
    print("\nPitfall 2: Inefficient UDFs")
    
    def bad_approach_udf():
        print("Bad approach:")
        start = time.time()
        
        # Row-by-row UDF processing is slow
        @udf(StringType())
        def categorize_restaurant(name, rating):
            if rating >= 4.5:
                return f"{name} (Excellent)"
            elif rating >= 4.0:
                return f"{name} (Very Good)"
            elif rating >= 3.0:
                return f"{name} (Good)"
            else:
                return f"{name} (Average)"
        
        # Apply UDF
        result = restaurants_df.withColumn(
            "categorized", 
            categorize_restaurant(col("name"), col("average_rating"))
        )
        
        count = result.count()
        duration = time.time() - start
        print(f"UDF processing: {count} rows in {duration:.2f} seconds")
    
    def better_approach_udf():
        print("Better approach:")
        start = time.time()
        
        # Use built-in functions instead of UDFs when possible
        from pyspark.sql.functions import when
        
        # Use built-in SQL functions
        result = restaurants_df.withColumn(
            "categorized",
            when(col("average_rating") >= 4.5, concat(col("name"), lit(" (Excellent)")))
            .when(col("average_rating") >= 4.0, concat(col("name"), lit(" (Very Good)")))
            .when(col("average_rating") >= 3.0, concat(col("name"), lit(" (Good)")))
            .otherwise(concat(col("name"), lit(" (Average)")))
        )
        
        count = result.count()
        duration = time.time() - start
        print(f"Built-in function processing: {count} rows in {duration:.2f} seconds")
    
    from pyspark.sql.functions import concat, lit
    
    bad_approach_udf()
    better_approach_udf()
    
    # Pitfall 3: Inefficient Aggregations
    print("\nPitfall 3: Inefficient Aggregations")
    
    def bad_approach_aggregation():
        print("Bad approach:")
        
        # Collect all restaurants by cuisine, then process in driver
        # This can cause OOM with large datasets
        cuisine_groups = restaurants_df.groupBy("cuisine_type").agg(
            collect_list("name").alias("restaurant_names")
        )
        
        # Process in driver (bad)
        for row in cuisine_groups.collect():
            cuisine = row.cuisine_type
            names = row.restaurant_names
            print(f"{cuisine}: {len(names)} restaurants")
    
    def better_approach_aggregation():
        print("Better approach:")
        
        # Do the aggregation in Spark, not in driver
        cuisine_counts = restaurants_df.groupBy("cuisine_type").count()
        
        # Get only the results
        for row in cuisine_counts.orderBy(col("count").desc()).limit(5).collect():
            print(f"{row.cuisine_type}: {row['count']} restaurants")
    
    bad_approach_aggregation()
    better_approach_aggregation()
    
    # Pitfall 4: Data Skew in GroupBy
    print("\nPitfall 4: Data Skew in GroupBy")
    
    def bad_approach_groupby():
        print("Bad approach:")
        
        # Standard groupBy can lead to skew
        result = restaurants_df.groupBy("cuisine_type").count()
        result.explain()
    
    def better_approach_groupby():
        print("Better approach:")
        
        # Two-phase aggregation for skewed data
        from pyspark.sql.functions import spark_partition_id
        
        # 1. Add a salting column for pre-aggregation
        salted = restaurants_df.withColumn("partition_id", spark_partition_id() % 10)
        
        # 2. Pre-aggregate with the salting column
        pre_aggregated = salted.groupBy("cuisine_type", "partition_id").count()
        
        # 3. Final aggregation
        result = pre_aggregated.groupBy("cuisine_type").sum("count").withColumnRenamed("sum(count)", "count")
        
        result.explain()
    
    bad_approach_groupby()
    better_approach_groupby()
    
    # Pitfall 5: Unnecessary Shuffling
    print("\nPitfall 5: Unnecessary Shuffling")
    
    def bad_approach_shuffle():
        print("Bad approach:")
        
        # Multiple groupBy operations cause multiple shuffles
        cuisine_counts = restaurants_df.groupBy("cuisine_type").count()
        city_cuisine_counts = cuisine_counts.join(
            restaurants_df.groupBy("city", "cuisine_type").count(),
            on="cuisine_type"
        )
        
        city_cuisine_counts.explain()
    
    def better_approach_shuffle():
        print("Better approach:")
        
        # Single groupBy with multiple aggregations
        from pyspark.sql.functions import countDistinct
        
        result = restaurants_df.groupBy("cuisine_type").agg(
            count("*").alias("cuisine_count"),
            countDistinct("city").alias("city_count")
        )
        
        result.explain()
    
    bad_approach_shuffle()
    better_approach_shuffle()
    
    spark.stop()
```

## 7. Performance Testing and Benchmarking

```python
# app/benchmarking.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, broadcast, explode, array, lit
import time
import gc

def benchmark_operations():
    """Benchmark different PySpark operations and optimizations"""
    spark = SparkSession.builder \
        .appName("PySpark Benchmarking") \
        .config("spark.sql.shuffle.partitions", "20") \
        .getOrCreate()
    
    # Load sample data
    print("Loading test data...")
    restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
    
    # Create larger test dataset (to better demonstrate performance differences)
    print("Creating expanded test dataset...")
    expanded_df = restaurants_df
    for i in range(3):  # Multiply data by 8
        expanded_df = expanded_df.union(expanded_df)
    
    # Cache for consistent benchmarking
    expanded_df.cache()
    print(f"Expanded to {expanded_df.count()} rows")
    
    # Benchmark class
    class SparkBenchmark:
        def __init__(self, name):
            self.name = name
            self.results = {}
        
        def benchmark(self, name, func):
            print(f"Running {name}...")
            # Clear cache and run GC before test
            gc.collect()
            # Run test
            start = time.time()
            result = func()
            duration = time.time() - start
            # Store result
            self.results[name] = {
                "duration": duration,
                "result": result
            }
            print(f"  Completed in {duration:.2f} seconds")
            return result
        
        def compare(self):
            print(f"\n=== {self.name} Results ===")
            if not self.results:
                print("No benchmarks run")
                return
            
            # Find baseline
            baseline = next(iter(self.results.values()))["duration"]
            
            # Print results
            for name, data in self.results.items():
                duration = data["duration"]
                relative = duration / baseline
                print(f"{name}: {duration:.2f}s ({relative:.2f}x)")
    
    # 1. Benchmark filtering operations
    filter_benchmark = SparkBenchmark("Filtering Operations")
    
    # Simple filter
    filter_benchmark.benchmark("Simple Filter", lambda: 
        expanded_df.filter(col("average_rating") > 4.0).count()
    )
    
    # Complex filter
    filter_benchmark.benchmark("Complex Filter", lambda:
        expanded_df.filter(
            (col("average_rating") > 4.0) & 
            (col("cuisine_type").isin(["Italian", "Chinese", "Indian"])) &
            (col("num_reviews") > 100)
        ).count()
    )
    
    # Pushed down filter with explain
    def pushed_filter():
        result = expanded_df.filter(col("average_rating") > 4.0)
        result.explain()
        return result.count()
    
    filter_benchmark.benchmark("Predicate Pushdown", pushed_filter)
    
    filter_benchmark.compare()
    
    # 2. Benchmark join operations
    # Create a small dimension table
    cities_df = expanded_df.select("city").distinct()
    
    join_benchmark = SparkBenchmark("Join Operations")
    
    # Regular join
    join_benchmark.benchmark("Regular Join", lambda:
        expanded_df.join(cities_df, on="city").count()
    )
    
    # Broadcast join
    join_benchmark.benchmark("Broadcast Join", lambda:
        expanded_df.join(broadcast(cities_df), on="city").count()
    )
    
    join_benchmark.compare()
    
    # 3. Benchmark groupBy operations
    groupby_benchmark = SparkBenchmark("GroupBy Operations")
    
    # Regular groupBy
    groupby_benchmark.benchmark("Regular GroupBy", lambda:
        expanded_df.groupBy("cuisine_type").count().collect()
    )
    
    # Optimized with custom partitioning
    def optimized_groupby():
        # Set appropriate number of partitions
        spark.conf.set("spark.sql.shuffle.partitions", "20")
        return expanded_df.groupBy("cuisine_type").count().collect()
    
    groupby_benchmark.benchmark("Custom Partitioning", optimized_groupby)
    
    groupby_benchmark.compare()
    
    # 4. Benchmark UDF vs. built-in functions
    udf_benchmark = SparkBenchmark("UDF vs. Built-in Functions")
    
    # UDF approach
    from pyspark.sql.functions import udf
    from pyspark.sql.types import StringType
    
    @udf(StringType())
    def rating_category_udf(rating):
        if rating is None:
            return "Unknown"
        
        rating_val = float(rating)
        if rating_val >= 4.5:
            return "Excellent"
        elif rating_val >= 4.0:
            return "Very Good"
        elif rating_val >= 3.0:
            return "Good"
        else:
            return "Average"
    
    udf_benchmark.benchmark("Python UDF", lambda:
        expanded_df.withColumn(
            "rating_category", 
            rating_category_udf(col("average_rating"))
        ).count()
    )
    
    # Built-in function approach
    from pyspark.sql.functions import when
    
    udf_benchmark.benchmark("Built-in Functions", lambda:
        expanded_df.withColumn(
            "rating_category",
            when(col("average_rating").isNull(), "Unknown")
            .when(col("average_rating") >= 4.5, "Excellent")
            .when(col("average_rating") >= 4.0, "Very Good")
            .when(col("average_rating") >= 3.0, "Good")
            .otherwise("Average")
        ).count()
    )
    
    udf_benchmark.compare()
    
    # Clean up
    expanded_df.unpersist()
    spark.stop()
```

## 8. Key Takeaways

1. **Understand Spark's Execution Model**: Learn how transformations and actions work.

2. **The 5 S's of Optimization**:
   - **Shuffle**: Minimize data movement between partitions
   - **Skew**: Handle imbalanced data distribution
   - **Spill**: Manage memory constraints and disk spillage
   - **Storage**: Use appropriate data formats and compression
   - **Serialization**: Optimize data serialization/deserialization

3. **Performance Debugging**:
   - Use `explain()` to understand execution plans
   - Monitor Spark UI for bottlenecks
   - Track task durations and data skew
   - Identify memory issues with spill metrics

4. **Serialization Best Practices**:
   - Use Kryo serialization for better performance
   - Broadcast small lookup tables
   - Be careful with complex objects in UDFs
   - Create resources inside UDFs rather than capturing them

5. **Advanced Join Strategies**:
   - Use broadcast joins for small tables
   - Align partitions for large-large joins
   - Apply salting for skewed keys
   - Consider bucketing for persistent tables

6. **Avoid Common Pitfalls**:
   - Don't `collect()` large datasets to the driver
   - Minimize UDFs, use built-in functions where possible
   - Watch for unnecessary shuffling 
   - Be careful with wide transformations
   - Test with realistic data volumes

These advanced techniques will help you build performant, robust PySpark applications that efficiently process large datasets in distributed environments.
