# Spark SQL Foundations - Part 2: Basic Transformation Operations

## Introduction

After ingesting data into Spark SQL, the next step is transforming it to extract insights. In this lesson, we'll explore basic SQL operations for data transformation in Spark, including SELECT statements, filtering with WHERE, grouping, and ordering.

## Setting Up Our Environment

Let's start by setting up our Spark session and loading the UberEats data we prepared in the previous lesson:

```python
from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder \
    .appName("Spark SQL Basic Transformations") \
    .master("local[*]") \
    .getOrCreate()

# Read our UberEats datasets and create temporary views
restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
drivers_df = spark.read.json("./storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl")
orders_df = spark.read.json("./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl")
ratings_df = spark.read.json("./storage/mysql/ratings/01JS4W5A7YWTYRQKDA7F7N95VZ.jsonl")

# Create temporary views
restaurants_df.createOrReplaceTempView("restaurants")
drivers_df.createOrReplaceTempView("drivers")
orders_df.createOrReplaceTempView("orders")
ratings_df.createOrReplaceTempView("ratings")

# Confirm the views are created
print("Available tables:")
for table in spark.catalog.listTables():
    print(f"- {table.name}")
```

## 1. Basic SELECT Queries

The SELECT statement is the foundation of SQL queries, allowing you to retrieve and transform data.

### Simple SELECT

```python
# Simple selection of all columns
all_restaurants = spark.sql("""
    SELECT * 
    FROM restaurants 
    LIMIT 5
""")

print("All Columns from Restaurants:")
all_restaurants.show()

# Select specific columns
restaurant_names = spark.sql("""
    SELECT name, cuisine_type, city, average_rating
    FROM restaurants
    LIMIT 5
""")

print("\nSelected Columns from Restaurants:")
restaurant_names.show()
```

### Using Column Aliases

```python
# Using aliases for better readability
aliased_columns = spark.sql("""
    SELECT 
        name AS restaurant_name,
        cuisine_type AS cuisine,
        average_rating AS rating,
        num_reviews AS review_count
    FROM restaurants
    LIMIT 5
""")

print("\nColumns with Aliases:")
aliased_columns.show()
```

### Computed Columns

```python
# Creating computed columns
computed_columns = spark.sql("""
    SELECT 
        name,
        average_rating,
        num_reviews,
        average_rating * SQRT(num_reviews / 1000) AS popularity_score,
        CASE
            WHEN average_rating >= 4.5 THEN 'Excellent'
            WHEN average_rating >= 4.0 THEN 'Very Good'
            WHEN average_rating >= 3.5 THEN 'Good'
            WHEN average_rating >= 3.0 THEN 'Average'
            ELSE 'Below Average'
        END AS rating_category
    FROM restaurants
    LIMIT 5
""")

print("\nComputed Columns:")
computed_columns.show()
```

## 2. Filtering Data with WHERE

The WHERE clause allows you to filter rows based on specified conditions.

### Basic Filtering

```python
# Filter restaurants with high ratings
high_rated = spark.sql("""
    SELECT name, cuisine_type, city, average_rating
    FROM restaurants
    WHERE average_rating > 4.0
    LIMIT 10
""")

print("\nHigh-Rated Restaurants:")
high_rated.show()
```

### Multiple Conditions

```python
# Multiple filter conditions with AND
popular_italian = spark.sql("""
    SELECT name, city, average_rating, num_reviews
    FROM restaurants
    WHERE cuisine_type = 'Italian' AND num_reviews > 3000
    LIMIT 10
""")

print("\nPopular Italian Restaurants:")
popular_italian.show()

# Using OR condition
specific_cuisines = spark.sql("""
    SELECT name, cuisine_type, city, average_rating
    FROM restaurants
    WHERE cuisine_type = 'French' OR cuisine_type = 'Japanese'
    LIMIT 10
""")

print("\nFrench or Japanese Restaurants:")
specific_cuisines.show()
```

### IN and BETWEEN Operators

```python
# Using IN operator
selected_cuisines = spark.sql("""
    SELECT name, cuisine_type, average_rating
    FROM restaurants
    WHERE cuisine_type IN ('Italian', 'French', 'Japanese', 'Chinese')
    LIMIT 10
""")

print("\nRestaurants with Selected Cuisines:")
selected_cuisines.show()

# Using BETWEEN operator
mid_range_ratings = spark.sql("""
    SELECT name, cuisine_type, average_rating
    FROM restaurants
    WHERE average_rating BETWEEN 3.5 AND 4.5
    LIMIT 10
""")

print("\nRestaurants with Mid-Range Ratings:")
mid_range_ratings.show()
```

### NULL Handling

```python
# Check for NULL values
missing_ratings = spark.sql("""
    SELECT name, cuisine_type, average_rating
    FROM restaurants
    WHERE average_rating IS NULL
    LIMIT 10
""")

print("\nRestaurants with Missing Ratings:")
missing_ratings.show()

# Handling NULL values with COALESCE
with_default_values = spark.sql("""
    SELECT 
        name,
        cuisine_type,
        COALESCE(average_rating, 0) AS rating,
        COALESCE(num_reviews, 0) AS reviews
    FROM restaurants
    LIMIT 10
""")

print("\nRestaurants with Default Values for NULLs:")
with_default_values.show()
```

## 3. Sorting with ORDER BY

The ORDER BY clause allows you to sort the results of a query.

### Basic Sorting

```python
# Sort by a single column
sorted_by_rating = spark.sql("""
    SELECT name, cuisine_type, average_rating
    FROM restaurants
    ORDER BY average_rating DESC
    LIMIT 10
""")

print("\nRestaurants Sorted by Rating (Descending):")
sorted_by_rating.show()
```

### Multiple Sort Columns

```python
# Sort by multiple columns
multi_column_sort = spark.sql("""
    SELECT cuisine_type, city, name, average_rating
    FROM restaurants
    ORDER BY cuisine_type ASC, average_rating DESC
    LIMIT 10
""")

print("\nRestaurants Sorted by Cuisine Type and Rating:")
multi_column_sort.show()
```

### Using Column Position

```python
# Sort by column position
position_sort = spark.sql("""
    SELECT name, cuisine_type, average_rating, num_reviews
    FROM restaurants
    ORDER BY 3 DESC, 4 DESC
    LIMIT 10
""")

print("\nRestaurants Sorted by Column Position:")
position_sort.show()
```

## 4. Grouping Data with GROUP BY

The GROUP BY clause allows you to aggregate data and perform calculations across groups.

### Basic Grouping

```python
# Count restaurants by cuisine type
cuisine_count = spark.sql("""
    SELECT cuisine_type, COUNT(*) AS restaurant_count
    FROM restaurants
    GROUP BY cuisine_type
    ORDER BY restaurant_count DESC
    LIMIT 10
""")

print("\nRestaurant Count by Cuisine Type:")
cuisine_count.show()
```

### Multiple Aggregations

```python
# Multiple aggregations
cuisine_stats = spark.sql("""
    SELECT 
        cuisine_type,
        COUNT(*) AS restaurant_count,
        ROUND(AVG(average_rating), 2) AS avg_rating,
        SUM(num_reviews) AS total_reviews,
        MAX(average_rating) AS max_rating,
        MIN(average_rating) AS min_rating
    FROM restaurants
    GROUP BY cuisine_type
    ORDER BY restaurant_count DESC
    LIMIT 10
""")

print("\nCuisine Type Statistics:")
cuisine_stats.show()
```

### Multi-Column Grouping

```python
# Group by multiple columns
location_cuisine = spark.sql("""
    SELECT 
        country,
        city,
        cuisine_type,
        COUNT(*) AS restaurant_count,
        ROUND(AVG(average_rating), 2) AS avg_rating
    FROM restaurants
    GROUP BY country, city, cuisine_type
    ORDER BY restaurant_count DESC
    LIMIT 10
""")

print("\nRestaurant Count by Location and Cuisine:")
location_cuisine.show()
```

### Filtering Groups with HAVING

```python
# Filter groups with HAVING
popular_cuisines = spark.sql("""
    SELECT 
        cuisine_type,
        COUNT(*) AS restaurant_count,
        ROUND(AVG(average_rating), 2) AS avg_rating
    FROM restaurants
    GROUP BY cuisine_type
    HAVING COUNT(*) > 2
    ORDER BY restaurant_count DESC
    LIMIT 10
""")

print("\nPopular Cuisine Types:")
popular_cuisines.show()

# Combining WHERE and HAVING
high_rated_cuisines = spark.sql("""
    SELECT 
        cuisine_type,
        COUNT(*) AS restaurant_count,
        ROUND(AVG(average_rating), 2) AS avg_rating
    FROM restaurants
    WHERE num_reviews > 1000
    GROUP BY cuisine_type
    HAVING AVG(average_rating) > 4.0
    ORDER BY avg_rating DESC
    LIMIT 10
""")

print("\nHigh-Rated Popular Cuisine Types:")
high_rated_cuisines.show()
```

## 5. Practical Application: Restaurant Analysis Dashboard

Let's combine what we've learned to create a comprehensive restaurant analysis dashboard using Spark SQL:

```python
def restaurant_analysis_dashboard(spark):
    """
    Create a comprehensive restaurant analysis using Spark SQL.
    Returns a dictionary of DataFrames with various analyses.
    """
    results = {}
    
    # 1. Top-rated restaurants overall
    top_restaurants = spark.sql("""
        SELECT 
            name,
            cuisine_type,
            city,
            average_rating,
            num_reviews,
            average_rating * SQRT(num_reviews / 1000) AS popularity_score
        FROM restaurants
        ORDER BY popularity_score DESC
        LIMIT 20
    """)
    results["top_restaurants"] = top_restaurants
    
    # 2. Cuisine type summary
    cuisine_summary = spark.sql("""
        SELECT 
            cuisine_type,
            COUNT(*) AS restaurant_count,
            ROUND(AVG(average_rating), 2) AS avg_rating,
            SUM(num_reviews) AS total_reviews,
            ROUND(AVG(average_rating * SQRT(num_reviews / 1000)), 2) AS avg_popularity
        FROM restaurants
        GROUP BY cuisine_type
        HAVING COUNT(*) > 1
        ORDER BY avg_popularity DESC
    """)
    results["cuisine_summary"] = cuisine_summary
    
    # 3. City performance
    city_performance = spark.sql("""
        SELECT 
            city,
            COUNT(*) AS restaurant_count,
            ROUND(AVG(average_rating), 2) AS avg_rating,
            ROUND(AVG(num_reviews), 0) AS avg_reviews,
            SUM(num_reviews) AS total_reviews
        FROM restaurants
        GROUP BY city
        HAVING COUNT(*) > 1
        ORDER BY total_reviews DESC
    """)
    results["city_performance"] = city_performance
    
    # 4. Rating distribution
    rating_distribution = spark.sql("""
        SELECT 
            FLOOR(average_rating * 2) / 2 AS rating_bucket,
            COUNT(*) AS restaurant_count
        FROM restaurants
        GROUP BY FLOOR(average_rating * 2) / 2
        ORDER BY rating_bucket
    """)
    results["rating_distribution"] = rating_distribution
    
    # 5. Order analysis by cuisine (assuming orders are joined with restaurants)
    order_analysis = spark.sql("""
        SELECT 
            r.cuisine_type,
            COUNT(o.order_id) AS order_count,
            ROUND(AVG(o.total_amount), 2) AS avg_order_value,
            ROUND(SUM(o.total_amount), 2) AS total_revenue
        FROM restaurants r
        JOIN orders o ON r.cnpj = o.restaurant_key
        GROUP BY r.cuisine_type
        ORDER BY total_revenue DESC
    """)
    results["order_analysis"] = order_analysis
    
    return results

# Execute the analysis
dashboard = restaurant_analysis_dashboard(spark)

# Display the results
print("\n=== Restaurant Analysis Dashboard ===\n")

print("Top Restaurants by Popularity:")
dashboard["top_restaurants"].show(5)

print("\nCuisine Type Summary:")
dashboard["cuisine_summary"].show(5)

print("\nCity Performance:")
dashboard["city_performance"].show(5)

print("\nRating Distribution:")
dashboard["rating_distribution"].show(5)

print("\nOrder Analysis by Cuisine:")
dashboard["order_analysis"].show(5)
```

## Conclusion and Best Practices

In this lesson, we've covered the fundamental transformation operations in Spark SQL:

1. **Basic SELECT Queries**: Retrieving and transforming data with column selection and aliases
2. **Filtering with WHERE**: Using conditions to filter rows
3. **Sorting with ORDER BY**: Arranging results in a specific order
4. **Grouping with GROUP BY**: Aggregating data and performing calculations across groups
5. **Practical Application**: Combining operations to build a comprehensive dashboard

**Best Practices:**

- Keep your SQL queries organized and well-formatted for readability
- Use appropriate column aliases to make your results more understandable
- Filter data early in your queries to reduce processing on unnecessary rows
- Choose the right aggregation functions for your analysis needs
- Use HAVING for filtering aggregated results and WHERE for filtering individual rows
- Combine multiple operations logically to build complex analyses
- Consider performance implications of very wide GROUP BY clauses on large datasets

In the next lesson, we'll explore more advanced transformation operations with Spark SQL.

## Exercise

1. Create a comprehensive driver performance analysis that:
   - Ranks drivers by the number and value of orders they've delivered
   - Analyzes performance by vehicle type and city
   - Identifies peak times and days for deliveries
   - Calculates efficiency metrics based on driver attributes

2. Build a customer behavior analysis that:
   - Identifies ordering patterns across different locations
   - Analyzes spending habits by cuisine type
   - Calculates average order frequency
   - Creates customer segments based on spending and frequency

## Resources

- [Spark SQL Documentation](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [Spark SQL Functions](https://spark.apache.org/docs/latest/api/sql/index.html)
