# Integrating Spark with PostgreSQL

## Introduction

In this practical session, we'll explore how to integrate Apache Spark with PostgreSQL databases. Connecting Spark to relational databases is a common requirement in data engineering, allowing us to build efficient pipelines that bridge traditional data systems with big data ecosystems. We'll learn how to configure JDBC connections, optimize data reading and writing, leverage predicate pushdown, and implement parallelism through partitioning.

## Setting Up Our Environment

Let's start by setting up our Spark session with the necessary configurations for PostgreSQL connectivity:

```python
# File: /src/app/spark_postgres_integration.py

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, upper, concat, lit
import os

# Initialize Spark session with PostgreSQL JDBC driver
spark = (SparkSession.builder
         .appName("Spark PostgreSQL Integration")
         .master("local[*]")
         # Add PostgreSQL JDBC driver to classpath
         .config("spark.jars", "./lib/postgresql-42.5.0.jar")
         .getOrCreate())

# Set log level to minimize output noise
spark.sparkContext.setLogLevel("WARN")

print("Spark session initialized successfully!")
```

> **Note**: For this to work, you'll need to download the PostgreSQL JDBC driver and place it in a `lib` directory. You can download it from the [PostgreSQL JDBC website](https://jdbc.postgresql.org/download/).

## Step 1: Establishing JDBC Connection

First, let's set up the connection parameters for our PostgreSQL database:

```python
# PostgreSQL connection parameters
pg_host = "localhost"
pg_port = "5432"
pg_database = "ubereats"
pg_user = "postgres"
pg_password = "postgres"

# JDBC URL
jdbc_url = f"jdbc:postgresql://{pg_host}:{pg_port}/{pg_database}"

# Connection properties
connection_properties = {
    "user": pg_user,
    "password": pg_password,
    "driver": "org.postgresql.Driver"
}

# Test connection
try:
    # Try to read a small amount of data to verify connection
    test_df = spark.read.jdbc(
        url=jdbc_url,
        table="(SELECT 1 as test) AS test_query",
        properties=connection_properties
    )
    test_df.show()
    print("PostgreSQL connection successful!")
except Exception as e:
    print(f"Error connecting to PostgreSQL: {str(e)}")
```

## Step 2: Reading Data from PostgreSQL

### Basic Data Reading

Let's start with a simple read operation:

```python
# Read entire 'restaurants' table
restaurants_df = spark.read.jdbc(
    url=jdbc_url,
    table="restaurants",
    properties=connection_properties
)

print(f"Read {restaurants_df.count()} rows from restaurants table")
restaurants_df.printSchema()
restaurants_df.show(5)
```

### Optimized Reading with Column Selection

Reading only the columns we need can significantly improve performance:

```python
# Read only selected columns
restaurant_names_df = spark.read.jdbc(
    url=jdbc_url,
    table="restaurants",
    properties=connection_properties,
    column="name, cuisine_type, average_rating"
)

print("Read restaurant names and ratings:")
restaurant_names_df.show(5)
```

### Predicate Pushdown

Spark can push down predicates (WHERE clauses) to the database, which executes filtering at the database level before transferring data to Spark:

```python
# Read with predicate pushdown
top_restaurants_df = spark.read.jdbc(
    url=jdbc_url,
    table="restaurants",
    properties=connection_properties,
    predicates=["average_rating > 4.0"]
)

print(f"Found {top_restaurants_df.count()} top-rated restaurants")
top_restaurants_df.show(5)
```

### Using SQL Queries Directly

For more complex queries, you can pass a SQL query directly:

```python
# Read using a custom SQL query
query = """
    (SELECT 
        r.name, 
        r.cuisine_type, 
        r.average_rating, 
        COUNT(o.order_id) as order_count
    FROM 
        restaurants r
    LEFT JOIN 
        orders o ON r.restaurant_id = o.restaurant_id
    GROUP BY 
        r.name, r.cuisine_type, r.average_rating
    ORDER BY 
        order_count DESC
    LIMIT 10) AS top_restaurants
"""

top_ordered_restaurants_df = spark.read.jdbc(
    url=jdbc_url,
    table=query,
    properties=connection_properties
)

print("Top restaurants by order count:")
top_ordered_restaurants_df.show()
```

## Step 3: Parallel Reading for Performance

For large tables, reading data in parallel can significantly improve performance. We can achieve this by partitioning the data based on a numeric column:

```python
# Parallel reading with partitioning
def read_table_in_parallel(table_name, partition_column, lower_bound, upper_bound, num_partitions):
    """
    Read a table in parallel using partitioning
    
    Args:
        table_name: PostgreSQL table name
        partition_column: Column to use for partitioning (must be numeric)
        lower_bound: Minimum value of partition column
        upper_bound: Maximum value of partition column
        num_partitions: Number of partitions to create
    
    Returns:
        DataFrame with the table data
    """
    return spark.read.jdbc(
        url=jdbc_url,
        table=table_name,
        properties=connection_properties,
        column=partition_column,
        lowerBound=lower_bound,
        upperBound=upper_bound,
        numPartitions=num_partitions
    )

# Get partition bounds for restaurant_id
bounds_df = spark.read.jdbc(
    url=jdbc_url,
    table="(SELECT MIN(restaurant_id) as min_id, MAX(restaurant_id) as max_id FROM restaurants) AS bounds",
    properties=connection_properties
)

min_id = bounds_df.first()["min_id"]
max_id = bounds_df.first()["max_id"]

# Read restaurants table in parallel
restaurants_parallel_df = read_table_in_parallel(
    table_name="restaurants",
    partition_column="restaurant_id",
    lower_bound=min_id,
    upper_bound=max_id + 1,  # +1 to include the upper bound
    num_partitions=4
)

print(f"Read {restaurants_parallel_df.count()} rows in parallel")
print(f"Number of partitions: {restaurants_parallel_df.rdd.getNumPartitions()}")
```

### Using Custom Query with Partitioning

For custom queries, we can create a subquery that includes the partition column:

```python
# Custom query with partitioning
def read_query_in_parallel(query, partition_column, lower_bound, upper_bound, num_partitions):
    """
    Read a custom query in parallel
    
    Args:
        query: SQL query without the final parentheses and alias
        partition_column: Column to use for partitioning (must be numeric)
        lower_bound: Minimum value of partition column
        upper_bound: Maximum value of partition column
        num_partitions: Number of partitions to create
    
    Returns:
        DataFrame with the query results
    """
    full_query = f"({query}) AS query_result"
    return spark.read.jdbc(
        url=jdbc_url,
        table=full_query,
        properties=connection_properties,
        column=partition_column,
        lowerBound=lower_bound,
        upperBound=upper_bound,
        numPartitions=num_partitions
    )

# Custom query for restaurants with order count
query = """
    SELECT 
        r.restaurant_id,
        r.name, 
        r.cuisine_type, 
        r.average_rating, 
        COUNT(o.order_id) as order_count
    FROM 
        restaurants r
    LEFT JOIN 
        orders o ON r.restaurant_id = o.restaurant_id
    GROUP BY 
        r.restaurant_id, r.name, r.cuisine_type, r.average_rating
"""

# Read query in parallel
restaurants_with_orders_df = read_query_in_parallel(
    query=query,
    partition_column="restaurant_id",
    lower_bound=min_id,
    upper_bound=max_id + 1,
    num_partitions=4
)

print("Restaurants with order counts (parallel read):")
restaurants_with_orders_df.show(5)
```

## Step 4: Writing Data to PostgreSQL

Now, let's see how to write data from Spark back to PostgreSQL. First, we'll load some data from our UberEats dataset:

```python
# Load sample data from JSON
drivers_df = spark.read.json("./storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl")
print("Loaded sample drivers data:")
drivers_df.show(5)
```

### Basic Write Operation

```python
# Write data to PostgreSQL
def write_to_postgres(df, table_name, mode="overwrite"):
    """
    Write DataFrame to PostgreSQL table
    
    Args:
        df: DataFrame to write
        table_name: Target table name
        mode: Write mode (overwrite, append, ignore, error)
    """
    df.write.jdbc(
        url=jdbc_url,
        table=table_name,
        mode=mode,
        properties=connection_properties
    )

# Write drivers data to PostgreSQL
try:
    write_to_postgres(drivers_df, "drivers_spark")
    print("Successfully wrote drivers data to PostgreSQL")
except Exception as e:
    print(f"Error writing to PostgreSQL: {str(e)}")
```

### Batch Write for Large Datasets

For large datasets, writing in batches can be more efficient:

```python
# Write in batches
def write_in_batches(df, table_name, batch_size=1000):
    """
    Write DataFrame to PostgreSQL in batches
    
    Args:
        df: DataFrame to write
        table_name: Target table name
        batch_size: Number of rows per batch
    """
    # Collect data to driver (only for demonstration - not recommended for large datasets)
    rows = df.collect()
    total_rows = len(rows)
    
    # Process in batches
    for i in range(0, total_rows, batch_size):
        end_idx = min(i + batch_size, total_rows)
        batch_rows = rows[i:end_idx]
        
        # Create DataFrame from batch
        batch_df = spark.createDataFrame(batch_rows, df.schema)
        
        # Write batch
        mode = "overwrite" if i == 0 else "append"
        batch_df.write.jdbc(
            url=jdbc_url,
            table=table_name,
            mode=mode,
            properties=connection_properties
        )
        
        print(f"Wrote batch {i//batch_size + 1} ({end_idx}/{total_rows} rows)")

# For demonstration only - in practice, use this for very large datasets
# write_in_batches(drivers_df, "drivers_batched", batch_size=2)
```

## Step 5: Advanced Techniques

### Handling Schema Evolution

When the source schema changes, we need to handle it gracefully:

```python
# Add new columns and handle schema changes
def write_with_schema_handling(df, table_name):
    """
    Write DataFrame to PostgreSQL with schema evolution handling
    
    Args:
        df: DataFrame to write
        table_name: Target table name
    """
    # Check if table exists
    table_exists_df = spark.read.jdbc(
        url=jdbc_url,
        table="(SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = '" + table_name + "') as exists) AS table_check",
        properties=connection_properties
    )
    
    table_exists = table_exists_df.first()["exists"]
    
    if table_exists:
        # Get current schema
        schema_df = spark.read.jdbc(
            url=jdbc_url,
            table=f"(SELECT column_name FROM information_schema.columns WHERE table_name = '{table_name}') AS schema_query",
            properties=connection_properties
        )
        
        existing_columns = [row["column_name"] for row in schema_df.collect()]
        df_columns = df.columns
        
        # Find new columns
        new_columns = [col for col in df_columns if col.lower() not in existing_columns]
        
        if new_columns:
            print(f"Found new columns: {new_columns}")
            # In a real application, you'd handle schema evolution here
            # This would involve ALTER TABLE statements or creating a new table
    
    # Write data
    df.write.jdbc(
        url=jdbc_url,
        table=table_name,
        mode="overwrite",  # Or use appropriate mode based on schema changes
        properties=connection_properties
    )

# Simulate schema evolution
drivers_evolved_df = drivers_df.withColumn(
    "full_name", 
    concat(col("first_name"), lit(" "), col("last_name"))
)

# write_with_schema_handling(drivers_evolved_df, "drivers_evolved")
```

### Using JDBC Bulk Loading for Better Performance

Some JDBC drivers support bulk loading, which can significantly improve write performance:

```python
# Bulk loading configuration
bulk_properties = connection_properties.copy()
bulk_properties.update({
    "batchsize": "10000",  # Number of rows per batch
    "reWriteBatchedInserts": "true"  # Enable rewriting to optimize for batching
})

def write_with_bulk_loading(df, table_name):
    """
    Write DataFrame to PostgreSQL using bulk loading
    
    Args:
        df: DataFrame to write
        table_name: Target table name
    """
    df.write.jdbc(
        url=jdbc_url,
        table=table_name,
        mode="overwrite",
        properties=bulk_properties
    )

# write_with_bulk_loading(drivers_df, "drivers_bulk")
```

## Step 6: Putting It All Together - A Complete ETL Pipeline

Let's build a complete ETL pipeline that reads from JSON files, transforms the data, and writes to PostgreSQL:

```python
# File: /src/app/spark_postgres_etl.py

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, round, datediff, current_date, avg, count, sum
import os

def create_spark_session():
    """Create a Spark session with PostgreSQL JDBC driver"""
    return (SparkSession.builder
            .appName("Spark PostgreSQL ETL")
            .master("local[*]")
            .config("spark.jars", "./lib/postgresql-42.5.0.jar")
            .getOrCreate())

def get_postgres_connection(host, port, database, user, password):
    """Create PostgreSQL connection parameters"""
    jdbc_url = f"jdbc:postgresql://{host}:{port}/{database}"
    properties = {
        "user": user,
        "password": password,
        "driver": "org.postgresql.Driver"
    }
    return jdbc_url, properties

def read_json_data(spark, base_path):
    """Read data from JSON files"""
    # Load restaurant data
    restaurants_path = os.path.join(base_path, "mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
    restaurants_df = spark.read.json(restaurants_path)
    
    # Load driver data
    drivers_path = os.path.join(base_path, "postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl")
    drivers_df = spark.read.json(drivers_path)
    
    # Load order data
    orders_path = os.path.join(base_path, "kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl")
    orders_df = spark.read.json(orders_path)
    
    return restaurants_df, drivers_df, orders_df

def transform_data(restaurants_df, drivers_df, orders_df):
    """Transform and aggregate data"""
    # Clean restaurant data
    restaurants_clean = (restaurants_df
                       .select("restaurant_id", "name", "cuisine_type", "city", 
                               "average_rating", "num_reviews", "cnpj")
                       .filter(col("average_rating").isNotNull()))
    
    # Clean driver data
    drivers_clean = (drivers_df
                   .select("driver_id", "first_name", "last_name", "city", 
                           "vehicle_type", "license_number")
                   .withColumn("full_name", 
                               col("first_name").cast("string").concat(" ").concat(col("last_name").cast("string"))))
    
    # Clean order data
    orders_clean = (orders_df
                  .select("order_id", "restaurant_key", "driver_key", "total_amount")
                  .filter(col("total_amount").isNotNull()))
    
    # Create restaurant analytics
    restaurant_analytics = (restaurants_clean
                          .join(orders_clean, restaurants_clean["cnpj"] == orders_clean["restaurant_key"], "left")
                          .groupBy("restaurant_id", "name", "cuisine_type", "city", "average_rating", "num_reviews")
                          .agg(
                              count("order_id").alias("order_count"),
                              round(sum("total_amount"), 2).alias("total_revenue"),
                              round(avg("total_amount"), 2).alias("avg_order_value")
                          ))
    
    # Create driver analytics
    driver_analytics = (drivers_clean
                      .join(orders_clean, drivers_clean["license_number"] == orders_clean["driver_key"], "left")
                      .groupBy("driver_id", "full_name", "city", "vehicle_type")
                      .agg(
                          count("order_id").alias("delivery_count"),
                          round(sum("total_amount"), 2).alias("total_delivery_value"),
                          round(avg("total_amount"), 2).alias("avg_delivery_value")
                      ))
    
    return restaurants_clean, drivers_clean, orders_clean, restaurant_analytics, driver_analytics

def write_to_postgres(df, jdbc_url, properties, table_name, mode="overwrite"):
    """Write DataFrame to PostgreSQL"""
    df.write.jdbc(
        url=jdbc_url,
        table=table_name,
        mode=mode,
        properties=properties
    )

def main():
    """Main ETL function"""
    # Create Spark session
    spark = create_spark_session()
    spark.sparkContext.setLogLevel("WARN")
    
    # PostgreSQL connection parameters
    host = "localhost"
    port = "5432"
    database = "ubereats"
    user = "postgres"
    password = "postgres"
    
    # Get connection parameters
    jdbc_url, properties = get_postgres_connection(host, port, database, user, password)
    
    try:
        # Read data
        print("Reading data from JSON files...")
        restaurants_df, drivers_df, orders_df = read_json_data(spark, "./storage")
        
        # Transform data
        print("Transforming data...")
        restaurants_clean, drivers_clean, orders_clean, restaurant_analytics, driver_analytics = transform_data(
            restaurants_df, drivers_df, orders_df
        )
        
        # Write to PostgreSQL
        print("Writing data to PostgreSQL...")
        
        # Write raw data
        write_to_postgres(restaurants_clean, jdbc_url, properties, "restaurants_raw")
        write_to_postgres(drivers_clean, jdbc_url, properties, "drivers_raw")
        write_to_postgres(orders_clean, jdbc_url, properties, "orders_raw")
        
        # Write analytics
        write_to_postgres(restaurant_analytics, jdbc_url, properties, "restaurant_analytics")
        write_to_postgres(driver_analytics, jdbc_url, properties, "driver_analytics")
        
        print("ETL pipeline completed successfully!")
        
    except Exception as e:
        print(f"Error in ETL pipeline: {str(e)}")
    
    finally:
        # Stop Spark
        spark.stop()
        print("Spark session stopped")

if __name__ == "__main__":
    main()
```

## Best Practices for Spark-PostgreSQL Integration

### 1. Connection Management

- **Connection Pooling**: Use connection pooling for multiple operations to avoid connection overhead
- **Timeout Settings**: Configure appropriate connection timeouts
- **Security**: Store credentials securely, not hardcoded in scripts

### 2. Performance Optimization

- **Predicate Pushdown**: Use filters that can be pushed down to the database
- **Column Selection**: Only select the columns you need
- **Partition Reads**: Use partitioning for parallel reads of large tables
- **Batch Size**: Tune batch size for writes to balance throughput and memory usage

### 3. Resource Management

- **Connection Limits**: Be aware of PostgreSQL's connection limits when parallelizing
- **Memory Configuration**: Set appropriate memory for Spark executor and driver
- **Transaction Size**: Manage transaction size to avoid excessive memory usage

### 4. Error Handling

- **Robust Error Catching**: Implement proper exception handling
- **Retries**: Add retry logic for transient failures
- **Validation**: Validate data before and after operations

### 5. Integration in Production Pipelines

- **Incremental Processing**: Use incremental loads where possible
- **CDC Integration**: Consider Change Data Capture for real-time updates
- **Schema Evolution**: Handle schema changes gracefully
- **Monitoring**: Add metrics and logging for observability

## Conclusion

In this session, we've explored how to integrate Spark with PostgreSQL databases. We've learned:

1. **Connection setup** using the JDBC driver
2. **Optimized reading** with predicate pushdown and column selection
3. **Parallel processing** through partitioning
4. **Efficient writing** with appropriate batch sizes and modes
5. **Advanced techniques** for performance optimization

These skills enable you to build efficient data pipelines that bridge traditional relational databases with modern big data ecosystems.

## Next Steps

- Experiment with different partition strategies for your specific data
- Implement incremental loading patterns for large tables
- Explore change data capture (CDC) for real-time data integration
- Test performance with various configuration parameters
- Integrate with a full data quality framework
