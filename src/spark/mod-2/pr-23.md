# First Steps with PySpark: End-to-End Pipeline Development

This guide walks through creating a simple end-to-end data processing pipeline with PySpark, following best practices for code structure, testing, and deployment.

## 1. Project Setup

First, let's set up a basic project structure:

```
src/
  app/
    main.py           # Main application entry point
    pipeline.py       # Pipeline logic
    config.py         # Configuration
    utils/
      validators.py   # Data validation functions
      transformers.py # Data transformation functions
tests/
  test_pipeline.py    # Tests for our pipeline
```

## 2. Basic PySpark Pipeline

Let's implement a simple pipeline that:
1. Reads data from JSON files
2. Applies transformations
3. Performs aggregations
4. Writes results to parquet format

### Config Module (config.py)

```python
# config.py
class AppConfig:
    # Input paths
    USERS_PATH = "./storage/mongodb/users/01JS4W5A7WWZBQ6Y1C465EYR76.jsonl"
    RESTAURANTS_PATH = "./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl"
    ORDERS_PATH = "./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl"
    
    # Output paths
    OUTPUT_DIR = "./output"
    
    # Spark configuration
    SPARK_EXECUTOR_MEMORY = "2g"
    SPARK_EXECUTOR_CORES = "2"
    SPARK_DRIVER_MEMORY = "4g"
```

### Utility Functions (utils/transformers.py)

```python
# utils/transformers.py
from pyspark.sql import DataFrame
import pyspark.sql.functions as F

def clean_phone_numbers(df: DataFrame, col_name: str) -> DataFrame:
    """Remove special characters from phone numbers"""
    return df.withColumn(
        col_name,
        F.regexp_replace(F.col(col_name), r'[()+-]', '')
    )

def convert_timestamps(df: DataFrame, col_name: str) -> DataFrame:
    """Convert string timestamps to proper timestamp type"""
    return df.withColumn(
        col_name,
        F.to_timestamp(F.col(col_name))
    )
```

### Validators (utils/validators.py)

```python
# utils/validators.py
from pyspark.sql import DataFrame
import pyspark.sql.functions as F

def validate_no_nulls(df: DataFrame, columns: list) -> bool:
    """Check if specified columns have null values"""
    for col in columns:
        if df.filter(F.col(col).isNull()).count() > 0:
            return False
    return True

def log_data_quality(df: DataFrame, name: str) -> None:
    """Log basic data quality metrics"""
    print(f"Data Quality Check for {name}:")
    print(f"- Row count: {df.count()}")
    print(f"- Column count: {len(df.columns)}")
    print(f"- Sample data: {df.limit(1).collect()}")
```

### Pipeline Implementation (pipeline.py)

```python
# pipeline.py
from pyspark.sql import SparkSession, DataFrame
import pyspark.sql.functions as F
from config import AppConfig
from utils.transformers import clean_phone_numbers, convert_timestamps
from utils.validators import validate_no_nulls, log_data_quality
import os

class UberEatsPipeline:
    def __init__(self, spark: SparkSession):
        self.spark = spark
        self.config = AppConfig()
    
    def extract(self) -> tuple:
        """Extract data from source files"""
        users_df = self.spark.read.json(self.config.USERS_PATH)
        restaurants_df = self.spark.read.json(self.config.RESTAURANTS_PATH)
        orders_df = self.spark.read.json(self.config.ORDERS_PATH)
        
        # Log extraction metrics
        log_data_quality(users_df, "Users")
        log_data_quality(restaurants_df, "Restaurants")
        log_data_quality(orders_df, "Orders")
        
        return users_df, restaurants_df, orders_df
    
    def transform(self, users_df, restaurants_df, orders_df) -> tuple:
        """Apply transformations to the data"""
        # Clean phone numbers
        users_df = clean_phone_numbers(users_df, "phone_number")
        restaurants_df = clean_phone_numbers(restaurants_df, "phone_number")
        
        # Convert timestamps
        orders_df = convert_timestamps(orders_df, "order_date")
        
        # Validate critical fields
        if not validate_no_nulls(users_df, ["user_id", "email"]):
            raise ValueError("User data contains null values in critical fields")
            
        # Create city-based user counts
        user_city_counts = users_df.groupBy("city").count().orderBy(F.desc("count"))
        
        # Create restaurant rating analysis
        rest_ratings = restaurants_df.select(
            "restaurant_id", "name", "cuisine_type", "average_rating", "num_reviews"
        ).orderBy(F.desc("average_rating"))
        
        # Order value analysis
        order_values = orders_df.select(
            "order_id", "total_amount", "order_date",
            F.year(F.col("order_date")).alias("year"),
            F.month(F.col("order_date")).alias("month")
        )
        
        return user_city_counts, rest_ratings, order_values
    
    def load(self, user_city_counts, rest_ratings, order_values) -> None:
        """Load the processed data to destination"""
        # Ensure output directory exists
        os.makedirs(self.config.OUTPUT_DIR, exist_ok=True)
        
        # Write results in parquet format
        user_city_counts.write.mode("overwrite").parquet(
            f"{self.config.OUTPUT_DIR}/user_city_counts"
        )
        
        rest_ratings.write.mode("overwrite").parquet(
            f"{self.config.OUTPUT_DIR}/restaurant_ratings"
        )
        
        order_values.write.mode("overwrite").parquet(
            f"{self.config.OUTPUT_DIR}/order_values"
        )
        
        print("Data successfully written to output directory")
    
    def run(self) -> None:
        """Execute the full pipeline"""
        users_df, restaurants_df, orders_df = self.extract()
        user_city_counts, rest_ratings, order_values = self.transform(
            users_df, restaurants_df, orders_df
        )
        self.load(user_city_counts, rest_ratings, order_values)
```

### Main Application (main.py)

```python
# main.py
from pyspark.sql import SparkSession
from pipeline import UberEatsPipeline
from config import AppConfig

def create_spark_session() -> SparkSession:
    """Create and configure a Spark session"""
    config = AppConfig()
    
    return SparkSession.builder \
        .appName("UberEatsPipeline") \
        .config("spark.executor.memory", config.SPARK_EXECUTOR_MEMORY) \
        .config("spark.executor.cores", config.SPARK_EXECUTOR_CORES) \
        .config("spark.driver.memory", config.SPARK_DRIVER_MEMORY) \
        .config("spark.sql.session.timeZone", "UTC") \
        .getOrCreate()

def main():
    """Main application entry point"""
    # Initialize Spark
    spark = create_spark_session()
    
    try:
        # Run the pipeline
        pipeline = UberEatsPipeline(spark)
        pipeline.run()
        
        print("Pipeline executed successfully!")
    except Exception as e:
        print(f"Pipeline failed with error: {str(e)}")
    finally:
        # Always stop the Spark session
        spark.stop()

if __name__ == "__main__":
    main()
```

## 3. Writing Tests (tests/test_pipeline.py)

```python
# tests/test_pipeline.py
import pytest
from pyspark.sql import SparkSession
import pyspark.sql.functions as F
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType

from app.utils.transformers import clean_phone_numbers, convert_timestamps
from app.utils.validators import validate_no_nulls

@pytest.fixture
def spark():
    """Create a Spark session for testing"""
    return SparkSession.builder \
        .appName("TestUberEatsPipeline") \
        .master("local[1]") \
        .getOrCreate()

def test_clean_phone_numbers(spark):
    # Create test data
    schema = StructType([
        StructField("user_id", IntegerType(), True),
        StructField("phone_number", StringType(), True)
    ])
    
    data = [
        (1, "(51) 4463-9821"),
        (2, "555-123-4567"),
        (3, "+1 (234) 567-8901")
    ]
    
    df = spark.createDataFrame(data, schema)
    
    # Apply transformation
    result_df = clean_phone_numbers(df, "phone_number")
    
    # Check results
    results = [row.phone_number for row in result_df.collect()]
    assert "51 44639821" in results
    assert "5551234567" in results
    assert "1 234 5678901" in results

def test_validate_no_nulls(spark):
    # Create test data with and without nulls
    schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("name", StringType(), True)
    ])
    
    data_with_nulls = [(1, "test"), (2, None)]
    data_without_nulls = [(1, "test"), (2, "test2")]
    
    df_with_nulls = spark.createDataFrame(data_with_nulls, schema)
    df_without_nulls = spark.createDataFrame(data_without_nulls, schema)
    
    # Check validation
    assert validate_no_nulls(df_without_nulls, ["name"]) == True
    assert validate_no_nulls(df_with_nulls, ["name"]) == False
```

## 4. Running the Pipeline

To run the pipeline, execute:

```bash
cd src
python -m app.main
```

## 5. Best Practices

1. **Modular Design**: The code is organized into modules with clear responsibilities
2. **Error Handling**: The pipeline includes validation and error handling
3. **Logging**: Key metrics and progress are logged
4. **Testing**: Separate test modules verify critical functions
5. **Configuration**: External configuration for paths and parameters
6. **Documentation**: Clear docstrings and comments

## 6. Next Steps

- Add more sophisticated transformations and business logic
- Implement incremental loading strategy
- Add CI/CD pipelines for automated testing and deployment
- Implement structured logging
- Add metrics collection for performance monitoring

By following this structure, you've created a maintainable, testable PySpark pipeline that can be expanded to handle more complex business requirements.
