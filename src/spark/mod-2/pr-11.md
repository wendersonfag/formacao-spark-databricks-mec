# Pandas API on Spark Foundations - Part 3: Advanced Transformation Operations

## Introduction

Building on our knowledge of basic transformations, we now explore advanced operations using Pandas API on Spark. These operations enable complex analyses using familiar Pandas syntax while leveraging Spark's distributed computing power.

## Setting Up Our Environment

Let's start by setting up our environment and loading the UberEats datasets:

```python
# Import Pandas API on Spark
import pyspark.pandas as ps
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Create a Spark session
from pyspark.sql import SparkSession
spark = SparkSession.builder \
    .appName("Pandas API Advanced Transformations") \
    .master("local[*]") \
    .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
    .getOrCreate()

# Load our datasets
restaurants_ps = ps.read_json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
drivers_ps = ps.read_json("./storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl")
orders_ps = ps.read_json("./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl")
ratings_ps = ps.read_json("./storage/mysql/ratings/01JS4W5A7YWTYRQKDA7F7N95VZ.jsonl")

# Show dataset info
print(f"Restaurants: {len(restaurants_ps)} records")
print(f"Drivers: {len(drivers_ps)} records")
print(f"Orders: {len(orders_ps)} records")
print(f"Ratings: {len(ratings_ps)} records")
```

## 1. GroupBy and Aggregations

Pandas API on Spark provides powerful grouping capabilities similar to native Pandas:

### Basic Grouping and Aggregation

```python
# Group restaurants by cuisine type and calculate statistics
cuisine_stats = restaurants_ps.groupby('cuisine_type').agg({
    'average_rating': ['mean', 'min', 'max'],
    'num_reviews': ['sum', 'mean', 'count']
})

# Display results
print("Cuisine Type Statistics:")
print(cuisine_stats.head())

# Flatten multi-level column names
cuisine_stats.columns = ['avg_rating', 'min_rating', 'max_rating', 
                         'total_reviews', 'avg_reviews', 'restaurant_count']

print("\nFlattened Column Names:")
print(cuisine_stats.head())

# Sort by average rating
top_cuisines = cuisine_stats.sort_values('avg_rating', ascending=False)
print("\nTop Cuisines by Average Rating:")
print(top_cuisines.head())
```

### Grouping by Multiple Columns

```python
# Group by cuisine type and city
location_cuisine = restaurants_ps.groupby(['country', 'city', 'cuisine_type']).agg({
    'restaurant_id': 'count',
    'average_rating': 'mean',
    'num_reviews': 'sum'
}).reset_index()

location_cuisine.columns = ['country', 'city', 'cuisine_type', 'restaurant_count', 
                           'avg_rating', 'total_reviews']

print("\nRestaurants by Location and Cuisine:")
print(location_cuisine.sort_values('restaurant_count', ascending=False).head())
```

### Filtering After Grouping

```python
# Find cuisine types with at least 3 restaurants
popular_cuisines = location_cuisine.groupby('cuisine_type')['restaurant_count'].sum()
popular_cuisines = popular_cuisines[popular_cuisines >= 3].reset_index()

print("\nPopular Cuisine Types:")
print(popular_cuisines.head())

# Filtering groups - equivalent to HAVING in SQL
high_rated_cities = location_cuisine.groupby('city').filter(
    lambda x: x['avg_rating'].mean() > 4.0
)

print("\nCities with High Average Ratings:")
print(high_rated_cities[['city', 'avg_rating']].head())
```

## 2. Merge and Join Operations

Pandas API on Spark supports various types of joins to combine datasets:

### Inner Join

```python
# Join orders with restaurants
# First, make the joining keys have the same name
orders_join = orders_ps.rename(columns={'restaurant_key': 'cnpj'})

# Perform inner join
orders_with_restaurants = orders_join.merge(
    restaurants_ps,
    on='cnpj',
    how='inner'
)

print("\nOrders with Restaurant Details:")
print(orders_with_restaurants[['order_id', 'name', 'cuisine_type', 'total_amount']].head())
print(f"Total joined records: {len(orders_with_restaurants)}")
```

### Left, Right, and Outer Joins

```python
# Left join - keep all restaurants
all_restaurants_orders = restaurants_ps.merge(
    orders_join,
    on='cnpj',
    how='left'
)

print("\nAll Restaurants with Orders (including those without orders):")
print(all_restaurants_orders[['name', 'cuisine_type', 'order_id']].head())
print(f"Total records after left join: {len(all_restaurants_orders)}")

# Count restaurants without orders
restaurants_without_orders = all_restaurants_orders[all_restaurants_orders['order_id'].isna()]
print(f"Restaurants without orders: {len(restaurants_without_orders)}")

# Right join example
all_orders_restaurants = restaurants_ps.merge(
    orders_join,
    on='cnpj',
    how='right'
)

print("\nAll Orders with Restaurant Details:")
print(f"Total records after right join: {len(all_orders_restaurants)}")

# Outer join
complete_join = restaurants_ps.merge(
    orders_join,
    on='cnpj',
    how='outer'
)

print(f"Total records after outer join: {len(complete_join)}")
```

### Multiple Joins

```python
# First join orders with restaurants
orders_restaurants = orders_join.merge(
    restaurants_ps,
    on='cnpj',
    how='inner'
)

# Then join with drivers
# Rename the joining key for drivers
orders_restaurants = orders_restaurants.rename(columns={'driver_key': 'license_number'})
complete_orders = orders_restaurants.merge(
    drivers_ps,
    on='license_number',
    how='inner'
)

print("\nComplete Order Details:")
complete_orders_sample = complete_orders[[
    'order_id', 'name', 'cuisine_type', 'first_name', 'last_name', 
    'vehicle_type', 'total_amount'
]]
print(complete_orders_sample.head())
print(f"Total records with all details: {len(complete_orders)}")
```

## 3. Applied Functions

Pandas API on Spark supports various ways to apply functions to data:

### Apply to Single Columns

```python
# Define a function to categorize restaurants
def categorize_restaurant(row):
    if row['average_rating'] >= 4.5:
        return 'Premium'
    elif row['average_rating'] >= 4.0:
        return 'High Quality'
    elif row['average_rating'] >= 3.5:
        return 'Good'
    elif row['average_rating'] >= 3.0:
        return 'Average'
    else:
        return 'Below Average'

# Apply the function to a small sample (to avoid large-scale computation for demo)
sample_restaurants = restaurants_ps.head(10)
sample_restaurants['category'] = sample_restaurants.apply(categorize_restaurant, axis=1)

print("\nRestaurant Categories:")
print(sample_restaurants[['name', 'average_rating', 'category']].head())
```

### Transform and Apply by Group

```python
# Calculate z-score within each cuisine type
def standardize(x):
    return (x - x.mean()) / x.std()

# Apply to a small dataset for demonstration
cuisines_sample = restaurants_ps.head(50)
cuisine_groups = cuisines_sample.groupby('cuisine_type')

# Transform applies the function to each value and keeps the original structure
cuisines_sample['rating_zscore'] = cuisine_groups['average_rating'].transform(standardize)

print("\nRestaurant Ratings Standardized within Cuisine:")
print(cuisines_sample[['name', 'cuisine_type', 'average_rating', 'rating_zscore']].head(10))
```

## 4. Window Operations

Window functions allow calculations across rows related to the current row:

### Rolling Calculations

```python
# First, sort restaurants by rating
sorted_restaurants = restaurants_ps.sort_values('average_rating')

# Calculate rolling average (moving average)
window_size = 5
sorted_restaurants['rolling_avg_rating'] = sorted_restaurants['average_rating'].rolling(window_size).mean()

print("\nRolling Average Ratings:")
print(sorted_restaurants[['name', 'average_rating', 'rolling_avg_rating']].head(10))
```

### Ranking and Cumulative Statistics

```python
# Rank restaurants by rating within each cuisine type
restaurants_ps['rating_rank'] = restaurants_ps.groupby('cuisine_type')['average_rating'].rank(ascending=False)

print("\nRestaurant Ranking within Cuisine:")
ranking_result = restaurants_ps[['name', 'cuisine_type', 'average_rating', 'rating_rank']]
print(ranking_result.sort_values(['cuisine_type', 'rating_rank']).head(10))

# Calculate cumulative count of restaurants by cuisine
cuisine_groups = restaurants_ps.sort_values('restaurant_id').groupby('cuisine_type')
restaurants_ps['cumulative_count'] = cuisine_groups['restaurant_id'].cumcount() + 1

print("\nCumulative Count of Restaurants by Cuisine:")
cumulative = restaurants_ps[['cuisine_type', 'name', 'cumulative_count']]
print(cumulative.sort_values(['cuisine_type', 'cumulative_count']).head(10))
```

### Shift Operations for Analysis

```python
# Sort by rating and get previous and next ratings
sorted_by_rating = restaurants_ps.sort_values('average_rating')
sorted_by_rating['prev_rating'] = sorted_by_rating['average_rating'].shift(1)
sorted_by_rating['next_rating'] = sorted_by_rating['average_rating'].shift(-1)
sorted_by_rating['rating_diff'] = sorted_by_rating['average_rating'] - sorted_by_rating['prev_rating']

print("\nRating Comparison with Previous Restaurant:")
print(sorted_by_rating[['name', 'average_rating', 'prev_rating', 'next_rating', 'rating_diff']].head(10))
```

## 5. Practical Application: Restaurant Performance Analysis

Let's combine these advanced techniques to create a comprehensive restaurant analysis:

```python
def analyze_restaurant_performance(restaurants_df, orders_df, ratings_df):
    """
    Perform comprehensive restaurant performance analysis using advanced Pandas API operations
    """
    # Prepare data and merge datasets
    orders_df = orders_df.rename(columns={'restaurant_key': 'cnpj'})
    
    # Join orders with restaurants
    merged_data = restaurants_df.merge(
        orders_df,
        on='cnpj',
        how='left'
    )
    
    # Calculate order statistics by restaurant
    restaurant_orders = merged_data.groupby('restaurant_id').agg({
        'order_id': 'count',
        'total_amount': ['sum', 'mean']
    })
    
    # Flatten column names
    restaurant_orders.columns = ['order_count', 'total_revenue', 'avg_order_value']
    restaurant_orders = restaurant_orders.reset_index()
    
    # Merge back to get restaurant details
    performance_data = restaurants_df.merge(
        restaurant_orders,
        on='restaurant_id',
        how='left'
    )
    
    # Fill missing values for restaurants without orders
    performance_data['order_count'] = performance_data['order_count'].fillna(0)
    performance_data['total_revenue'] = performance_data['total_revenue'].fillna(0)
    performance_data['avg_order_value'] = performance_data['avg_order_value'].fillna(0)
    
    # Calculate performance metrics
    performance_data['revenue_rank'] = performance_data.groupby('cuisine_type')['total_revenue'].rank(ascending=False)
    performance_data['rating_rank'] = performance_data.groupby('cuisine_type')['average_rating'].rank(ascending=False)
    performance_data['order_rank'] = performance_data.groupby('cuisine_type')['order_count'].rank(ascending=False)
    
    # Create a combined performance score (weighted average of normalized metrics)
    max_rev = performance_data['total_revenue'].max() if performance_data['total_revenue'].max() > 0 else 1
    max_rating = performance_data['average_rating'].max() if performance_data['average_rating'].max() > 0 else 1
    max_orders = performance_data['order_count'].max() if performance_data['order_count'].max() > 0 else 1
    
    performance_data['performance_score'] = (
        0.4 * (performance_data['total_revenue'] / max_rev) +
        0.3 * (performance_data['average_rating'] / max_rating) +
        0.3 * (performance_data['order_count'] / max_orders)
    )
    
    # Create cuisine-level summary
    cuisine_summary = performance_data.groupby('cuisine_type').agg({
        'restaurant_id': 'count',
        'average_rating': 'mean',
        'order_count': 'sum',
        'total_revenue': 'sum',
        'performance_score': 'mean'
    }).reset_index()
    
    cuisine_summary.columns = [
        'cuisine_type', 'restaurant_count', 'avg_rating', 
        'total_orders', 'total_revenue', 'avg_performance'
    ]
    
    return {
        'restaurant_performance': performance_data,
        'cuisine_summary': cuisine_summary
    }

# Run the analysis
analysis_results = analyze_restaurant_performance(restaurants_ps, orders_ps, ratings_ps)

# Display restaurant performance
top_restaurants = analysis_results['restaurant_performance'].sort_values('performance_score', ascending=False)
print("\nTop Performing Restaurants:")
print(top_restaurants[[
    'name', 'cuisine_type', 'average_rating', 'order_count', 
    'total_revenue', 'performance_score'
]].head(10))

# Display cuisine summary
print("\nCuisine Performance Summary:")
print(analysis_results['cuisine_summary'].sort_values('avg_performance', ascending=False))
```

## Conclusion and Best Practices

In this lesson, we've explored advanced transformation operations using Pandas API on Spark:

1. **GroupBy and Aggregations**: Summarizing data by categories
2. **Merge and Join Operations**: Combining data from different sources
3. **Applied Functions**: Using apply, transform, and other function application methods
4. **Window Operations**: Performing calculations across related rows
5. **Practical Application**: Building comprehensive analytical workflows

**Best Practices:**

- Use appropriate merge/join types to avoid unexpected data loss
- Be cautious with `apply()` on large datasets as it can be less efficient than vectorized operations
- Remember that operations in Pandas API on Spark are distributed - optimize accordingly
- Reset indexes after operations like `sort_values()` and `groupby()` to avoid confusion
- Consider performance implications of complex window operations on large datasets
- Use `transform()` when you need to maintain the original DataFrame structure

In the next lesson, we'll explore advanced techniques with Pandas API on Spark, including performance optimization and scalability considerations.

## Exercise

1. Analyze driver performance:
   - Join orders with driver data
   - Calculate metrics like average order value and order count per driver
   - Rank drivers by performance within each vehicle type
   - Create a driver performance score based on multiple metrics

2. Perform time-based analysis on orders:
   - Extract date components from timestamps
   - Group orders by time periods
   - Calculate rolling averages of order values over time
   - Identify peak order times and trends

## Resources

- [Pandas API on Spark GroupBy](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/groupby.html)
- [Pandas API on Spark Merge/Join](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.merge.html)
