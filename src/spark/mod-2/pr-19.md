# Performance Comparison: PySpark vs. Pandas API vs. Spark SQL

## Overview

A common misconception is that different Spark APIs (PySpark DataFrame API, Pandas API on Spark, and Spark SQL) might have significantly different performance characteristics. This practical demonstration will show that all three APIs ultimately compile down to the same execution plan within the Spark SQL engine, resulting in nearly identical performance for equivalent operations.

## Setup

First, let's initialize our Spark session and load the UberEats sample data:

```python
from pyspark.sql import SparkSession
import pyspark.pandas as ps
import pandas as pd
import time
import matplotlib.pyplot as plt
import numpy as np

# Create a Spark session
spark = SparkSession.builder \
    .appName("API Performance Comparison") \
    .master("local[*]") \
    .config("spark.sql.shuffle.partitions", "20") \
    .getOrCreate()

# Load the UberEats data
restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
drivers_df = spark.read.json("./storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl")
orders_df = spark.read.json("./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl")
ratings_df = spark.read.json("./storage/mysql/ratings/01JS4W5A7YWTYRQKDA7F7N95VZ.jsonl")

# Register temporary views for SQL
restaurants_df.createOrReplaceTempView("restaurants")
drivers_df.createOrReplaceTempView("drivers")
orders_df.createOrReplaceTempView("orders")
ratings_df.createOrReplaceTempView("ratings")

# Convert to Pandas API on Spark
ps_restaurants = ps.DataFrame(restaurants_df)
ps_drivers = ps.DataFrame(drivers_df)
ps_orders = ps.DataFrame(orders_df)
ps_ratings = ps.DataFrame(ratings_df)

# Show the data loaded
print(f"Restaurants: {restaurants_df.count()} records")
print(f"Drivers: {drivers_df.count()} records")
print(f"Orders: {orders_df.count()} records")
print(f"Ratings: {ratings_df.count()} records")
```

## Performance Comparison: Common Operations

Let's compare the performance of various common operations across the three APIs.

### Simple Filtering and Aggregation

```python
# Create a utility function for benchmarking
def benchmark_operation(name, operation_func, iterations=5, warmup=2):
    """Run an operation multiple times and return the average execution time."""
    # Warmup runs
    for _ in range(warmup):
        operation_func()
    
    # Timed runs
    times = []
    for _ in range(iterations):
        start_time = time.time()
        operation_func()
        end_time = time.time()
        times.append(end_time - start_time)
    
    avg_time = sum(times) / len(times)
    print(f"{name}: {avg_time:.4f} seconds (avg of {iterations} runs)")
    return avg_time

# Define operations for each API (filtering restaurants by rating and calculating average per cuisine)
def pyspark_operation():
    from pyspark.sql.functions import avg, count, round
    result = restaurants_df.filter("average_rating > 3.0") \
        .groupBy("cuisine_type") \
        .agg(
            round(avg("average_rating"), 2).alias("avg_rating"),
            count("*").alias("count")
        ) \
        .orderBy("cuisine_type")
    return result.collect()  # Force execution

def pandas_api_operation():
    result = ps_restaurants[ps_restaurants.average_rating > 3.0] \
        .groupby("cuisine_type") \
        .agg({
            "average_rating": "mean",
            "restaurant_id": "count"
        }) \
        .rename(columns={"average_rating": "avg_rating", "restaurant_id": "count"}) \
        .reset_index() \
        .sort_values("cuisine_type")
    return result.to_pandas()  # Force execution

def sql_operation():
    result = spark.sql("""
        SELECT 
            cuisine_type,
            ROUND(AVG(average_rating), 2) AS avg_rating,
            COUNT(*) AS count
        FROM restaurants
        WHERE average_rating > 3.0
        GROUP BY cuisine_type
        ORDER BY cuisine_type
    """)
    return result.collect()  # Force execution

# Run benchmark
print("\n=== Simple Filtering and Aggregation ===")
time_pyspark = benchmark_operation("PySpark", pyspark_operation)
time_pandas_api = benchmark_operation("Pandas API", pandas_api_operation)
time_sql = benchmark_operation("Spark SQL", sql_operation)

# Comparison ratios
fastest = min(time_pyspark, time_pandas_api, time_sql)
print(f"\nRelative Performance (1.0 = fastest):")
print(f"PySpark: {time_pyspark / fastest:.2f}x")
print(f"Pandas API: {time_pandas_api / fastest:.2f}x")
print(f"Spark SQL: {time_sql / fastest:.2f}x")
```

### Join and Complex Aggregation

```python
# Join restaurants with ratings and calculate metrics
def pyspark_join_operation():
    from pyspark.sql.functions import col, avg, count, round
    
    result = restaurants_df.join(
        ratings_df,
        restaurants_df.cnpj == ratings_df.restaurant_identifier,
        "inner"
    ).groupBy(
        "cuisine_type", "city"
    ).agg(
        round(avg("rating"), 2).alias("avg_rating"),
        count("rating_id").alias("rating_count"),
        round(avg("average_rating"), 2).alias("avg_restaurant_rating")
    ).orderBy("cuisine_type", "city")
    
    return result.collect()  # Force execution

def pandas_api_join_operation():
    # Pandas API on Spark join
    result = ps_restaurants.merge(
        ps_ratings,
        left_on="cnpj",
        right_on="restaurant_identifier",
        how="inner"
    ).groupby(["cuisine_type", "city"]).agg({
        "rating": "mean",
        "rating_id": "count",
        "average_rating": "mean"
    }).rename(columns={
        "rating": "avg_rating", 
        "rating_id": "rating_count",
        "average_rating": "avg_restaurant_rating"
    }).reset_index().sort_values(["cuisine_type", "city"])
    
    # Round for consistency with other APIs
    result["avg_rating"] = result["avg_rating"].round(2)
    result["avg_restaurant_rating"] = result["avg_restaurant_rating"].round(2)
    
    return result.to_pandas()  # Force execution

def sql_join_operation():
    result = spark.sql("""
        SELECT 
            r.cuisine_type,
            r.city,
            ROUND(AVG(rt.rating), 2) AS avg_rating,
            COUNT(rt.rating_id) AS rating_count,
            ROUND(AVG(r.average_rating), 2) AS avg_restaurant_rating
        FROM restaurants r
        INNER JOIN ratings rt ON r.cnpj = rt.restaurant_identifier
        GROUP BY r.cuisine_type, r.city
        ORDER BY r.cuisine_type, r.city
    """)
    
    return result.collect()  # Force execution

# Run benchmark
print("\n=== Join and Complex Aggregation ===")
time_pyspark_join = benchmark_operation("PySpark", pyspark_join_operation)
time_pandas_api_join = benchmark_operation("Pandas API", pandas_api_join_operation)
time_sql_join = benchmark_operation("Spark SQL", sql_join_operation)

# Comparison ratios
fastest_join = min(time_pyspark_join, time_pandas_api_join, time_sql_join)
print(f"\nRelative Performance (1.0 = fastest):")
print(f"PySpark: {time_pyspark_join / fastest_join:.2f}x")
print(f"Pandas API: {time_pandas_api_join / fastest_join:.2f}x")
print(f"Spark SQL: {time_sql_join / fastest_join:.2f}x")
```

### Window Functions

```python
# Calculate restaurant rank within each city based on rating
def pyspark_window_operation():
    from pyspark.sql.functions import rank, col, desc
    from pyspark.sql.window import Window
    
    window_spec = Window.partitionBy("city").orderBy(desc("average_rating"))
    
    result = restaurants_df.withColumn(
        "rank", 
        rank().over(window_spec)
    ).filter(
        col("rank") <= 3
    ).select(
        "city", "name", "cuisine_type", "average_rating", "rank"
    ).orderBy("city", "rank")
    
    return result.collect()  # Force execution

def pandas_api_window_operation():
    # Group by city, sort by average_rating and assign rank
    result = ps_restaurants.assign(
        rank=ps_restaurants.groupby("city")["average_rating"].rank(ascending=False, method="min")
    ).query("rank <= 3").sort_values(["city", "rank"])[
        ["city", "name", "cuisine_type", "average_rating", "rank"]
    ]
    
    return result.to_pandas()  # Force execution

def sql_window_operation():
    result = spark.sql("""
        WITH RankedRestaurants AS (
            SELECT 
                city, 
                name, 
                cuisine_type, 
                average_rating,
                RANK() OVER (PARTITION BY city ORDER BY average_rating DESC) as rank
            FROM restaurants
        )
        SELECT city, name, cuisine_type, average_rating, rank
        FROM RankedRestaurants
        WHERE rank <= 3
        ORDER BY city, rank
    """)
    
    return result.collect()  # Force execution

# Run benchmark
print("\n=== Window Functions ===")
time_pyspark_window = benchmark_operation("PySpark", pyspark_window_operation)
time_pandas_api_window = benchmark_operation("Pandas API", pandas_api_window_operation)
time_sql_window = benchmark_operation("Spark SQL", sql_window_operation)

# Comparison ratios
fastest_window = min(time_pyspark_window, time_pandas_api_window, time_sql_window)
print(f"\nRelative Performance (1.0 = fastest):")
print(f"PySpark: {time_pyspark_window / fastest_window:.2f}x")
print(f"Pandas API: {time_pandas_api_window / fastest_window:.2f}x")
print(f"Spark SQL: {time_sql_window / fastest_window:.2f}x")
```

### Data Visualization of Results

```python
# Collect all benchmark results
operations = ['Simple Filter/Agg', 'Join/Complex Agg', 'Window Functions']
pyspark_times = [time_pyspark, time_pyspark_join, time_pyspark_window]
pandas_api_times = [time_pandas_api, time_pandas_api_join, time_pandas_api_window]
sql_times = [time_sql, time_sql_join, time_sql_window]

# Set up the plot
fig, ax = plt.figure(figsize=(12, 6)), plt.subplot(111)
width = 0.25
x = np.arange(len(operations))

# Create the bars
bar1 = ax.bar(x - width, pyspark_times, width, label='PySpark', color='#4285F4')
bar2 = ax.bar(x, pandas_api_times, width, label='Pandas API', color='#0F9D58')
bar3 = ax.bar(x + width, sql_times, width, label='Spark SQL', color='#F4B400')

# Add labels and title
ax.set_ylabel('Time (seconds)')
ax.set_title('Performance Comparison of Spark APIs')
ax.set_xticks(x)
ax.set_xticklabels(operations)
ax.legend()

# Add value labels on top of each bar
def add_labels(bars):
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}s', ha='center', va='bottom')

add_labels(bar1)
add_labels(bar2)
add_labels(bar3)

plt.tight_layout()
plt.savefig('api_performance_comparison.png')
plt.show()
```

## Examining the Execution Plans

To demonstrate that all three APIs compile to the same execution plan, let's look at the execution plans for a simple operation:

```python
# Define a common operation across all APIs - filtering and aggregating restaurant data
query_to_explain = "Find the average rating for each cuisine type where the rating is > 3.0"

# PySpark DataFrame API version
pyspark_query = restaurants_df.filter("average_rating > 3.0") \
    .groupBy("cuisine_type") \
    .agg({"average_rating": "avg"}) \
    .orderBy("cuisine_type")

# Pandas API on Spark version  
pandas_query = ps_restaurants[ps_restaurants.average_rating > 3.0] \
    .groupby("cuisine_type") \
    .agg({"average_rating": "mean"}) \
    .reset_index() \
    .sort_values("cuisine_type")

# Spark SQL version
sql_query = spark.sql("""
    SELECT 
        cuisine_type,
        AVG(average_rating) as avg_rating
    FROM restaurants
    WHERE average_rating > 3.0
    GROUP BY cuisine_type
    ORDER BY cuisine_type
""")

# Print the execution plans for each API
print("\n=== Execution Plan Comparison ===")
print("\nQuery: " + query_to_explain)

print("\n--- PySpark DataFrame API Execution Plan ---")
print(pyspark_query.explain())

print("\n--- Pandas API on Spark Execution Plan ---")
# Since ps.DataFrame.explain() isn't directly available, convert to spark first
pandas_query_spark = pandas_query.to_spark()
print(pandas_query_spark.explain())

print("\n--- Spark SQL Execution Plan ---")
print(sql_query.explain())
```

## Understanding the Results

Analyzing the performance and execution plans reveals several key insights:

1. **Nearly Identical Performance**: All three APIs show remarkably similar performance across different operation types, with minor variations attributed to:
   - API translation overhead (especially for Pandas API)
   - Caching effects
   - Random execution variations

2. **Same Execution Plan**: The execution plans confirm that regardless of which API you use, Spark converts your code into the same logical and physical execution plan.

3. **Unified Catalyst Optimizer**: All three APIs leverage Spark's Catalyst optimizer, which:
   - Analyzes the logical plan
   - Applies optimizations
   - Generates an efficient physical execution plan
   - Compiles to the same underlying code

4. **Performance Consistency**: For larger datasets, the performance gap narrows even further as the data processing time dominates over API overhead.

## Key Takeaways

1. **Focus on Readability and Familiarity**: Choose the API that is most readable and familiar to your team, as performance differences are negligible.

2. **Leverage API Strengths**: Use each API for what it does best:
   - PySpark: General purpose data processing
   - Pandas API: Data science workflows with familiar pandas syntax
   - Spark SQL: Complex analytical queries and SQL-based integrations

3. **Unified Ecosystem**: You can freely mix APIs in the same application without performance concerns.

4. **Optimization Impacts All APIs**: Performance optimizations at the Spark level (partitioning, caching, etc.) benefit all three APIs equally.

## Conclusion

This performance analysis demonstrates that all three Spark APIs—PySpark DataFrame API, Pandas API on Spark, and Spark SQL—translate to the same execution plan and achieve nearly identical performance. This means you should select your API based on code readability, team expertise, and specific use case requirements rather than performance concerns.

In subsequent lessons, we'll explore each API in depth, focusing on their unique syntax, specialized features, and best practices.
