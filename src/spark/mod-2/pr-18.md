# Spark SQL Foundations - Part 5: Data Delivery

## Introduction

In this final part of our Spark SQL fundamentals series, we'll explore how to deliver processed data in structured and optimized formats for downstream consumption. We'll cover creating and managing permanent tables, optimizing data for efficient reading, working with metadata and statistics, and integrating with BI tools.

## Setting Up Our Environment

Let's start by initializing our Spark session and loading the UberEats datasets we've been working with:

```python
from pyspark.sql import SparkSession
import os

# Initialize Spark session with Hive support
spark = SparkSession.builder \
    .appName("SparkSQL_Data_Delivery") \
    .master("local[*]") \
    .config("spark.sql.warehouse.dir", "./spark-warehouse") \
    .enableHiveSupport() \
    .getOrCreate()

# Set log level to minimize output noise
spark.sparkContext.setLogLevel("WARN")

# Load our datasets
restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
drivers_df = spark.read.json("./storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl")
orders_df = spark.read.json("./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl")

# Register temporary views
restaurants_df.createOrReplaceTempView("restaurants_temp")
drivers_df.createOrReplaceTempView("drivers_temp")
orders_df.createOrReplaceTempView("orders_temp")

print("Spark session initialized and datasets loaded successfully!")
```

## 1. Creating and Managing Permanent Tables

Temporary views only persist during the current Spark session. For data that needs to be available across sessions, we use permanent tables.

### Creating a Permanent Table from a DataFrame

```python
# Create database if not exists
spark.sql("CREATE DATABASE IF NOT EXISTS ubereats_analytics")

# Set the current database
spark.sql("USE ubereats_analytics")

# Create a permanent table from DataFrame
restaurants_df.write \
    .format("parquet") \
    .mode("overwrite") \
    .saveAsTable("ubereats_analytics.restaurants")

# Create a table using SQL
spark.sql("""
    CREATE TABLE IF NOT EXISTS ubereats_analytics.drivers
    USING PARQUET
    AS SELECT * FROM drivers_temp
""")

print("Permanent tables created successfully!")
```

### Creating a Managed vs. External Table

```python
# Create a managed table (Spark manages both data and metadata)
spark.sql("""
    CREATE TABLE IF NOT EXISTS ubereats_analytics.order_summary
    USING PARQUET
    AS
    SELECT 
        o.order_id,
        o.total_amount,
        r.name AS restaurant_name,
        r.cuisine_type,
        o.order_date
    FROM orders_temp o
    JOIN restaurants_temp r ON o.restaurant_key = r.cnpj
""")

# Create an external table (Spark manages metadata only, data is stored externally)
# First, save the data to a specific location
orders_summary_path = "./spark-warehouse/ubereats_analytics/external_order_summary"

spark.sql("""
    SELECT 
        o.order_id,
        o.total_amount,
        r.name AS restaurant_name,
        r.cuisine_type,
        o.order_date
    FROM orders_temp o
    JOIN restaurants_temp r ON o.restaurant_key = r.cnpj
""").write.format("parquet").mode("overwrite").save(orders_summary_path)

# Create the external table pointing to that location
spark.sql(f"""
    CREATE EXTERNAL TABLE IF NOT EXISTS ubereats_analytics.external_order_summary
    USING PARQUET
    LOCATION '{orders_summary_path}'
""")

print("Managed and external tables created successfully!")
```

### Listing and Describing Tables

```python
# List all tables in the database
tables = spark.sql("SHOW TABLES IN ubereats_analytics").collect()
print("Tables in ubereats_analytics database:")
for table in tables:
    print(f" - {table.tableName} ({'external' if table.isTemporary else 'managed'})")

# Get detailed table information
spark.sql("DESCRIBE EXTENDED ubereats_analytics.restaurants").show(truncate=False)
```

### Dropping Tables

```python
# Dropping a managed table deletes both metadata and data
spark.sql("DROP TABLE IF EXISTS ubereats_analytics.temp_table")

# Dropping an external table only deletes metadata, not the actual data
spark.sql("DROP TABLE IF EXISTS ubereats_analytics.external_temp")
```

## 2. Optimizing Tables for Reading

Optimization is crucial for efficient data access. Let's explore several techniques to optimize our tables.

### Partitioning

Partitioning divides data into separate directories based on column values, allowing Spark to skip irrelevant partitions during queries:

```python
# Create a partitioned table by cuisine type and order date
spark.sql("""
    CREATE TABLE IF NOT EXISTS ubereats_analytics.order_summary_partitioned
    USING PARQUET
    PARTITIONED BY (cuisine_type, order_year)
    AS
    SELECT 
        o.order_id,
        o.total_amount,
        r.name AS restaurant_name,
        r.cuisine_type,
        YEAR(o.order_date) AS order_year,
        o.order_date
    FROM orders_temp o
    JOIN restaurants_temp r ON o.restaurant_key = r.cnpj
""")

# Show partition information
spark.sql("SHOW PARTITIONS ubereats_analytics.order_summary_partitioned").show()

# Query efficiency example - only reads relevant partitions
italian_orders_2022 = spark.sql("""
    SELECT * FROM ubereats_analytics.order_summary_partitioned
    WHERE cuisine_type = 'Italian' AND order_year = 2022
""")

print("Number of Italian orders in 2022:", italian_orders_2022.count())
```

### Bucketing

Bucketing distributes data evenly across a fixed number of buckets, optimizing join operations:

```python
# Create a bucketed table
spark.sql("""
    CREATE TABLE IF NOT EXISTS ubereats_analytics.restaurants_bucketed
    USING PARQUET
    CLUSTERED BY (cuisine_type) INTO 4 BUCKETS
    AS SELECT * FROM restaurants_temp
""")

# Join optimization with bucketed tables
spark.sql("""
    CREATE TABLE IF NOT EXISTS ubereats_analytics.orders_bucketed
    USING PARQUET
    CLUSTERED BY (restaurant_key) INTO 4 BUCKETS
    AS 
    SELECT * FROM orders_temp
""")

# Enable bucketed join optimization
spark.sql("SET spark.sql.autoBroadcastJoinThreshold = -1")
spark.sql("SET spark.sql.join.preferSortMergeJoin = false")

# Execute a join that benefits from bucketing
bucketed_join = spark.sql("""
    SELECT 
        o.order_id,
        r.name AS restaurant_name,
        o.total_amount
    FROM ubereats_analytics.orders_bucketed o
    JOIN ubereats_analytics.restaurants_bucketed r ON o.restaurant_key = r.cnpj
""")

print("Bucketed join executed successfully!")
```

### File Format Optimization

Different file formats offer various advantages:

```python
# Compare different file formats
# 1. Parquet (columnar, compressed)
spark.sql("""
    CREATE TABLE IF NOT EXISTS ubereats_analytics.restaurants_parquet
    USING PARQUET
    AS SELECT * FROM restaurants_temp
""")

# 2. ORC (optimized columnar format)
spark.sql("""
    CREATE TABLE IF NOT EXISTS ubereats_analytics.restaurants_orc
    USING ORC
    AS SELECT * FROM restaurants_temp
""")

# 3. Avro (row-based, good for schema evolution)
spark.sql("""
    CREATE TABLE IF NOT EXISTS ubereats_analytics.restaurants_avro
    USING AVRO
    AS SELECT * FROM restaurants_temp
""")

# Storage comparison
for format in ["parquet", "orc", "avro"]:
    table_name = f"ubereats_analytics.restaurants_{format}"
    size = spark.sql(f"DESCRIBE FORMATTED {table_name}").filter("col_name = 'Statistics'").select("data_type").collect()[0][0]
    print(f"{format.upper()} format size: {size}")
```

## 3. Working with Metadata and Statistics

Metadata and statistics help Spark's optimizer make better decisions.

### Collecting Table Statistics

```python
# Collect statistics for a table
spark.sql("ANALYZE TABLE ubereats_analytics.restaurants COMPUTE STATISTICS")

# Collect column-level statistics
spark.sql("""
    ANALYZE TABLE ubereats_analytics.restaurants 
    COMPUTE STATISTICS FOR COLUMNS cuisine_type, average_rating, num_reviews
""")

# View table statistics
table_stats = spark.sql("DESCRIBE EXTENDED ubereats_analytics.restaurants").filter("col_name = 'Statistics'").collect()
print("Table statistics:", table_stats[0]["data_type"])
```

### Creating Table Properties

```python
# Add properties/metadata to a table
spark.sql("""
    ALTER TABLE ubereats_analytics.restaurants
    SET TBLPROPERTIES (
        'owner' = 'data_engineering',
        'source' = 'mysql',
        'last_updated' = '2025-04-21',
        'description' = 'Restaurant information for UberEats analytics',
        'quality_level' = 'gold'
    )
""")

# View table properties
properties = spark.sql("SHOW TBLPROPERTIES ubereats_analytics.restaurants").collect()
print("Table properties:")
for prop in properties:
    print(f" - {prop.key}: {prop.value}")
```

### Schema Evolution and Management

```python
# Add a new column to existing table
spark.sql("""
    ALTER TABLE ubereats_analytics.restaurants
    ADD COLUMNS (last_updated_timestamp TIMESTAMP)
""")

# Update the new column
spark.sql("""
    UPDATE ubereats_analytics.restaurants
    SET last_updated_timestamp = CURRENT_TIMESTAMP()
""")

# Verify schema change
spark.sql("DESCRIBE ubereats_analytics.restaurants").show()
```

## 4. Data Catalog and Organization

Organizing your data with a proper catalog structure improves discoverability and governance.

### Creating a Multi-Level Organization

```python
# Create databases for different data zones
spark.sql("CREATE DATABASE IF NOT EXISTS ubereats_bronze")  # Raw data
spark.sql("CREATE DATABASE IF NOT EXISTS ubereats_silver")  # Cleaned and validated
spark.sql("CREATE DATABASE IF NOT EXISTS ubereats_gold")    # Business-ready

# Create tables in the respective zones
# Bronze layer - raw data
spark.sql("""
    CREATE TABLE IF NOT EXISTS ubereats_bronze.restaurants
    USING PARQUET
    AS SELECT * FROM restaurants_temp
""")

# Silver layer - cleaned and validated
spark.sql("""
    CREATE TABLE IF NOT EXISTS ubereats_silver.restaurants
    USING PARQUET
    AS SELECT 
        restaurant_id,
        name,
        cuisine_type,
        city,
        average_rating,
        num_reviews,
        opening_time,
        closing_time,
        CASE WHEN average_rating > 5 OR average_rating < 0 THEN NULL ELSE average_rating END AS validated_rating
    FROM ubereats_bronze.restaurants
""")

# Gold layer - business-ready
spark.sql("""
    CREATE TABLE IF NOT EXISTS ubereats_gold.restaurant_performance
    USING PARQUET
    AS SELECT 
        r.restaurant_id,
        r.name,
        r.cuisine_type,
        r.city,
        r.validated_rating,
        r.num_reviews,
        COUNT(o.order_id) AS order_count,
        SUM(o.total_amount) AS total_revenue,
        AVG(o.total_amount) AS avg_order_value,
        r.validated_rating * LOG(10, GREATEST(r.num_reviews, 10)) AS popularity_score
    FROM ubereats_silver.restaurants r
    LEFT JOIN orders_temp o ON r.restaurant_id = o.restaurant_key
    GROUP BY 
        r.restaurant_id, r.name, r.cuisine_type, r.city, 
        r.validated_rating, r.num_reviews
""")

print("Multi-level data organization created successfully!")
```

### Managing Table Permissions (conceptual)

```python
# In a production environment, you would set permissions like:
spark.sql("""
    -- Grant read access to analysts
    GRANT SELECT ON DATABASE ubereats_gold TO ROLE analyst
    
    -- Grant write access to data engineers
    GRANT ALL PRIVILEGES ON DATABASE ubereats_bronze TO ROLE data_engineer
""")

print("Note: Permission commands are conceptual and depend on your Spark deployment")
```

## 5. Integration with BI Tools

Spark can integrate with various BI tools. Here, we'll prepare data for BI integration.

### Creating Views for BI Tools

```python
# Create a view optimized for BI reporting
spark.sql("""
    CREATE OR REPLACE VIEW ubereats_gold.v_restaurant_dashboard AS
    SELECT 
        r.restaurant_id,
        r.name,
        r.cuisine_type,
        r.city,
        r.validated_rating,
        r.num_reviews,
        COUNT(o.order_id) AS order_count,
        SUM(o.total_amount) AS total_revenue,
        AVG(o.total_amount) AS avg_order_value,
        
        -- KPIs
        SUM(o.total_amount) / NULLIF(COUNT(o.order_id), 0) AS revenue_per_order,
        COUNT(o.order_id) / NULLIF(COUNT(DISTINCT DATE(o.order_date)), 0) AS orders_per_day,
        
        -- Time dimensions
        YEAR(o.order_date) AS order_year,
        MONTH(o.order_date) AS order_month,
        DAYOFWEEK(o.order_date) AS order_day_of_week
    FROM ubereats_silver.restaurants r
    LEFT JOIN orders_temp o ON r.restaurant_id = o.restaurant_key
    GROUP BY 
        r.restaurant_id, r.name, r.cuisine_type, r.city, 
        r.validated_rating, r.num_reviews,
        YEAR(o.order_date), MONTH(o.order_date), DAYOFWEEK(o.order_date)
""")

print("BI view created successfully!")
```

### Exporting Data for External Tools

```python
# Export data for external BI tools (e.g., Tableau, Power BI, etc.)
bi_export_path = "./bi_exports/restaurant_dashboard"

# Export to CSV (common format for BI tools)
spark.sql("SELECT * FROM ubereats_gold.v_restaurant_dashboard").write \
    .format("csv") \
    .option("header", "true") \
    .mode("overwrite") \
    .save(bi_export_path + "/csv")

# Export to Parquet (better for tools that support it)
spark.sql("SELECT * FROM ubereats_gold.v_restaurant_dashboard").write \
    .format("parquet") \
    .mode("overwrite") \
    .save(bi_export_path + "/parquet")

print(f"Data exported to {bi_export_path} for BI tool integration")
```

### JDBC Connection for Live BI Access

```python
# Conceptual example - JDBC connection for BI tools
# Note: This requires the Spark cluster to be configured with JDBC access

# Start the Spark Thrift server (conceptual)
# In a real environment, this would typically be done at the Spark cluster level
print("""
To enable JDBC/ODBC access:
1. Start the Thrift server: 
   $SPARK_HOME/sbin/start-thriftserver.sh
   
2. Configure BI tool to connect to:
   jdbc:hive2://<spark-master>:10000/ubereats_gold
   
3. Provide necessary credentials for the connection
""")
```

## 6. Practical Example: Complete Restaurant Analytics Data Mart

Let's build a complete data mart for restaurant analytics:

```python
# Create the restaurant analytics data mart
spark.sql("""
    CREATE TABLE IF NOT EXISTS ubereats_gold.restaurant_analytics_mart
    USING PARQUET
    PARTITIONED BY (cuisine_type, city)
    AS
    WITH restaurant_orders AS (
        SELECT 
            r.restaurant_id,
            r.name,
            r.cuisine_type,
            r.city,
            r.average_rating,
            r.num_reviews,
            o.order_id,
            o.total_amount,
            o.order_date,
            YEAR(o.order_date) AS year,
            MONTH(o.order_date) AS month,
            DAYOFWEEK(o.order_date) AS day_of_week,
            HOUR(o.order_date) AS hour_of_day
        FROM restaurants_temp r
        LEFT JOIN orders_temp o ON r.cnpj = o.restaurant_key
    ),
    restaurant_metrics AS (
        SELECT 
            restaurant_id,
            name,
            cuisine_type,
            city,
            average_rating,
            num_reviews,
            COUNT(order_id) AS order_count,
            SUM(total_amount) AS total_revenue,
            AVG(total_amount) AS avg_order_value,
            
            -- Time-based metrics
            COUNT(DISTINCT CONCAT(year, '-', month)) AS active_months,
            COUNT(DISTINCT CASE WHEN year = 2023 THEN order_id ELSE NULL END) AS orders_2023,
            SUM(CASE WHEN year = 2023 THEN total_amount ELSE 0 END) AS revenue_2023,
            
            -- Peak hours
            MAX(CASE WHEN hour_of_day BETWEEN 11 AND 14 THEN COUNT(order_id) ELSE 0 END) 
                OVER (PARTITION BY restaurant_id) AS lunch_orders,
            MAX(CASE WHEN hour_of_day BETWEEN 18 AND 21 THEN COUNT(order_id) ELSE 0 END) 
                OVER (PARTITION BY restaurant_id) AS dinner_orders,
                
            -- Ranking metrics
            PERCENT_RANK() OVER (PARTITION BY cuisine_type ORDER BY average_rating) AS percentile_in_cuisine,
            DENSE_RANK() OVER (PARTITION BY city ORDER BY average_rating DESC) AS rank_in_city
        FROM restaurant_orders
        GROUP BY 
            restaurant_id, name, cuisine_type, city, average_rating, num_reviews
    )
    SELECT 
        restaurant_id,
        name,
        cuisine_type,
        city,
        average_rating,
        num_reviews,
        order_count,
        total_revenue,
        avg_order_value,
        active_months,
        orders_2023,
        revenue_2023,
        lunch_orders,
        dinner_orders,
        CASE 
            WHEN percentile_in_cuisine >= 0.8 THEN 'Top 20%'
            WHEN percentile_in_cuisine >= 0.5 THEN 'Above Average'
            WHEN percentile_in_cuisine >= 0.2 THEN 'Below Average'
            ELSE 'Bottom 20%'
        END AS cuisine_percentile_category,
        rank_in_city
    FROM restaurant_metrics
""")

# Analyze table for better query performance
spark.sql("ANALYZE TABLE ubereats_gold.restaurant_analytics_mart COMPUTE STATISTICS")

# Create a view with common KPIs for business users
spark.sql("""
    CREATE OR REPLACE VIEW ubereats_gold.v_restaurant_kpis AS
    SELECT 
        name,
        cuisine_type,
        city,
        average_rating,
        num_reviews,
        order_count,
        total_revenue,
        avg_order_value,
        total_revenue / NULLIF(order_count, 0) AS revenue_per_order,
        CASE 
            WHEN average_rating >= 4.5 THEN 'Excellent'
            WHEN average_rating >= 4.0 THEN 'Very Good'
            WHEN average_rating >= 3.5 THEN 'Good'
            WHEN average_rating >= 3.0 THEN 'Average'
            ELSE 'Below Average'
        END AS rating_category,
        CASE 
            WHEN total_revenue > 10000 THEN 'High Revenue'
            WHEN total_revenue > 5000 THEN 'Medium Revenue'
            ELSE 'Low Revenue'
        END AS revenue_category,
        rank_in_city
    FROM ubereats_gold.restaurant_analytics_mart
""")

print("Restaurant analytics data mart created successfully!")
```

## Conclusion

In this final part of our Spark SQL fundamentals series, we've learned how to deliver and optimize data for consumption:

1. **Creating and Managing Permanent Tables**: Establishing persistent data structures
2. **Optimizing Tables for Reading**: Using partitioning, bucketing, and file format selection
3. **Working with Metadata and Statistics**: Improving query optimization
4. **Data Catalog and Organization**: Building a structured data ecosystem
5. **Integration with BI Tools**: Preparing data for business intelligence analysis

With these skills, you can now complete the entire Spark SQL pipeline, from data ingestion through transformation to final delivery. These techniques enable you to make your processed data available in structured, optimized formats for analytics, reporting, and data science applications.

## Cleanup

```python
# Clean up resources (comment out if you want to keep the tables)
# spark.sql("DROP DATABASE IF EXISTS ubereats_analytics CASCADE")
# spark.sql("DROP DATABASE IF EXISTS ubereats_bronze CASCADE")
# spark.sql("DROP DATABASE IF EXISTS ubereats_silver CASCADE")
# spark.sql("DROP DATABASE IF EXISTS ubereats_gold CASCADE")

# Stop the Spark session
spark.stop()
print("Spark session stopped")
```

## Next Steps

- Explore integrating Spark with Delta Lake for ACID transactions
- Implement automated workflows to update your data marts
- Set up a proper data governance structure with metadata management
- Connect your Spark tables to a BI tool like Tableau or Power BI
- Implement data quality monitoring for your analytical tables
