# Pandas API on Spark Foundations - Part 2: Basic Transformation Operations

## Introduction

Having set up our environment and learned how to ingest data, we now focus on basic transformation operations using Pandas API on Spark. These operations will allow us to manipulate data with familiar Pandas syntax while leveraging Spark's distributed processing power.

## Setting Up Our Environment

Let's start by setting up our environment and loading the UberEats datasets:

```python
# Import Pandas API on Spark
import pyspark.pandas as ps
import pandas as pd
import numpy as np

# For comparison with regular PySpark
from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder \
    .appName("Pandas API Basic Transformations") \
    .master("local[*]") \
    .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
    .getOrCreate()

# Load our datasets
restaurants_ps = ps.read_json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
drivers_ps = ps.read_json("./storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl")
orders_ps = ps.read_json("./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl")

# Show dataset info
print(f"Restaurants: {len(restaurants_ps)} records with {len(restaurants_ps.columns)} columns")
print(f"Drivers: {len(drivers_ps)} records with {len(drivers_ps.columns)} columns")
print(f"Orders: {len(orders_ps)} records with {len(orders_ps.columns)} columns")
```

## 1. Selection and Filtering

### Selecting Columns

Like in Pandas, you can select columns using bracket notation or dot notation:

```python
# Select specific columns using brackets
restaurant_details = restaurants_ps[['name', 'cuisine_type', 'city', 'average_rating']]

# Display the selected columns
print("Restaurant Details:")
print(restaurant_details.head())

# Alternative: Select using dot notation
restaurant_locations = restaurants_ps[['name', 'city', 'country']]

print("\nRestaurant Locations:")
print(restaurant_locations.head())
```

### Filtering Rows

Filter rows using boolean conditions:

```python
# Filter restaurants with high ratings
high_rated = restaurants_ps[restaurants_ps.average_rating > 4.0]
print(f"\nHigh-rated restaurants: {len(high_rated)} records")
print(high_rated[['name', 'cuisine_type', 'average_rating']].head())

# Multiple filter conditions
popular_italian = restaurants_ps[
    (restaurants_ps.cuisine_type == 'Italian') & 
    (restaurants_ps.num_reviews > 3000)
]
print(f"\nPopular Italian restaurants: {len(popular_italian)} records")
print(popular_italian[['name', 'city', 'average_rating', 'num_reviews']].head())

# Using query method (similar to Pandas)
french_restaurants = restaurants_ps.query("cuisine_type == 'French' and average_rating > 3.0")
print(f"\nGood French restaurants: {len(french_restaurants)} records")
print(french_restaurants[['name', 'city', 'average_rating']].head())
```

## 2. Operations with Columns

### Creating New Columns

```python
# Add a new column
restaurants_with_score = restaurants_ps.copy()
restaurants_with_score['popularity_score'] = np.sqrt(restaurants_with_score['num_reviews']) * restaurants_with_score['average_rating'] / 5

print("\nRestaurants with Popularity Score:")
print(restaurants_with_score[['name', 'average_rating', 'num_reviews', 'popularity_score']].head())

# Create multiple columns at once
enhanced_restaurants = restaurants_ps.assign(
    rating_scaled = restaurants_ps['average_rating'] / 5,
    is_highly_rated = restaurants_ps['average_rating'] > 4.0,
    location = restaurants_ps['city'] + ', ' + restaurants_ps['country']
)

print("\nEnhanced Restaurant Data:")
print(enhanced_restaurants[['name', 'rating_scaled', 'is_highly_rated', 'location']].head())
```

### Column Arithmetic

```python
# Basic column arithmetic
orders_ps['tax_amount'] = orders_ps['total_amount'] * 0.1  # 10% tax
orders_ps['final_amount'] = orders_ps['total_amount'] + orders_ps['tax_amount']

print("\nOrders with Tax Calculation:")
print(orders_ps[['order_id', 'total_amount', 'tax_amount', 'final_amount']].head())

# More complex calculations
restaurants_ps['rating_diff_from_avg'] = restaurants_ps['average_rating'] - restaurants_ps['average_rating'].mean()

print("\nRating Difference from Average:")
print(restaurants_ps[['name', 'average_rating', 'rating_diff_from_avg']].sort_values('rating_diff_from_avg', ascending=False).head())
```

## 3. Vectorized Functions

Pandas API on Spark supports vectorized operations for efficiency:

### Using Built-in Functions

```python
# String operations
restaurants_ps['name_upper'] = restaurants_ps['name'].str.upper()
restaurants_ps['name_length'] = restaurants_ps['name'].str.len()

print("\nString Operations:")
print(restaurants_ps[['name', 'name_upper', 'name_length']].head())

# Mathematical functions
restaurants_ps['log_reviews'] = np.log1p(restaurants_ps['num_reviews'])
restaurants_ps['sqrt_reviews'] = np.sqrt(restaurants_ps['num_reviews'])

print("\nMath Functions:")
print(restaurants_ps[['name', 'num_reviews', 'log_reviews', 'sqrt_reviews']].head())

# Binning values
rating_bins = [0, 2.0, 3.0, 4.0, 5.0]
rating_labels = ['Poor', 'Average', 'Good', 'Excellent']
restaurants_ps['rating_category'] = pd.cut(
    restaurants_ps['average_rating'], 
    bins=rating_bins, 
    labels=rating_labels
)

print("\nRating Categories:")
print(restaurants_ps[['name', 'average_rating', 'rating_category']].head())
```

### Applying Functions to Columns

```python
# Using map and apply
# Note: These operations may trigger computation in Pandas API on Spark

# Format currency amounts
def format_currency(amount):
    return f"${amount:.2f}"

# Format phone numbers
def format_phone(phone):
    # Simple formatting for demonstration
    if pd.isna(phone) or phone is None:
        return None
    return phone.replace("(", "").replace(")", "-")

# Apply to columns
restaurants_sample = restaurants_ps.head(5)  # Work with a small sample for demonstration
restaurants_sample['formatted_phone'] = restaurants_sample['phone_number'].apply(format_phone)

print("\nFormatted Phone Numbers:")
print(restaurants_sample[['name', 'phone_number', 'formatted_phone']])
```

## 4. Type Conversion

### Checking and Converting Data Types

```python
# Check data types
print("\nData Types in Restaurants DataFrame:")
print(restaurants_ps.dtypes)

# Convert data types
converted_restaurants = restaurants_ps.copy()
converted_restaurants['average_rating'] = converted_restaurants['average_rating'].astype('float32')
converted_restaurants['num_reviews'] = converted_restaurants['num_reviews'].astype('int32')

print("\nConverted Data Types:")
print(converted_restaurants.dtypes)

# Convert string to datetime
if 'dt_current_timestamp' in restaurants_ps.columns:
    converted_restaurants['timestamp'] = pd.to_datetime(converted_restaurants['dt_current_timestamp'])
    print("\nDatetime Conversion:")
    print(converted_restaurants[['dt_current_timestamp', 'timestamp']].head())
```

### Handling Missing Values

```python
# Check for missing values
print("\nMissing Values in Restaurants DataFrame:")
print(restaurants_ps.isna().sum())

# Fill missing values
filled_restaurants = restaurants_ps.copy()
filled_restaurants['average_rating'] = filled_restaurants['average_rating'].fillna(0)
filled_restaurants['num_reviews'] = filled_restaurants['num_reviews'].fillna(0)

# Drop rows with missing values
complete_restaurants = restaurants_ps.dropna()
print(f"\nComplete records (no missing values): {len(complete_restaurants)}")
```

## 5. Practical Application: Restaurant Data Transformation

Let's combine these operations to create a comprehensive transformation pipeline:

```python
def transform_restaurant_data(restaurants_df):
    """
    Apply basic transformations to restaurant data using Pandas API on Spark
    """
    # Create a copy to avoid modifying the original
    df = restaurants_df.copy()
    
    # 1. Select relevant columns
    df = df[['name', 'cuisine_type', 'city', 'country', 'average_rating', 'num_reviews', 
             'opening_time', 'closing_time']]
    
    # 2. Filter out restaurants with too few reviews
    df = df[df['num_reviews'] >= 1000]
    
    # 3. Create derived columns
    df = df.assign(
        # Location formatting
        location = df['city'] + ', ' + df['country'],
        
        # Business hours
        hours_of_operation = df['opening_time'] + ' - ' + df['closing_time'],
        
        # Rating metrics
        rating_score = df['average_rating'] / 5,  # Normalize to 0-1 scale
        popularity_score = np.sqrt(df['num_reviews']) * df['average_rating'] / 5,
        
        # Rating category
        rating_category = pd.cut(
            df['average_rating'], 
            bins=[0, 2.0, 3.0, 4.0, 5.0], 
            labels=['Poor', 'Average', 'Good', 'Excellent']
        )
    )
    
    # 4. Drop original columns we no longer need
    df = df.drop(['opening_time', 'closing_time'], axis=1)
    
    # 5. Reorder columns in a logical sequence
    df = df[[
        'name', 'cuisine_type', 'location', 'hours_of_operation',
        'average_rating', 'rating_category', 'num_reviews', 
        'rating_score', 'popularity_score'
    ]]
    
    # 6. Sort by popularity
    df = df.sort_values('popularity_score', ascending=False)
    
    return df

# Apply the transformation
transformed_restaurants = transform_restaurant_data(restaurants_ps)

# Show the results
print("\nTransformed Restaurant Data:")
print(transformed_restaurants.head(10))

# Analyze by cuisine type
cuisine_analysis = transformed_restaurants.groupby('cuisine_type').agg({
    'average_rating': 'mean',
    'num_reviews': 'sum',
    'popularity_score': 'mean'
}).reset_index()

print("\nCuisine Type Analysis:")
print(cuisine_analysis.sort_values('popularity_score', ascending=False))
```

## Conclusion and Best Practices

In this lesson, we've covered the fundamental transformation operations in Pandas API on Spark:

1. **Selection and Filtering**: Accessing specific columns and rows with conditions
2. **Column Operations**: Creating and modifying columns
3. **Vectorized Functions**: Efficiently applying operations to entire columns
4. **Type Conversion**: Changing data types and handling missing values
5. **Practical Application**: Combining operations to transform data effectively

**Best Practices:**

- Use vectorized operations whenever possible for better performance
- Remember that execution is lazy; operations are only performed when needed
- Take advantage of familiar Pandas syntax, but be aware of distributed processing differences
- Use `.assign()` for multiple column creation to improve readability
- Convert data types appropriately to optimize memory usage
- Handle missing values explicitly to avoid unexpected errors

In the next lesson, we'll explore more advanced transformation operations using Pandas API on Spark.

## Exercise

1. Transform the drivers dataset to:
   - Select and rename relevant columns
   - Create a derived "vehicle_age" column from current year and vehicle_year
   - Categorize drivers by vehicle type and age
   - Calculate statistics grouped by city

2. For the orders dataset:
   - Add tax and tip columns
   - Format currency amounts
   - Convert date strings to proper datetime objects
   - Create a day of week column
   - Analyze order patterns by day

## Resources

- [Pandas API on Spark Data Structures](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/frame.html)
- [Pandas API on Spark Data Selection](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/indexing.html)
