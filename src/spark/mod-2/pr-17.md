# Spark SQL Foundations - Part 4: Advanced Techniques

## Introduction

In this practical session, we'll explore advanced Spark SQL techniques including window functions, user-defined functions (UDFs), analytical functions, and pivot/unpivot operations. These powerful techniques allow for complex data analysis entirely within SQL, avoiding the need for imperative code.

## Setting Up Our Environment

Let's start by initializing our Spark session and loading the relevant UberEats datasets:

```python
from pyspark.sql import SparkSession
from pyspark.sql.types import StringType, DoubleType
import math

# Initialize Spark session
spark = SparkSession.builder \
    .appName("SparkSQL_Advanced_Techniques") \
    .master("local[*]") \
    .getOrCreate()

# Set log level to minimize output noise
spark.sparkContext.setLogLevel("WARN")

# Load datasets and create temporary views
restaurants_df = spark.read.json("./storage/mysql/restaurants/01JS4W5A7YWTYRQKDA7F7N95VY.jsonl")
ratings_df = spark.read.json("./storage/mysql/ratings/01JS4W5A7YWTYRQKDA7F7N95VZ.jsonl")
drivers_df = spark.read.json("./storage/postgres/drivers/01JS4W5A74BK7P4BPTJV1D3MHA.jsonl")
orders_df = spark.read.json("./storage/kafka/orders/01JS4W5A7XY65S9Z69BY51BEJ4.jsonl")
status_df = spark.read.json("./storage/kafka/status/01JS4W5A7YWTYRQKDA7F7N95W0.jsonl")
shifts_df = spark.read.json("./storage/kafka/shift/01JS4W5A7XY65S9Z69BY51BEJ6.jsonl")

# Create temporary views
restaurants_df.createOrReplaceTempView("restaurants")
ratings_df.createOrReplaceTempView("ratings")
drivers_df.createOrReplaceTempView("drivers")
orders_df.createOrReplaceTempView("orders")
status_df.createOrReplaceTempView("order_status")
shifts_df.createOrReplaceTempView("driver_shifts")

print("Spark session initialized and datasets loaded successfully!")
```

## 1. Window Functions

Window functions perform calculations across rows related to the current row without grouping the results.

### Restaurant Rankings by City and Cuisine Type

```python
# Ranking restaurants by average rating within city and cuisine type
restaurant_rankings = spark.sql("""
    SELECT 
        name,
        city,
        cuisine_type,
        average_rating,
        num_reviews,
        RANK() OVER (PARTITION BY city ORDER BY average_rating DESC) as city_rank,
        RANK() OVER (PARTITION BY cuisine_type ORDER BY average_rating DESC) as cuisine_rank,
        DENSE_RANK() OVER (PARTITION BY cuisine_type ORDER BY average_rating DESC) as cuisine_dense_rank,
        ROW_NUMBER() OVER (PARTITION BY cuisine_type ORDER BY average_rating DESC, num_reviews DESC) as cuisine_row_num
    FROM restaurants
    WHERE num_reviews > 1000
""")

print("Restaurant Rankings by City and Cuisine Type:")
restaurant_rankings.show(10)
```

### Moving Averages of Driver Earnings

```python
# Calculate moving averages for driver earnings
driver_earnings_trend = spark.sql("""
    SELECT 
        driver_id,
        shift_type,
        TO_DATE(start_time) as shift_date,
        earnings_brl,
        AVG(earnings_brl) OVER (
            PARTITION BY driver_id 
            ORDER BY TO_DATE(start_time) 
            ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
        ) as earnings_3day_avg,
        MAX(earnings_brl) OVER (PARTITION BY driver_id) as max_earnings,
        earnings_brl - LAG(earnings_brl, 1, 0) OVER (
            PARTITION BY driver_id ORDER BY TO_DATE(start_time)
        ) as earnings_change
    FROM driver_shifts
    ORDER BY driver_id, shift_date
""")

print("\nDriver Earnings Trends with Moving Averages:")
driver_earnings_trend.show(10)
```

### Cumulative Metrics Analysis

```python
# Calculate cumulative metrics over time
restaurant_growth = spark.sql("""
    WITH daily_counts AS (
        SELECT 
            TO_DATE(dt_current_timestamp) as date,
            COUNT(*) as new_restaurants
        FROM restaurants
        GROUP BY TO_DATE(dt_current_timestamp)
    )
    SELECT 
        date,
        new_restaurants,
        SUM(new_restaurants) OVER (ORDER BY date) as total_restaurants,
        ROUND(100 * new_restaurants / SUM(new_restaurants) OVER (), 2) as percentage_of_total,
        ROUND(100 * SUM(new_restaurants) OVER (ORDER BY date) / SUM(new_restaurants) OVER (), 2) as cumulative_percentage
    FROM daily_counts
    ORDER BY date
""")

print("\nRestaurant Growth with Cumulative Metrics:")
restaurant_growth.show(10)
```

## 2. User-Defined Functions (UDFs) in SQL

UDFs allow extending SQL with custom functions for specialized logic.

### Rating Category Classification

```python
# Register UDF for rating classification
spark.udf.register("rating_category", 
                  lambda r: "Excellent" if r >= 4.5 else
                            "Very Good" if r >= 4.0 else
                            "Good" if r >= 3.5 else
                            "Average" if r >= 3.0 else
                            "Below Average",
                  StringType())

# Use UDF in a query
restaurant_categories = spark.sql("""
    SELECT 
        name,
        cuisine_type,
        city,
        average_rating,
        rating_category(average_rating) as rating_category,
        num_reviews
    FROM restaurants
    ORDER BY average_rating DESC
""")

print("\nRestaurants with Rating Categories:")
restaurant_categories.show(10)
```

### Distance Calculation with Haversine Formula

```python
# Register UDF for haversine distance calculation
def haversine_distance(lat1, lon1, lat2, lon2):
    # Convert latitude and longitude from degrees to radians
    lat1_rad = math.radians(float(lat1))
    lon1_rad = math.radians(float(lon1))
    lat2_rad = math.radians(float(lat2))
    lon2_rad = math.radians(float(lon2))
    
    # Haversine formula
    dlon = lon2_rad - lon1_rad
    dlat = lat2_rad - lat1_rad
    a = math.sin(dlat/2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon/2)**2
    c = 2 * math.asin(math.sqrt(a))
    r = 6371  # Radius of Earth in kilometers
    
    return c * r

spark.udf.register("haversine", haversine_distance, DoubleType())

# Create sample data for restaurant and delivery locations
spark.sql("""
    CREATE OR REPLACE TEMPORARY VIEW delivery_locations AS
    SELECT 
        'Palmas' as city,
        -10.1686 as lat,
        -48.3305 as lon
    UNION ALL
    SELECT 
        'São Paulo' as city,
        -23.5505 as lat,
        -46.6333 as lon
    UNION ALL
    SELECT 
        'Rio de Janeiro' as city,
        -22.9068 as lat,
        -43.1729 as lon
""")

# Calculate distances between cities and restaurants
# Note: This is a demonstration - actual restaurant coordinates would be used in a real scenario
distance_calc = spark.sql("""
    SELECT 
        r.name as restaurant_name,
        r.city as restaurant_city,
        d.city as delivery_city,
        ROUND(haversine(-10.2, -48.3, -23.5, -46.6), 2) as distance_km
    FROM restaurants r
    CROSS JOIN delivery_locations d
    WHERE r.city = 'Guarará' OR r.city = 'Lontras'
    LIMIT 10
""")

print("\nDistance Calculation with Haversine UDF:")
distance_calc.show(10)
```

## 3. Analytical Functions

Analytical functions provide powerful calculations for data analysis tasks.

### Percentile Analysis of Restaurant Ratings

```python
# Percentile analysis of restaurant ratings
rating_percentiles = spark.sql("""
    SELECT 
        cuisine_type,
        COUNT(*) as restaurant_count,
        ROUND(AVG(average_rating), 2) as mean_rating,
        PERCENTILE(average_rating, 0.25) as percentile_25,
        PERCENTILE(average_rating, 0.5) as median_rating,
        PERCENTILE(average_rating, 0.75) as percentile_75,
        PERCENTILE(average_rating, 0.9) as percentile_90,
        MAX(average_rating) as max_rating,
        ROUND(STDDEV(average_rating), 2) as rating_stddev
    FROM restaurants
    GROUP BY cuisine_type
    HAVING COUNT(*) > 0
    ORDER BY restaurant_count DESC
""")

print("\nRating Percentile Analysis by Cuisine Type:")
rating_percentiles.show(10)
```

### Cumulative Distribution Analysis

```python
# Cumulative distribution of order values
order_value_distribution = spark.sql("""
    SELECT 
        total_amount as order_value,
        COUNT(*) OVER () as total_orders,
        ROW_NUMBER() OVER (ORDER BY total_amount) as order_rank,
        ROUND(ROW_NUMBER() OVER (ORDER BY total_amount) / COUNT(*) OVER () * 100, 2) as percentile,
        CUME_DIST() OVER (ORDER BY total_amount) as cum_dist,
        PERCENT_RANK() OVER (ORDER BY total_amount) as percent_rank
    FROM orders
    ORDER BY order_value
""")

print("\nOrder Value Distribution Analysis:")
order_value_distribution.show(10)
```

### Identifying Outliers with Standard Deviation

```python
# Identify outlier restaurants based on standard deviation
outlier_restaurants = spark.sql("""
    WITH cuisine_stats AS (
        SELECT 
            cuisine_type,
            AVG(average_rating) as mean_rating,
            STDDEV(average_rating) as stddev_rating
        FROM restaurants
        GROUP BY cuisine_type
        HAVING COUNT(*) > 2
    )
    SELECT 
        r.name,
        r.city,
        r.cuisine_type,
        r.average_rating,
        cs.mean_rating as cuisine_mean,
        cs.stddev_rating as cuisine_stddev,
        (r.average_rating - cs.mean_rating) / cs.stddev_rating as z_score
    FROM restaurants r
    JOIN cuisine_stats cs ON r.cuisine_type = cs.cuisine_type
    WHERE ABS((r.average_rating - cs.mean_rating) / cs.stddev_rating) > 1.5
    ORDER BY ABS((r.average_rating - cs.mean_rating) / cs.stddev_rating) DESC
""")

print("\nOutlier Restaurants Analysis:")
outlier_restaurants.show(10)
```

## 4. Pivot and Unpivot Operations

Pivot operations transform row data into columnar format, while unpivot does the reverse.

### Restaurant Count by City and Cuisine

```python
# Pivot table of restaurant counts by city and cuisine
restaurant_pivot = spark.sql("""
    SELECT * FROM (
        SELECT 
            city, 
            cuisine_type
        FROM restaurants
    ) PIVOT (
        COUNT(*) AS count
        FOR cuisine_type IN ('Italian', 'Indian', 'French', 'Chinese', 'American', 'Mexican')
    )
    ORDER BY city
""")

print("\nPivot Table: Restaurant Count by City and Cuisine:")
restaurant_pivot.show(10)
```

### Rating Distribution by Cuisine Type

```python
# Pivot table of rating distribution by cuisine type
rating_distribution = spark.sql("""
    SELECT * FROM (
        SELECT 
            cuisine_type,
            CASE 
                WHEN average_rating >= 4.5 THEN 'Excellent (4.5+)'
                WHEN average_rating >= 4.0 THEN 'Very Good (4.0-4.4)'
                WHEN average_rating >= 3.5 THEN 'Good (3.5-3.9)'
                WHEN average_rating >= 3.0 THEN 'Average (3.0-3.4)'
                ELSE 'Below Average (<3.0)'
            END as rating_category
        FROM restaurants
        WHERE average_rating IS NOT NULL
    ) PIVOT (
        COUNT(*) AS count
        FOR rating_category IN (
            'Excellent (4.5+)', 
            'Very Good (4.0-4.4)', 
            'Good (3.5-3.9)', 
            'Average (3.0-3.4)', 
            'Below Average (<3.0)'
        )
    )
    ORDER BY cuisine_type
""")

print("\nPivot Table: Rating Distribution by Cuisine Type:")
rating_distribution.show(10)
```

## 5. Combined Complex Analysis

Now let's combine all these techniques to perform some comprehensive analyses.

### Restaurant Performance Dashboard

```python
restaurant_performance = spark.sql("""
    WITH 
    RestaurantMetrics AS (
        SELECT
            r.name,
            r.cuisine_type,
            r.city,
            r.average_rating,
            r.num_reviews,
            COUNT(o.order_id) as order_count,
            SUM(o.total_amount) as total_revenue,
            AVG(o.total_amount) as avg_order_value,
            r.average_rating * SQRT(r.num_reviews / 1000) as popularity_score
        FROM restaurants r
        LEFT JOIN orders o ON r.cnpj = o.restaurant_key
        GROUP BY r.name, r.cuisine_type, r.city, r.average_rating, r.num_reviews
    ),
    CuisineStats AS (
        SELECT
            cuisine_type,
            AVG(average_rating) as cuisine_avg_rating,
            AVG(order_count) as cuisine_avg_orders,
            AVG(total_revenue) as cuisine_avg_revenue
        FROM RestaurantMetrics
        GROUP BY cuisine_type
    )
    SELECT 
        rm.name,
        rm.cuisine_type,
        rm.city,
        rm.average_rating,
        ROUND((rm.average_rating - cs.cuisine_avg_rating) / cs.cuisine_avg_rating * 100, 1) as rating_vs_cuisine_pct,
        rm.num_reviews,
        rm.order_count,
        ROUND(rm.total_revenue, 2) as total_revenue,
        ROUND(rm.avg_order_value, 2) as avg_order_value,
        ROUND(rm.popularity_score, 2) as popularity_score,
        RANK() OVER (PARTITION BY rm.cuisine_type ORDER BY rm.total_revenue DESC) as revenue_rank_in_cuisine,
        RANK() OVER (PARTITION BY rm.city ORDER BY rm.total_revenue DESC) as revenue_rank_in_city,
        rating_category(rm.average_rating) as rating_category
    FROM RestaurantMetrics rm
    JOIN CuisineStats cs ON rm.cuisine_type = cs.cuisine_type
    ORDER BY total_revenue DESC
""")

print("\nComprehensive Restaurant Performance Dashboard:")
restaurant_performance.show(10)
```

### Driver Efficiency Analysis

```python
driver_efficiency = spark.sql("""
    WITH 
    DriverMetrics AS (
        SELECT 
            d.driver_id,
            d.first_name || ' ' || d.last_name as driver_name,
            d.vehicle_type,
            d.city,
            COUNT(o.order_id) as order_count,
            SUM(o.total_amount) as total_revenue,
            SUM(ds.shift_duration_min) as total_shift_minutes,
            SUM(ds.distance_covered_km) as total_distance_km
        FROM drivers d
        LEFT JOIN orders o ON d.license_number = o.driver_key
        LEFT JOIN driver_shifts ds ON d.driver_id = ds.driver_id
        GROUP BY d.driver_id, d.first_name, d.last_name, d.vehicle_type, d.city
    )
    SELECT 
        driver_id,
        driver_name,
        vehicle_type,
        city,
        order_count,
        ROUND(total_revenue, 2) as total_revenue,
        ROUND(total_shift_minutes / 60, 1) as total_shift_hours,
        ROUND(total_distance_km, 1) as total_distance_km,
        ROUND(order_count / NULLIF(total_shift_minutes / 60, 0), 2) as orders_per_hour,
        ROUND(total_revenue / NULLIF(total_shift_minutes / 60, 0), 2) as revenue_per_hour,
        ROUND(total_revenue / NULLIF(order_count, 0), 2) as revenue_per_order,
        ROUND(total_distance_km / NULLIF(order_count, 0), 2) as km_per_order,
        RANK() OVER (PARTITION BY vehicle_type ORDER BY total_revenue / NULLIF(total_shift_minutes / 60, 0) DESC) as efficiency_rank
    FROM DriverMetrics
    WHERE total_shift_minutes > 0
    ORDER BY revenue_per_hour DESC
""")

print("\nDriver Efficiency Analysis:")
driver_efficiency.show(10)
```

### Order Status Flow Analysis

```python
order_flow = spark.sql("""
    WITH status_transitions AS (
        SELECT 
            order_identifier,
            status.status_name,
            status.timestamp,
            LEAD(status.status_name, 1) OVER (
                PARTITION BY order_identifier 
                ORDER BY status.timestamp
            ) as next_status,
            LEAD(status.timestamp, 1) OVER (
                PARTITION BY order_identifier 
                ORDER BY status.timestamp
            ) - status.timestamp as time_to_next_status
        FROM order_status
    )
    SELECT 
        status_name as current_status,
        next_status,
        COUNT(*) as transition_count,
        ROUND(AVG(time_to_next_status) / 60000, 2) as avg_minutes_to_next,
        ROUND(MIN(time_to_next_status) / 60000, 2) as min_minutes_to_next,
        ROUND(MAX(time_to_next_status) / 60000, 2) as max_minutes_to_next,
        ROUND(PERCENTILE(time_to_next_status / 60000, 0.5), 2) as median_minutes_to_next
    FROM status_transitions
    WHERE next_status IS NOT NULL
    GROUP BY current_status, next_status
    ORDER BY current_status, next_status
""")

print("\nOrder Status Flow Analysis:")
order_flow.show(10)
```

## Conclusion

In this practical session, we've explored several advanced Spark SQL techniques:

1. **Window Functions**: For ranking, running calculations, and comparative analysis
2. **User-Defined Functions (UDFs)**: To extend SQL with custom logic and calculations
3. **Analytical Functions**: For statistical analysis and data distribution insights
4. **Pivot/Unpivot Operations**: To transform data between row and column formats
5. **Combined Complex Analyses**: Using all techniques together for comprehensive insights

These advanced techniques enable sophisticated data analysis directly in SQL, avoiding the need for imperative code. By mastering these capabilities, you can perform complex analytics in a more declarative and often more readable way.

## Cleanup

```python
# Stop the Spark session
spark.stop()
print("Spark session stopped")
```
